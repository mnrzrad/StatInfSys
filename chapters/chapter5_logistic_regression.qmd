---
engine: knitr
filters: 
  - webr
---

```{webr-r}
#| context: setup
library(tidyverse)
```
# Classification

Until now, we have been doing clustering -- we had data with no labels, and we asked the computer to discover natural groups. 

Now, we move to classification. 

Suppose that we collected data from the engines on several ships: *temperature*, *vibration level*, and *pressure*. For each moment, we have numbers like $\text{temperature} = 82^\circ \mathrm{C}$, $\text{vibration} = 4.1$, and $\text{pressure} = 205 kPa$. 

At first, imagine we do not know if the engine was OK or about to fail at those moments. We just have the measurements. We ask the computer to group similar engine states together. The algorithm gives $3$ groups: 

- Group 1: normal temperature, low vibration
- Group 2: slightly higher temperature, medim vibration
- Group 3: very high temperature, very high vibration

Now, we look at Group 3 and we say that "This looks dangerous."

It is very important that the algorithm never knew the words "safe" or "danger". We discovered the groups and we interpreted them after. This is *clustering*.

Now, imagine we do have labels from the maintenance log. For every past moment, we know that 

- Group 0 means the engine did not fail in the next 24 hours.
- Group 1 means the engine failed in the next 24 hours

We can train a model. The model sees examples like $(82^\circ \textrm{C}, 4.1 \textrm{vibration}, 205 \textrm{kPa})$ results in Group $0$ (no failure) and $(96^\circ \textrm{C}, 7.5 \textrm{vibration}, 240 \textrm{kPa})$ results in Group $1$ (failure). After training, we can give the model in the next 24 hours: yes or no? That task -- taking a new case and predicting $0$ or $1$ -- is called *classification*


There are several method that can act as classifiers, such as:

- logistic regression
- $k$-nearest neighbors
- decision trees and random forests

Here, we focus only on logistic regression. 

## Logistic regression

In many situation, we want to make a "yes/no" decision. For example:

- Is this email spam or not?
- Will this student pass or fail the exam?
- Is the fish a salmon or a sea bass?
- Is this transaction fraudulent or normal?
- will a loaner enter default or not, based on their credit score and income;
-   will a driver be involved in an accident or not, based on their age, automobile age and driving history;
-   will a patient have a heart attack or not, based on their cholesterol level and blood pressure, and other health indicators.

This kind of task is called a **binary classification** problem where the outcome variable can take on only two possible values. Usually we use $0$ and $1$ to denote two classes $0$ and $1$. For example, fail ( = $0$) vs. pass ( = $1$); normal (=$1$) vs. fault (=$0$), survived (=$1$) vs. died (=$0$). 

Our goal is

> Given information (variables) about something we have not seen before, can we decide if it is class $0$ or class $1$ based on what we learned from the previous labeled examples?

:::{.callout-note}
### Difference between clustering and Classification

In clustering, we did **not** know the groups in advance. We tried to discover "natural groups".

In classification, we already know the correct group for past observations (they are labeled $0$ or $1$). We used that labeled history to learn how to classify new observations in the future. 
:::

For this kind of problem, the dataset used to fit the model is composed of observations $(\mathbf{x}_1, y_1)$, $(\mathbf{x}_2, y_2)$, ..., $(\mathbf{x}_n, y_n)$ where each $y_i$ is usually labelled 0 (failure, no event, ...) or 1 (success, event, ...) and $\mathbf{x}_i$ is a vector of input variables.

The task at hand is to build a model that, given an arbitrary $\mathbf{x}$, will predict the probability that $y = 1$.

## Modelling Probabilities

Why not use a linear regression model to predict the probability that $y = 1$? After all, linear regression is a well-known and widely used method.

```{webr-r}
#| autorun: true
n <- 20
x <- rbeta(n, 0.2, 0.4)
y <- rbinom(n, 1, x)
ggplot(tibble(x, y), aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "",
       x = "x",
       y = "y")
```

There are some problems with this approach:

-  the output values are never (with probability 1) exactly 0 or 1, the values that are taken by $y$;
-  even considering a threshold (e.g. 0.5) to classify the predicted values into 0 or 1, this would be mostly an arbitrary chosen value.

## Sigmoid Functions

Instead of a linear function, we can opt to model the *probability* of success of the event described by $y$, i.e $\operatorname{P}(Y=1)$. This is a more sound and natural way to model the prediction of a random binary event. In order to do this, we can use **sigmoid functions**. A sigmoid function $f(x)$ is a function having an "S" shaped curve (sigmoid curve) with the following properties:

-   $0<f(x)<1$;
-   $\displaystyle \lim_{x \to -\infty} f(x) = 0$, $\displaystyle \lim_{x \to +\infty} f(x) = 1$;
-   $f'(x)>0$.

A common sigmoid function is the logistic function, defined as:

$$
f(x) = \frac{e^x}{1 + e^x} = \frac{1}{1 + e^{-x}}.
$$

A logistic function can be used to model the probability that $y = 1$ given $\mathbf{x}$.

```{webr-r}
#| autorun: true
ggplot(tibble(x, y), aes(x, y)) +
  geom_point() +
  geom_smooth(method = "glm", 
       method.args = list(family = "binomial"), 
       se = FALSE) +
  labs(title = "",
       x = "x",
       y = "y")
  
```

This function will play a central row in the logistic regression model, as we will see briefly.

## Logistic Model

The first random variable that comes to mind in order to model a random binary event is the **Bernoulli random variable**, which is a discrete random variable that takes value 1 with probability $p$ and value 0 with probability $1-p$. The parameter $p$ is the probability of success of the event described by the random variable, and it's probability function is

$$
p(y) = p^y (1-p)^{1-y}, \quad y \in \{0, 1\}.
$$
The expected value of a Bernoulli random variable is $\operatorname{E}[Y] = p$ and it's variance is $\operatorname{V}(Y) = p(1-p)$.

::: callout-note
A natural extension is the **Binomial random variable**, which is the sum of $n$ independent Bernoulli random variables with the same parameter $p$. The parameter $p$ is again the probability of success of the event described by the random variable, and it's probability function is

$$
p(y) = \binom{n}{y} p^y (1-p)^{n-y}, \quad y \in \{0, 1, ..., n\}.
$$
:::

Another way of expressing the probability of an event is through the **odds** of the event, defined as the ratio between the probability of the event and the probability of it's complementary event:

$$
\operatorname{odds} = \frac{p}{1-p} = \frac{1}{1-p^{-1}}, \quad p \in ]0, 1[.
$$

The odds are a number between 0 and $+\infty$. If the odds are equal to 1, then the event and it's complementary are equally likely. If the odds are greater than 1, then the event is more likely than it's complementary event, and vice versa. It's easy to see that odds are monotonically increasing with respect to $p$:

$$
{(\operatorname{odds})}' = \frac{1}{(1-p)^2} > 0.
$$

```{webr-r}
#| autorun: true
odds <- function(p) p / (1 - p)
p <- seq(0.1, 0.9, length.out = 90)
tibble(p, odds = odds(p)) %>%
  ggplot(aes(p, odds)) +
  geom_line() +
  geom_vline(xintercept = 0.5, color = "red", linetype = "dashed") +
  geom_hline(yintercept = 1, color = "blue", linetype = "dashed") +
  annotate("text", x = 0.52, y = max(odds(p))*0.8, label = "x = 0.5", color = "red", hjust = 0) +
  annotate("text", x = 0.8, y = 1.05, label = "y = 1", color = "blue", vjust = 0) +
  labs(title = "Odds as a function of probability",
       x = "Probability",
       y = "Odds")
```

Odds are an alternative way of expressing probabilities, still  being relatively intuitive concerning interpretation. Nevertheless, they are still confined to the interval $]0, +\infty[$, because

$$
\lim_{p \to 0^+} \frac{1}{1-p^{-1}} = 0, \quad \lim_{p \to 1^-} \frac{1}{1-p^{-1}} = +\infty.
$$

If we want to have a represantation of probabilities that spans the whole real line, we can use the **log-odds**, also called **logit** function, defined as

$$
\operatorname{logit}(p) = \log\left(\frac{p}{1-p}\right), \quad p \in ]0, 1[.
$$

This creates a one-to-one mapping between the interval $]0, 1[$ and the whole real line $\mathbb{R}$.

```{webr-r}
#| autorun: true
logit <- function(p) log(p / (1 - p))
prob <- seq(0.01, 0.99, length.out = 99)
tibble(prob, logit = logit(prob)) %>%
  ggplot(aes(prob, logit)) +
  geom_line() +
  geom_vline(xintercept = 0.5, color = "red", linetype = "dashed") +
  geom_hline(yintercept = 0, color = "blue", linetype = "dashed") +
  annotate("text", x = 0.52, y = max(logit(prob))*0.8, label = "x = 0.5", color = "red", hjust = 0) +
  annotate("text", x = 0.8, y = 0.05, label = "y = 0", color = "blue", vjust = 0) +
  labs(title = "Logit as a function of probability",
       x = "Probability",
       y = "Logit")
```

The logit function is also monotonically increasing with respect to $p$:

$$
{\big(\operatorname{logit}(p)\big)}' = \frac{1}{p(1-p)} > 0.
$$

This representation of probabilities is well suited to be used in a regression model, because it spans the whole real line. In fact, in a logistic regression model, we assume that the log-odds of the probability that $y = 1$ is a linear combination of the input variables:
$$
\operatorname{logit}\big(\operatorname{P}(Y=1|\mathbf{x})\big) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \mathbf{x}^T \boldsymbol{\beta} = \eta (\mathbf{x}),
$$
where $\boldsymbol{\beta} = (\beta_0, \beta_1, ..., \beta_p)^T$ is the vector of parameters to be estimated.

Note that the logit function is the inverse of the logistic function! Hence, we can rewrite $\operatorname{P}(Y=1|\mathbf{X}=\mathbf{x})$ as

$$
\operatorname{P}(Y=1|\mathbf{x}) = \frac{e^{\eta(\mathbf{x})}}{1 + e^{\eta(\mathbf{x})}} = \frac{1}{1 + e^{-\eta(\mathbf{x})}}.
$$ {#eq-logit}

::: callout-warning
@eq-logit describes a model for the expected value of the random variable $Y$ given $\mathbf{x}$, that follows a Bernoulli distribution with parameter $p = \operatorname{P}(Y=1|\mathbf{x})$. It does **not** describe a model for $Y$ itself, which is a discrete random variable taking values 0 or 1.
:::
