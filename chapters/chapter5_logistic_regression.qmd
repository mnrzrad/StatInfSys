---
engine: knitr
filters: 
  - webr
---

```{webr-r}
#| context: setup
library(tidyverse)
```

# Logistic Regression

A binary classification problem is one where the outcome variable can take on only two possible values. For example:

-   will a loaner enter default or not, based on their credit score and income;
-   will a driver be involved in an accident or not, based on their age, automobile age and driving history;
-   will a patient have a heart attack or not, based on their cholesterol level and blood pressure, and other health indicators.

For this kind of problem, the dataset used to fit the model is composed of observations $(\mathbf{x}_1, y_1)$, $(\mathbf{x}_2, y_2)$, ..., $(\mathbf{x}_n, y_n)$ where each $y_i$ is usually labelled 0 (failure, no event, ...) or 1 (success, event, ...) and $\mathbf{x}_i$ is a vector of input variables.

The task at hand is to build a model that, given an arbitrary $\mathbf{x}$, will predict the probability that $y = 1$.

## Modelling Probabilities

Why not use a linear regression model to predict the probability that $y = 1$? After all, linear regression is a well-known and widely used method.

```{webr-r}
#| autorun: true
n <- 20
x <- rbeta(n, 0.2, 0.4)
y <- rbinom(n, 1, x)
ggplot(tibble(x, y), aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "",
       x = "x",
       y = "y")
```

There are some problems with this approach:

-  the output values are never (with probability 1) exactly 0 or 1, the values that are taken by $y$;
-  even considering a threshold (e.g. 0.5) to classify the predicted values into 0 or 1, this would be mostly an arbitrary chosen value.

## Sigmoid Functions

Instead of a linear function, we can opt to model the *probability* of success of the event described by $y$, i.e $\operatorname{P}(Y=1)$. This is a more sound and natural way to model the prediction of a random binary event. In order to do this, we can use **sigmoid functions**. A sigmoid function $f(x)$ is a function having an "S" shaped curve (sigmoid curve) with the following properties:

-   $0<f(x)<1$;
-   $\displaystyle \lim_{x \to -\infty} f(x) = 0$, $\displaystyle \lim_{x \to +\infty} f(x) = 1$;
-   $f'(x)>0$.

A common sigmoid function is the logistic function, defined as:

$$
f(x) = \frac{e^x}{1 + e^x} = \frac{1}{1 + e^{-x}}.
$$

A logistic function can be used to model the probability that $y = 1$ given $\mathbf{x}$.

```{webr-r}
#| autorun: true
ggplot(tibble(x, y), aes(x, y)) +
  geom_point() +
  geom_smooth(method = "glm", 
       method.args = list(family = "binomial"), 
       se = FALSE) +
  labs(title = "",
       x = "x",
       y = "y")
  
```

This function will play a central row in the logistic regression model, as we will see briefly.

## Logistic Model

The first random variable that comes to mind in order to model a random binary event is the **Bernoulli random variable**, which is a discrete random variable that takes value 1 with probability $p$ and value 0 with probability $1-p$. The parameter $p$ is the probability of success of the event described by the random variable, and it's probability function is

$$
p(y) = p^y (1-p)^{1-y}, \quad y \in \{0, 1\}.
$$
The expected value of a Bernoulli random variable is $\operatorname{E}[Y] = p$ and it's variance is $\operatorname{V}(Y) = p(1-p)$.

::: callout-note
A natural extension is the **Binomial random variable**, which is the sum of $n$ independent Bernoulli random variables with the same parameter $p$. The parameter $p$ is again the probability of success of the event described by the random variable, and it's probability function is

$$
p(y) = \binom{n}{y} p^y (1-p)^{n-y}, \quad y \in \{0, 1, ..., n\}.
$$
:::

Another way of expressing the probability of an event is through the **odds** of the event, defined as the ratio between the probability of the event and the probability of it's complementary event:

$$
\operatorname{odds} = \frac{p}{1-p} = \frac{1}{1-p^{-1}}, \quad p \in ]0, 1[.
$$

The odds are a number between 0 and $+\infty$. If the odds are equal to 1, then the event and it's complementary are equally likely. If the odds are greater than 1, then the event is more likely than it's complementary event, and vice versa. It's easy to see that odds are monotonically increasing with respect to $p$:

$$
{(\operatorname{odds})}' = \frac{1}{(1-p)^2} > 0.
$$

```{webr-r}
#| autorun: true
odds <- function(p) p / (1 - p)
p <- seq(0.1, 0.9, length.out = 90)
tibble(p, odds = odds(p)) %>%
  ggplot(aes(p, odds)) +
  geom_line() +
  geom_vline(xintercept = 0.5, color = "red", linetype = "dashed") +
  geom_hline(yintercept = 1, color = "blue", linetype = "dashed") +
  annotate("text", x = 0.52, y = max(odds(p))*0.8, label = "x = 0.5", color = "red", hjust = 0) +
  annotate("text", x = 0.8, y = 1.05, label = "y = 1", color = "blue", vjust = 0) +
  labs(title = "Odds as a function of probability",
       x = "Probability",
       y = "Odds")
```

Odds are an alternative way of expressing probabilities, still  being relatively intuitive concerning interpretation. Nevertheless, they are still confined to the interval $]0, +\infty[$, because

$$
\lim_{p \to 0^+} \frac{1}{1-p^{-1}} = 0, \quad \lim_{p \to 1^-} \frac{1}{1-p^{-1}} = +\infty.
$$

If we want to have a represantation of probabilities that spans the whole real line, we can use the **log-odds**, also called **logit** function, defined as

$$
\operatorname{logit}(p) = \log\left(\frac{p}{1-p}\right), \quad p \in ]0, 1[.
$$

This creates a one-to-one mapping between the interval $]0, 1[$ and the whole real line $\mathbb{R}$.

```{webr-r}
#| autorun: true
logit <- function(p) log(p / (1 - p))
prob <- seq(0.01, 0.99, length.out = 99)
tibble(prob, logit = logit(prob)) %>%
  ggplot(aes(prob, logit)) +
  geom_line() +
  geom_vline(xintercept = 0.5, color = "red", linetype = "dashed") +
  geom_hline(yintercept = 0, color = "blue", linetype = "dashed") +
  annotate("text", x = 0.52, y = max(logit(prob))*0.8, label = "x = 0.5", color = "red", hjust = 0) +
  annotate("text", x = 0.8, y = 0.05, label = "y = 0", color = "blue", vjust = 0) +
  labs(title = "Logit as a function of probability",
       x = "Probability",
       y = "Logit")
```

The logit function is also monotonically increasing with respect to $p$:

$$
{\big(\operatorname{logit}(p)\big)}' = \frac{1}{p(1-p)} > 0.
$$

This representation of probabilities is well suited to be used in a regression model, because it spans the whole real line. In fact, in a logistic regression model, we assume that the log-odds of the probability that $y = 1$ is a linear combination of the input variables:
$$
\operatorname{logit}\big(\operatorname{P}(Y=1|\mathbf{x})\big) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \mathbf{x}^T \boldsymbol{\beta} = \eta (\mathbf{x}),
$$
where $\boldsymbol{\beta} = (\beta_0, \beta_1, ..., \beta_p)^T$ is the vector of parameters to be estimated.

Note that the logit function is the inverse of the logistic function! Hence, we can rewrite $\operatorname{P}(Y=1|\mathbf{X}=\mathbf{x})$ as

$$
\operatorname{P}(Y=1|\mathbf{x}) = \frac{e^{\eta(\mathbf{x})}}{1 + e^{\eta(\mathbf{x})}} = \frac{1}{1 + e^{-\eta(\mathbf{x})}}.
$$ {#eq-logit}

::: callout-warning
@eq-logit describes a model for the expected value of the random variable $Y$ given $\mathbf{x}$, that follows a Bernoulli distribution with parameter $p = \operatorname{P}(Y=1|\mathbf{x})$. It does **not** describe a model for $Y$ itself, which is a discrete random variable taking values 0 or 1.
:::
