---
engine: knitr
filters: 
  - webr
---

```{webr-r}
#| context: setup
library(tidyverse)
```
# Classification

Until now, we have been doing clustering -- we had data with no labels, and we asked the computer to discover natural groups. 

Now, we move to classification. 

Suppose that we collected data from the engines on several ships: *temperature*, *vibration level*, and *pressure*. For each moment, we have numbers like $\text{temperature} = 82^\circ \mathrm{C}$, $\text{vibration} = 4.1$, and $\text{pressure} = 205 kPa$. 

At first, imagine we do not know if the engine was OK or about to fail at those moments. We just have the measurements. We ask the computer to group similar engine states together. The algorithm gives $3$ groups: 

- Group 1: normal temperature, low vibration
- Group 2: slightly higher temperature, medium vibration
- Group 3: very high temperature, very high vibration

Now, we look at Group 3 and we say that "This looks dangerous."

It is very important that the algorithm never knew the words "safe" or "danger". We discovered the groups and we interpreted them after. This is *clustering*.

Now, imagine we do have labels from the maintenance log. For every past moment, we know that 

- Group 0 means the engine did not fail in the next 24 hours.
- Group 1 means the engine failed in the next 24 hours

We can train a model. The model sees examples like $(82^\circ \textrm{C}, 4.1 \textrm{vibration}, 205 \textrm{kPa})$ results in Group $0$ (no failure) and $(96^\circ \textrm{C}, 7.5 \textrm{vibration}, 240 \textrm{kPa})$ results in Group $1$ (failure). After training, we can give the model in the next 24 hours: yes or no? That task -- taking a new case and predicting $0$ or $1$ -- is called *classification*


There are several method that can act as classifiers, such as:

- logistic regression
- $k$-nearest neighbors
- decision trees and random forests

Here, we focus only on logistic regression. 

## Logistic regression

In many situation, we want to make a "yes/no" decision. For example:

- Is this email spam or not?
- Will this student pass or fail the exam?
- Is the fish a salmon or a sea bass?
- Is this transaction fraudulent or normal?
- will a loaner enter default or not, based on their credit score and income;
-   will a driver be involved in an accident or not, based on their age, automobile age and driving history;
-   will a patient have a heart attack or not, based on their cholesterol level and blood pressure, and other health indicators.

This kind of task is called a **binary classification** problem where the outcome variable can take on only two possible values. Usually we use $0$ and $1$ to denote two classes $0$ and $1$. For example, fail ( = $0$) vs. pass ( = $1$); normal (=$1$) vs. fault (=$0$), survived (=$1$) vs. died (=$0$). 

Our goal is

> Given information (variables) about something we have not seen before, can we decide if it is class $0$ or class $1$ based on what we learned from the previous labeled examples?

:::{.callout-note}
### Difference between clustering and Classification

In clustering, we did **not** know the groups in advance. We tried to discover "natural groups".

In classification, we already know the correct group for past observations (they are labeled $0$ or $1$). We used that labeled history to learn how to classify new observations in the future. 
:::

Imagine we are classifying fish using two measurements: *length* (cm) and *weight* (g).

Suppose we measured many fish, and for each fish we also know the species: $1$ = salmon, and $0$ = sea bass. 

Now, we plot weight vs. length.

```{r}
#| echo: false 
#| warning: false
#| message: false
# ---------------------------------
# Toy example: salmon vs sea bass
# ---------------------------------

# We'll need ggplot2 for plotting
library(ggplot2)

set.seed(123)  # for reproducibility

# 1. Simulate "sea bass" (class 0)
#    Slightly shorter, lighter fish
n0 <- 40
length_seabass <- rnorm(n0, mean = 30, sd = 2)   # cm
weight_seabass <- rnorm(n0, mean = 200, sd = 15) # g

# 2. Simulate "salmon" (class 1)
#    Slightly longer, heavier fish
n1 <- 40
length_salmon <- rnorm(n1, mean = 35, sd = 2)    # cm
weight_salmon <- rnorm(n1, mean = 250, sd = 15)  # g

# 3. Combine into one data frame
fish <- data.frame(
  length = c(length_seabass, length_salmon),
  weight = c(weight_seabass, weight_salmon),
  species = factor(
    c(rep(0, n0), rep(1, n1)),
    levels = c(0, 1),
    labels = c("Sea bass (0)", "Salmon (1)")
  )
)

# 4. Let's define a simple linear decision boundary by hand.
#    We'll say:
#       predict "Salmon (1)" if weight >= 7*length - 20
#       predict "Sea bass (0)" otherwise
#
#    This is just a line:
#       weight = 7*length - 20
#
#    (Totally arbitrary! We're pretending this is our classifier.)
library(ggplot2)

# assuming you already have `fish` as in the previous script
# turn species into 0/1 for glm
fish$y <- ifelse(fish$species == "Salmon (1)", 1, 0)

# fit logistic regression: y ~ length + weight
m <- glm(y ~ length + weight, data = fish, family = binomial())

# p = 0.5 boundary: b0 + b1*length + b2*weight = 0  =>  weight = -(b0 + b1*length)/b2
b <- coef(m)
slope_logit     <- -b["length"] / b["weight"]
intercept_logit <- -b["(Intercept)"] / b["weight"]

# (optional) a new fish to plot
new_fish <- data.frame(length = 32, weight = 230)

ggplot(fish, aes(length, weight, color = species)) +
  geom_point(size = 3, alpha = 0.8) +
  # learned decision boundary (prob = 0.5)
  geom_abline(
    intercept = intercept_logit, slope = slope_logit,
    linetype = "dashed", linewidth = 1, color = "black",
    inherit.aes = FALSE
  ) +
  # new fish marker (optional)
  geom_point(data = new_fish, aes(length, weight),
             shape = 8, size = 5, color = "black", stroke = 1.2, inherit.aes = FALSE) +
  annotate("text", x = new_fish$length + 0.5, y = new_fish$weight + 5,
           label = "new fish?", hjust = 0) +
  labs(
    x = "Length (cm)", y = "Weight (g)", color = "Species",
    title = "Learned decision boundary"
  ) +
  theme_classic(base_size = 14)

```

There are some problems with this approach:

-  the output values are never (with probability 1) exactly 0 or 1, the values that are taken by $y$;
-  even considering a threshold (e.g. 0.5) to classify the predicted values into 0 or 1, this would be mostly an arbitrary chosen value.

## Sigmoid Functions

Instead of a linear function, we can opt to model the *probability* of success of the event described by $y$, i.e $\operatorname{P}(Y=1)$. This is a more sound and natural way to model the prediction of a random binary event. In order to do this, we can use **sigmoid functions**. A sigmoid function $f(x)$ is a function having an "S" shaped curve (sigmoid curve) with the following properties:

-   $0<f(x)<1$;
-   $\displaystyle \lim_{x \to -\infty} f(x) = 0$, $\displaystyle \lim_{x \to +\infty} f(x) = 1$;
-   $f'(x)>0$.

A common sigmoid function is the logistic function, defined as:

$$
f(x) = \frac{e^x}{1 + e^x} = \frac{1}{1 + e^{-x}}.
$$

A logistic function can be used to model the probability that $y = 1$ given $\mathbf{x}$.

```{webr-r}
#| autorun: true
ggplot(tibble(x, y), aes(x, y)) +
  geom_point() +
  geom_smooth(method = "glm", 
       method.args = list(family = "binomial"), 
       se = FALSE) +
  labs(title = "",
       x = "x",
       y = "y")
  
```

Based on the plot, we are seeing that each dot is a fish measured by *length* and *weight*; colors show the true species. 

Now, imagine a new fish appears. We measure its length and weight. In the plot, the black star is a new fish. Where it lands on the plot will "look closer" to one group or the other.

Vary natural idea is
> Let us just draw a line that separates salmon from sea bass.

The dashed line is the classifier's **decision boundary**-- the set of points where the model is 50-50 between the two classes.

So at this stage you can think:

- A classifier is something that finds a good decision boundary in the dataset.
- If a new point falls on one side, we say class 1. On the other side, it belongs to class 0. 

This is already a valid mental model of classification. 

Sometimes we do not want the answer.

We might see:

- Above or right of the line: the model predicts *salmon* (class 1)
- Below or left of the line: the model predicts see bass (class 0)

Because it sits below the boundary, the model predicts *sea bass* -- but notice it is close to the line, so confidence is only moderate. Far from the line the model is more confident; near the line it is less sure. 

The picture captures the core idea of classification: a model learns a good boundary from labeled data and uses it to decide the class of new points. Logistic regression is one such model that also gives a probability for class 1, with the boundary marking the 50% line.


This function will play a central row in the logistic regression model, as we will see briefly.

## Logistic Model

The first random variable that comes to mind in order to model a random binary event is the **Bernoulli random variable**, which is a discrete random variable that takes value 1 with probability $p$ and value 0 with probability $1-p$. The parameter $p$ is the probability of success of the event described by the random variable, and it's probability function is

$$
p(y) = p^y (1-p)^{1-y}, \quad y \in \{0, 1\}.
$$
The expected value of a Bernoulli random variable is $\operatorname{E}[Y] = p$ and it's variance is $\operatorname{V}(Y) = p(1-p)$.

::: callout-note
A natural extension is the **Binomial random variable**, which is the sum of $n$ independent Bernoulli random variables with the same parameter $p$. The parameter $p$ is again the probability of success of the event described by the random variable, and it's probability function is

$$
p(y) = \binom{n}{y} p^y (1-p)^{n-y}, \quad y \in \{0, 1, ..., n\}.
$$
:::

Another way of expressing the probability of an event is through the **odds** of the event, defined as the ratio between the probability of the event and the probability of it's complementary event:

$$
\operatorname{odds} = \frac{p}{1-p} = \frac{1}{1-p^{-1}}, \quad p \in ]0, 1[.
$$

The odds are a number between 0 and $+\infty$. If the odds are equal to 1, then the event and it's complementary are equally likely. If the odds are greater than 1, then the event is more likely than it's complementary event, and vice versa. It's easy to see that odds are monotonically increasing with respect to $p$:

$$
{(\operatorname{odds})}' = \frac{1}{(1-p)^2} > 0.
$$

```{webr-r}
#| autorun: true
odds <- function(p) p / (1 - p)
p <- seq(0.1, 0.9, length.out = 90)
tibble(p, odds = odds(p)) %>%
  ggplot(aes(p, odds)) +
  geom_line() +
  geom_vline(xintercept = 0.5, color = "red", linetype = "dashed") +
  geom_hline(yintercept = 1, color = "blue", linetype = "dashed") +
  annotate("text", x = 0.52, y = max(odds(p))*0.8, label = "x = 0.5", color = "red", hjust = 0) +
  annotate("text", x = 0.8, y = 1.05, label = "y = 1", color = "blue", vjust = 0) +
  labs(title = "Odds as a function of probability",
       x = "Probability",
       y = "Odds")
```

Odds are an alternative way of expressing probabilities, still  being relatively intuitive concerning interpretation. Nevertheless, they are still confined to the interval $]0, +\infty[$, because

$$
\lim_{p \to 0^+} \frac{1}{1-p^{-1}} = 0, \quad \lim_{p \to 1^-} \frac{1}{1-p^{-1}} = +\infty.
$$

If we want to have a represantation of probabilities that spans the whole real line, we can use the **log-odds**, also called **logit** function, defined as

$$
\operatorname{logit}(p) = \log\left(\frac{p}{1-p}\right), \quad p \in ]0, 1[.
$$

This creates a one-to-one mapping between the interval $]0, 1[$ and the whole real line $\mathbb{R}$.

```{webr-r}
#| autorun: true
logit <- function(p) log(p / (1 - p))
prob <- seq(0.01, 0.99, length.out = 99)
tibble(prob, logit = logit(prob)) %>%
  ggplot(aes(prob, logit)) +
  geom_line() +
  geom_vline(xintercept = 0.5, color = "red", linetype = "dashed") +
  geom_hline(yintercept = 0, color = "blue", linetype = "dashed") +
  annotate("text", x = 0.52, y = max(logit(prob))*0.8, label = "x = 0.5", color = "red", hjust = 0) +
  annotate("text", x = 0.8, y = 0.05, label = "y = 0", color = "blue", vjust = 0) +
  labs(title = "Logit as a function of probability",
       x = "Probability",
       y = "Logit")
```

The logit function is also monotonically increasing with respect to $p$:

$$
{\big(\operatorname{logit}(p)\big)}' = \frac{1}{p(1-p)} > 0.
$$

This representation of probabilities is well suited to be used in a regression model, because it spans the whole real line. In fact, in a logistic regression model, we assume that the log-odds of the probability that $y = 1$ is a linear combination of the input variables:
$$
\operatorname{logit}\big(\operatorname{P}(Y=1|\mathbf{x})\big) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p = \mathbf{x}^T \boldsymbol{\beta} = \eta (\mathbf{x}),
$$
where $\boldsymbol{\beta} = (\beta_0, \beta_1, ..., \beta_p)^T$ is the vector of parameters to be estimated.

Note that the logit function is the inverse of the logistic function! Hence, we can rewrite $\operatorname{P}(Y=1|\mathbf{X}=\mathbf{x})$ as

$$
\operatorname{P}(Y=1|\mathbf{x}) = \frac{e^{\eta(\mathbf{x})}}{1 + e^{\eta(\mathbf{x})}} = \frac{1}{1 + e^{-\eta(\mathbf{x})}}.
$$ {#eq-logit}

::: callout-warning
@eq-logit describes a model for the expected value of the random variable $Y$ given $\mathbf{x}$, that follows a Bernoulli distribution with parameter $p = \operatorname{P}(Y=1|\mathbf{x})$. It does **not** describe a model for $Y$ itself, which is a discrete random variable taking values 0 or 1.
:::
