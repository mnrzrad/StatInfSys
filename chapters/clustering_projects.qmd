---
engine: knitr
filters: 
  - webr
---

```{r setup, include=FALSE}
required_packages <- c("cluster", "lsa", "GGally", "factoextra", "stats", "tibble", 'dplyr', 'kmed', 'hopkins', 'fpc', 'mclust', 'clValid', "jpeg", "ggplot2", "grid", "tidyverse", "gridExtra", 'imager')

for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, repos = "https://cloud.r-project.org")
    library(pkg, character.only = TRUE)
  }
}
```

# Projects


## Image Clustering and Compression

The goal of this analysis is to **simplify an image** by reducing the number of distinct colors while preserving its overall appearance.  

In other words, we are using **clustering** to group similar pixels in the RGB color space so that all pixels within a cluster share a representative color (the **centroid** or **medoid**).  

This technique:

- *Reduces file size* (useful for compression),
- *Highlights dominant colors* (helpful in computer vision, design, and pattern recognition),
- *Illustrates how clustering groups similar observations* — here, similar colors.

So, clustering acts like a smart palette optimizer: it finds the key "color families" that best describe the image.

### Load and Visualize the Image 

Here, we use a small flower image. 

:::{.callout-note}
You can replace the URL with any image (small then 1 MB).
:::

```{webr-r}
#| label: img-load 
#| autorun: false 
#| out-width: "80%"
#| message: false
#| warning: false
library(jpeg)
library(ggplot2)
library(dplyr)

# Download an example image 
url <- "https://images.pexels.com/photos/1661179/pexels-photo-1661179.jpeg"

temp <- tempfile(fileext = ".jpg")
download.file(url, temp, mode = 'wb')
img <- readJPEG(temp)

# Show the original image 
grid::grid.raster(img)
```


```{webr-r}
#| label: img-convert_data
#| autorun: false
#| message: true
#| warning: true
# Convert to data frame of RGB values 
# img_df <- data.frame(
#     R = as.vector(img[,,1]),
#     G = as.vector(img[,,2]),
#     B = as.vector(img[,,3])
#     )

# dim(img_df)

# library(imager)
# img <- as.cimg(img)
# img_small <- imresize(img, scale = 0.45)  # adjust scale until ~10k pixels

# img_df <- data.frame(
#   R = as.vector(img_small[,,1]),
#   G = as.vector(img_small[,,2]),
#   B = as.vector(img_small[,,3])
# )
# dim(img_df)
# Obtain the dimension
imgDm <- dim(img)
imgDm

# Assign RGB channels to data frame
imgRGB <- data.frame(
  x = rep(1:imgDm[2], each = imgDm[1]),
  y = rep(imgDm[1]:1, imgDm[2]),
  R = as.vector(img[,,1]),
  G = as.vector(img[,,2]),
  B = as.vector(img[,,3])
  )

dim(imgRGB)

rgb_mat <- imgRGB %>%
   select(R, G, B)

# Scale RGB so R/G/B have equal weight
rgb_scaled <- scale(rgb_mat)
```

Before applying clustering, let us look at what our image data actually looks like. 

When we read the image, each pixel is represented by three values — **Red (R)**, **Green (G)**, and **Blue (B)** — that together define its color. So, an image becomes a dataset where **each row = one pixel**, and the columns are **R**, **G**, and **B**. This is why we can treat image processing as a **clustering problem** — we want to group pixels with *similar colors*.

```{webr-r}
#| label: img-df-preview-1
#| autorun: false
#| message: false
#| warning: false
head(img_df)
```

```{webr-r}
#| label: img-df-preview-2
#| autorun: false
#| message: false
#| warning: false
summary(img_df)
```

Now that we understand what the data looks like (each pixel is a point in RGB space), our next goal is to **group pixels with similar colors** into clusters.

We will start with $k$-means and later, will do $k$-medoids. At the end, we will *compare both methods*.

### $k$-means

#### Choosing the Number of clusters 

We already know three standard methods for determining the best $k$:

- *Elbow Method:* looks at how the *within-cluster sum of squares (WSS)* decreases with increasing $k$; the *elbow* marks a good balance between simplicity and accuracy.  
- *Silhouette Coefficient:* evaluates how well each point fits within its cluster; the closer to $1$, the better.  
- *Gap Statistic:* compares within-cluster dispersion to that of random uniform data.

We use all three to guide our choice of $k$.

Because clustering every pixel in a large image is computationally heavy and memory-intensive, we can analyze only a random subset of pixels that still represents the overall color diversity.

In clustering, what matters most is diversity in the data, not volume.
A well-chosen random sample gives us the same structure at a fraction of the cost.
Once we know how many clusters we need, we can apply that to all pixels for the final image.

If our image has many similar pixels (which most real images do), then sampling $5\,000$ pixels is enough to capture all the dominant color regions. If the image has tiny regions of rare colors (e.g., coral reef photos, satellite imagery), a small sample might miss some color groups. In this case, use a larger sample (e.g., $20\,000$).

Here, we use a sampled dataset (`sample_img`) to decide the optimal $k$.

```{webr-r}
#| label: kmeans-elbow-plot
#| autorun: false
#| message: true
#| warning: true
#| out-width: "80%"
library(factoextra)

set.seed(123)

# Take a random sample of 5000 pixels
# n_sample <- 5000
# sample_img <- img_df[sample(1:nrow(img_df), n_sample), ]

fviz_nbclust(img_df, kmeans, method = "wss", k.max = 6) +
  ggtitle("Elbow Method — Optimal Number of Colors")
```

Based on this plot, we can see that WSS decreases sharply until around $k = 3$, after which the improvement flattens. So, the "elbow" suggests that $3$ clusters capture most of the color variance efficiently.
```{webr-r}
#| label: kmeans-silhouette-plot
#| autorun: false
#| message: false
#| warning: false
#| out-width: "80%"
set.seed(123)
fviz_nbclust(sample_img, kmeans, method = "silhouette", k.max = 6) +
  ggtitle("Silhouette Method — Cluster Quality")
```
The average silhouette width peaks at $k = 2$, meaning that clusters are most compact and well-separated at this point. However, the drop from $k = 2$ to $k = 3$ is small, and with images (continuous color gradients), slightly lower silhouette scores are expected for more nuanced color palettes.
```{webr-r}
#| label: kmeans-gap_statistic-plot
#| autorun: false
#| message: false
#| warning: false
#| out-width: "80%"
set.seed(123)
# n_sample <- 2000
# sample_img <- img_df[sample(1:nrow(img_df), n_sample), ]

fviz_nbclust(img_df, kmeans, method = "gap_stat", nboot = 20, k.max = 6) +
  ggtitle("Gap Statistic — Cluster Quality")
```
The gap statistic reaches its maximum around $k = 4$, meaning that $4$ clusters explain the data structure significantly better than a random uniform reference.

**What do we choose?**

When methods disagree slightly, *Elbow: 3 clusters, Silhouette: 2 clusters, Gap-Statistic: 4 clusters*, a reasonable consensus is to choose $k = 3$ or $k = 4$, depending on how much color detail we want in the compressed image. Choosing $k = 3$ gives smoother and more general color areas. And, $k = 4$ preserves finer details that is more realistic for natural photos. 

In the following code, we fit two $k$-means models and reconstruct the compressed images and show them side-by-side to see how the number of clusters affects color richness and smoothness.
```{webr-r}
#| label: compare-kmeans-3-4
#| autorun: false
#| message: false
#| warning: false
#| fig-cap: "Comparison of K-Means compression with k = 3 and k = 4"
#| out-width: "80%"
library(ggplot2)
library(gridExtra)


set.seed(123)
# Step 1: Optionally downscale large images to save memory 
if (dim(img)[1] > 400) {
img <- img[seq(1, dim(img)[1], by = 2), seq(1, dim(img)[2], by = 2), ]
}
# Step 2: Convert image to RGB data frame 
img_df <- data.frame(
R = as.vector(img[,,1]),
G = as.vector(img[,,2]),
B = as.vector(img[,,3])
)
# Step 3: Fit K-Means on the full (or reduced) image 
km3 <- kmeans(img_df, centers = 3, nstart = 10)
km4 <- kmeans(img_df, centers = 4, nstart = 10)

# Step 4: Reconstruct compressed image keeping spatial layout 
compress_full <- function(km_model, img_full) {
centers <- km_model$centers
cluster_colors <- centers[km_model$cluster, ]
img_array <- array(cluster_colors, dim = dim(img_full))
as.raster(img_array)
}

compressed_3 <- compress_full(km3, img)
compressed_4 <- compress_full(km4, img)

# Step 5: Plot side by side
p1 <- ggplot() + annotation_raster(compressed_3, 0, 1, 0, 1) +
ggtitle("K-Means Compression (k = 3)") + theme_void()
p2 <- ggplot() + annotation_raster(compressed_4, 0, 1, 0, 1) +
ggtitle("K-Means Compression (k = 4)") + theme_void()

gridExtra::grid.arrange(p1, p2, ncol = 2)
```
In the original image, each pixel's RGB value is treated as a 3D point in color space. In this output, each pixel is replaced by its cluster's centroid color. 

The image based on $k = 3$ in the left shows an image that is compressed into just 3 colors and it looks a poster with strong color bands and high compression. In the right image (i.e., choosing $k = 4$), a bit more detail and smoother gradients appear, especially in the faces and background, but still simplified compared to the original. 

Increasing $k$ adds more colors which results in higher visual quality but less compression. 

**At what value of k does the image start to look realistic again?**

```{webr-r}
#| label: compare-k-values
#| autorun: false
#| message: false
#| warning: false
#| fig-cap: "Visual comparison of K-Means compression for different k values"
#| out-width: "90%"

library(ggplot2)
library(gridExtra)

compress_full <- function(k, img_data, img_shape) {
km <- kmeans(img_data, centers = k, nstart = 10)
cluster_colors <- km$centers[km$cluster, ]
img_array <- array(cluster_colors, dim = img_shape)
as.raster(img_array)
}

k_values <- c(3, 4, 5, 6, 8, 10, 12, 15)
plots <- list()

for (i in seq_along(k_values)) {
k <- k_values[i]
compressed_img <- compress_full(k, img_df, dim(img))
plots[[i]] <- ggplot() +
annotation_raster(compressed_img, 0, 1, 0, 1) +
ggtitle(paste("K-Means (k =", k, ")")) +
theme_void()
}

gridExtra::grid.arrange(grobs = plots, ncol = 2) 
```

As we see, at $k=3$, the compression is extreme -- only broad regions of color remain, and fine details (like skin tone, clothing patterns, or light gradients) are lost. Around $k = 6-8$, the image starts to look visually realistic again, as the model captures enough distinct color shades to represent lighting, texture, and contrast. Beyond that, improvements become barely noticeable, while file size and computation cost increase.

### $k$-Medoids (PAM)

While $k$-means uses centroids (means of points in a luster), $k$-medoids uses medoids (actual data points that minimize total dissmilarity). This makes $k$-medoids to be *more robust to outliers*, *works with non-Euclidean distances*, although it is more computationally expensive. 

In this example, each pixel is treated as a point in RGB space again but instead of averaging colors (which can produce unrealistic shades), $k$-medoids selects real pixel colors as representives.

```{webr-r}
#| label: kmedoid-elbow-plot
#| autorun: false
#| message: false
#| warning: false
#| out-width: "80%"
library(cluster)
library(factoextra)

set.seed(123)
n_sample <- 5000
sample_img <- img_df[sample(1:nrow(img_df), n_sample), ]

fviz_nbclust(sample_img, pam, method = "wss", k.max = 6) +
  ggtitle("Elbow Method — Optimal Number of Medoids (PAM)")
```

```{webr-r}
#| label: kmedoid-silhouette-plot
#| autorun: false
#| message: false
#| warning: false
#| out-width: "80%"
library(cluster)
library(factoextra)

set.seed(123)
fviz_nbclust(sample_img, pam, method = "silhouette", k.max = 6) +
  ggtitle("Silhouette Method — Optimal Number of Medoids (PAM)")
```

```{webr-r}
#| label: kmedoid-gap_statistic-plot
#| autorun: false
#| message: false
#| warning: false
#| out-width: "80%"
library(cluster)
library(factoextra)

set.seed(123)
n_sample <- 2000
sample_img <- img_df[sample(1:nrow(img_df), n_sample), ]

fviz_nbclust(sample_img, pam, method = "gap_stat", nboot = 20, k.max = 6) +
  ggtitle("Gap Statistic — Optimal Number of Medoids (PAM)")
```

:::{.callout-note}
The *Gap Statistic* is computationally heavy because it repeatedly compares real clustering quality to that of random data. 
For large datasets like images, we either reduce the bootstrap count or rely on faster methods such as the Silhouette coefficient.
:::
The results for $k$-medoids (PAM) show consistent patterns with those obtained from $k$-means. Elbow methods suggests 3 clusters. Silhouette method indicates 2 clusters as the configuration with the most compact and well-separated groups. Gap statistic peaks at 4 clusters. 

Although the exact optimal $k$ differs slightly between methods, all three converge on the same range (2-4 clusters), confirming that the image's dominant colors can be represented effectively with only a few color groups. This agreement also shows that $k$-medoids captures a similar underlying structure as $k$-means, while being more robust to outliers and based on actual data points rather than centroids; so, you should expect more time for seeing the output.

To better understand how the number of clusters affects the representation, we wil test $k = 3$ and $k = 4$ with the PAM algorithm.

Unlike $k$-means (which assumes Euclidean distance), PAM allows any dissimilarity measure. 

For image data (and in general, for colour data), Euclidean usually works well, but we can test Manhattan as well, since it can handle abrupt colour transitions more robustly. 

```{webr-r}
#| label: pam-compare-metrics
#| message: true
#| warning: true
#| fig-cap: "Comparison of K-Medoids (PAM) clustering for different k and distance metrics"

library(cluster)
library(ggplot2)
library(gridExtra)

pam_compress <- function(data, k, metric) {
pam_fit <- pam(data, k = k, metric = metric)
medoid_colors <- pam_fit$medoids
cluster_colors <- medoid_colors[pam_fit$clustering, ]
array(cluster_colors, dim = c(80, 50, 3)) |> as.raster()
}

n_sample <- 6000
sample_img <- img_df[sample(1:nrow(img_df), n_sample), ]

p_eu_4 <- pam_compress(sample_df, 4, "euclidean")
p_ma_3 <- pam_compress(sample_df, 3, "manhattan")
p_eu_3 <- pam_compress(sample_df, 3, "euclidean")
p_ma_4 <- pam_compress(sample_df, 4, "manhattan")

plots <- list(
ggplot() + annotation_raster(p_eu_3, 0, 1, 0, 1) +
ggtitle("PAM (k = 3, Euclidean)") + theme_void(),
ggplot() + annotation_raster(p_eu_4, 0, 1, 0, 1) +
ggtitle("PAM (k = 4, Euclidean)") + theme_void(),
ggplot() + annotation_raster(p_ma_3, 0, 1, 0, 1) +
ggtitle("PAM (k = 3, Manhattan)") + theme_void(),
ggplot() + annotation_raster(p_ma_4, 0, 1, 0, 1) +
ggtitle("PAM (k = 4, Manhattan)") + theme_void()
)

gridExtra::grid.arrange(grobs = plots, ncol = 2)
```

