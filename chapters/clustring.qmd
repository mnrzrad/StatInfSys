---
engine: knitr
filters: 
  - webr
---

```{r setup, include=FALSE}
required_packages <- c("cluster", "lsa", "GGally", "factoextra", "stats", "tibble", 'dplyr')

for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, repos = "https://cloud.r-project.org")
    library(pkg, character.only = TRUE)
  }
}
```

# Clustering

Cluster analysis, or *Clustering*, is a technique used to find groups of objects such that the objects within the same group are similar (or closely realted) to one another, while the objects from different groups are dissimilar (or unrelated).

## Applications of Clustering

Clustering techniques are widely used across multiple disciplines to discover patterns, structure data, and support decision-making.  Below are some common and illustrative applications.

**Customer Segmentation**

In marketing and business analytics, clustering helps identify groups of customers with similar behaviors, preferences, or purchasing habits.  This segmentation allows companies to optimize advertising strategies, personalize product offerings, and design campaigns for specific focus groups.

**Example**: 
Grouping supermarket customers based on purchase frequency, product categories, and spending level.

**Web Visitor Segmentation**

Web analytics platforms use clustering to classify website visitors according to their browsing patterns, time spent on pages, or interaction behaviors. This segmentation supports personalized content delivery and improved user experience.

**Example**: 
Optimizing web navigation or recommendations for distinct *user segments* such as new visitors vs. returning users.

**Data Aggregation and Reduction**

Clustering can be used to represent large datasets by a smaller set of representative elements (centroids or medoids).  
This reduces computational complexity and facilitates data visualization.

**Example**:  
Reducing the color palette of an image to *k representative colors* using algorithms like *k-means*.

**Text Collection Organization**

In natural language processing (NLP), clustering is applied to group similar documents or texts into topics or themes.  
It is particularly useful when the number or nature of topics is *unknown in advance*.

**Example**:  
Grouping news articles, research abstracts, or emails into topic clusters.

**Biology and Taxonomy**

In biological sciences, clustering supports the classification of living organisms based on genetic, morphological, or behavioral similarities.  
It forms the basis for hierarchical structures such as *kingdom, phylum, class, order, family, genus,* and *species*.

**Example**:  
Clustering DNA sequences to identify genetic relationships among species.

**Information Retrieval**

In computer science and library systems, clustering helps organize large document collections to improve search and retrieval efficiency. 
By grouping related documents, search engines can return more *contextually relevant* results.

**Example**:  
Document clustering for semantic search or grouping research papers by field of study.

## Distance and Similarity in Clustering

The notion of *distance* or *similarity* lies at the heart of clustering. Since clustering aims to group similar observations together, we must have a way to measure how close or far two observations are from each other in the feature space. These measures quantify the degree of resemblance (similarity) or difference (dissimilarity) between data points.

> The main goal is to partition a set of data points into clusters where **intra-cluster** distances (distances between points within the same cluster) are **minimized**, and **inter-cluster** distances (distances between points from different clusters) are **maximized**.

Thus, the main goal of clustering can be rewritten as:

> The main goal is to partition a set of data points into clusters where intra-cluster similarities (similarities between points within the same cluster) are maximized, and inter-cluster similarities (similarities between points from different clusters) are minimized.

To avoid confusion, remember that *distance* and *similarity* are inverse concepts: when two objects are close (small distance), they are considered similar (high similarity).  
The table below summarizes their relationship and how each measure behaves in clustering.

| **Concept** | **Meaning** | **High Value Indicates** | **Low Value Indicates** | **Goal in Clustering** |
|:-------------|:------------|:--------------------------|:--------------------------|:------------------------|
| **Distance** | A measure of dissimilarity between two objects | Objects are far apart (dissimilar) | Objects are close together (similar) | **Minimize intra-cluster distances** and **maximize inter-cluster distances** |
| **Similarity** | A measure of closeness or resemblance between two objects | Objects are close together (similar) | Objects are far apart (dissimilar) | **Maximize intra-cluster similarities** and **minimize inter-cluster similarities** |

```{r}
#| label: similarity-vs-distance
#| fig-cap: "Relationship between distance and similarity — points in the same cluster have high similarity (low distance)."
#| echo: false
#| message: false
#| warning: false

library(ggplot2)
set.seed(123)

# Generate two clusters

n <- 30
cluster1 <- data.frame(x = rnorm(n, 2, 0.3), y = rnorm(n, 2, 0.3), cluster = "Cluster A")
cluster2 <- data.frame(x = rnorm(n, 6, 0.3), y = rnorm(n, 6, 0.3), cluster = "Cluster B")
df <- rbind(cluster1, cluster2)

# Pick two points to illustrate distance and similarity

p1 <- df[5, ]
p2 <- df[6, ]
p3 <- df[40, ]

# Plot

ggplot(df, aes(x, y, color = cluster)) +
geom_point(size = 3) +
geom_segment(aes(x = p1$x, y = p1$y, xend = p2$x, yend = p2$y),
color = "darkgreen", linewidth = 1.2,
arrow = arrow(length = unit(0.15, "inches"), ends = "both")) +
geom_segment(aes(x = p1$x, y = p1$y, xend = p3$x, yend = p3$y),
color = "red3", linewidth = 1.2,
arrow = arrow(length = unit(0.15, "inches"), ends = "both")) +
annotate("text", x = (p1$x + p2$x)/2, y = (p1$y + p2$y)/2 + 0.99,
label = "Intra-Cluster\nHigh similarity\n(Low distance)", color = "darkgreen", size = 4) +
annotate("text", x = (p1$x + p3$x)/2, y = (p1$y + p3$y)/2 + 0.9,
label = "Inter-Cluster\nLow similarity\n(High distance)", color = "red3", size = 4) +
theme_minimal() +
labs(title = "Distance vs Similarity",
subtitle = "Closer points → higher similarity (smaller distance)")
```

A **distance metric** $d(\mathbf{x}, \mathbf{y})$ measures the dissimilarity between two points $\mathbf{x}=(x_1, x_2, \ldots, x_p)$ and $\mathbf{y} = (y_1, y_2, \ldots, y_p)$.  Formally, a function $d: X \times X \rightarrow \mathbb{R}$ is a *metric* if it satisfies:
$$
\begin{aligned}
1.\;& d(\mathbf{x}, \mathbf{y}) \ge 0 && \text{(non-negativity)} \\
2.\;& d(\mathbf{x}, \mathbf{y}) = 0 \iff \mathbf{x} = \mathbf{y} && \text{(identity)} \\
3.\;& d(\mathbf{x}, \mathbf{y}) = d(\mathbf{y}, \mathbf{x}) && \text{(symmetry)} \\
4.\;& d(\mathbf{x}, \mathbf{y}) \le d(\mathbf{x}, \mathbf{z}) + d(\mathbf{z}, \mathbf{y}) && \text{(triangle inequality)}
\end{aligned}
$$

A **similarity measure** $s(\mathbf{x}, \mathbf{y})$ expresses how close or related two points are.  
It usually satisfies:
$$
\begin{aligned}
1.\;& s(\mathbf{x}, \mathbf{y}) \ge 0 && \text{(non-negativity)} \\
2.\;& s(\mathbf{x}, \mathbf{y}) = s(\mathbf{y}, \mathbf{x}) && \text{(symmetry)} \\
3.\;& s(\mathbf{x}, \mathbf{y}) \in [0, 1] && \text{(boundedness)} \\
4.\;& s(\mathbf{x}, \mathbf{x}) = 1 && \text{(maximum self-similarity)}
\end{aligned}
$$
A simple transformation links both concepts:
$$
s(\mathbf{x}, \mathbf{y}) = \frac{1}{1 + d(\mathbf{x}, \mathbf{y})}, \quad \text{where } s(\mathbf{x}, \mathbf{y}) \in [0, 1].
$$

### Common Distance Metrics

Different clustering algorithms may behave very differently depending on the metric used.  
Below are the most widely used **distance measures**.

#### **Euclidean Distance**

The most common metric, representing the straight-line distance between two points in $\mathbb{R}^p$:
$$
d_E(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{p} (x_i - y_i)^2}
$$

:::{.callout-note}
Euclidean distance is sensitive to scale; therefore, variables should usually be standardized before applying it.
:::

```{webr-r}
#| label: euclidean-distance
#| autorun: true 
#| warning: false 
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# Euclidean distance
d_E <- sqrt(sum((x - y)^2))
cat("Euclidean distance using direct function:", d_E, "\n")

# Another way
df <- data.frame(rbind(x, y))
dist_euclidean <- dist(df, method = 'euclidean')
cat("Euclidean distance using 'dist()' function:", dist_euclidean, "\n")

# Using the package "factoextra" 
library(factoextra)
dist_euclidean_factoextra <- get_dist(df, method = 'euclidean')
cat("Euclidean distance using 'factoextra::get_dist()' function:", dist_euclidean_factoextra)
```

#### **Manhattan (or City-Block) Distance**
This distance sums the absolute differences across all coordinates:
$$
d_M(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{p} |x_i - y_i|
$$

:::{.callout-note}
The Manhattan distance is more robust to outliers and suitable when features represent grid-like or discrete steps.
:::

```{webr-r}
#| label: manhattan-distance
#| autorun: true
#| warning: false
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# Manhattan distance
d_M <- sum(abs(x - y))
cat("Manhattan distance using direct function:", d_M, "\n")

# Another way
df <- data.frame(rbind(x, y))
dist_manhattan <- dist(df, method = 'manhattan')
cat("Manhattan distance using 'dist()' function:", dist_manhattan, "\n")

# Using the package "factoextra" 
library(factoextra)
dist_manhattan_factoextra <- get_dist(df, method = 'manhattan')
cat("Manhattan distance using 'factoextra::get_dist()' function:", dist_manhattan_factoextra)
```

```{r}
#| label: distance-compare
#| fig-cap: "Comparison of Euclidean and Manhattan distance between two points"
#| echo: false
#| message: false
#| warning: false
#| out-width: "80%"

library(ggplot2)

# Define two points
A <- c(1, 1)
B <- c(5, 5)

# Create a stepwise Manhattan path (red, geometric)
manhattan_path <- data.frame(
  x = c(1, 2, 2, 3, 3, 4, 4, 5, 5),
  y = c(1, 1, 2, 2, 3, 3, 4, 4, 5)
)

# Calculate distances (algebraic)
d_euclidean <- sqrt((B[1] - A[1])^2 + (B[2] - A[2])^2)
d_manhattan <- abs(B[1] - A[1]) + abs(B[2] - A[2])

# Build the plot
ggplot() +
  # Geometric Manhattan path (red, stepwise)
  geom_path(data = manhattan_path, aes(x, y), color = "red", linewidth = 1.3) +
  # Algebraic Manhattan path (green L-shape)
  geom_segment(aes(x = A[1], y = A[2], xend = A[1], yend = B[2]),
               color = "green3", linewidth = 1.3) +
  geom_segment(aes(x = A[1], y = B[2], xend = B[1], yend = B[2]),
               color = "green3", linewidth = 1.3) +
  # Euclidean (straight line)
  geom_segment(aes(x = A[1], y = A[2], xend = B[1], yend = B[2]),
               color = "blue", linewidth = 1.3) +
  # Points A and B
  geom_point(aes(x = A[1], y = A[2]), size = 4, color = "black") +
  geom_point(aes(x = B[1], y = B[2]), size = 4, color = "black") +
  # Labels for points
  annotate("text", x = A[1]-0.3, y = A[2]-0.3, label = "A", fontface = "bold", size = 5) +
  annotate("text", x = B[1]+0.3, y = B[2]+0.3, label = "B", fontface = "bold", size = 5) +
  # Axes and grid
  scale_x_continuous(breaks = 0:6, limits = c(0,6)) +
  scale_y_continuous(breaks = 0:6, limits = c(0,6)) +
  coord_equal() +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_line(color = "gray90")) +
  # Text annotations for distances
  annotate("text", x = 1, y = 5.8, label = paste0("Euclidean = ", round(d_euclidean, 2)),
           color = "blue", hjust = 0, size = 4) +
  annotate("text", x = 1, y = 5.4, label = paste0("Manhattan = ", round(d_manhattan, 2)),
           color = "red", hjust = 0, size = 4) +  
  # Legend substitute
  annotate("text", x = 1, y = 0.5, label = "Red = stepwise Manhattan", color = "red", hjust = 0, size = 4) +
  annotate("text", x = 1, y = 0.1, label = "Green = L-path Manhattan", color = "green3", hjust = 0, size = 4) +
  annotate("text", x = 1.1, y = 4.5, label = "Blue = Euclidean", color = "blue", hjust = 0, size = 4) +
  labs(title = "Euclidean vs Manhattan Distance (Two Interpretations)",x = "", y = "")
```

#### **Minkowski Distance**

A generalization of Euclidean and Manhattan distances, controlled by a parameter $r$:
$$
d_r(\mathbf{x}, \mathbf{y}) = \left( \sum_{i=1}^{p} |x_i - y_i|^r \right)^{1/r}
$$

- When $r = 1$, it becomes Manhattan distance. 
   
- When $r = 2$, it becomes Euclidean distance.  

- Larger values of $r$ emphasize large coordinate differences.

```{webr-r}
#| label: manhattan-distance
#| autorun: true
#| warning: false
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# Minkowski distance
d_r <- function(x, y, r) {
  (sum(abs(x - y)^r))^(1/r)
}
cat('Manhattan distance:', d_r(x, y, r = 1), 'that should be equal to d_M which is', d_M, '\n')
cat('Euclidean distance:', d_r(x, y, r = 2), 'that should be equal to d_E which is', d_E, '\n')
cat('Minkowski Distance', d_r(x, y, r = 5), '\n')
cat('Minkowski Distance', d_r(x, y, r = 10), '\n')

# Another way
df <- data.frame(rbind(x, y))
dist_minkowski_5 <- dist(df, method = 'minkowski', p = 5)
dist_minkowski_10 <- dist(df, method = 'minkowski', p = 10)
cat("Minkowski distance using 'dist()' function (p = 5):", dist_minkowski_5, '\n')
cat("Minkowski distance using 'dist()' function (p = 10):", dist_minkowski_10, '\n')


# Using the package "factoextra" 
library(factoextra)
dist_minkowski_5_factoextra <- get_dist(df, method = 'manhattan', p = 5)
cat("Minkowski distance using 'factoextra::get_dist()' function:", dist_minkowski_5_factoextra, '\n') 

dist_minkowski_10_factoextra <- get_dist(df, method = 'manhattan', p = 10)
cat("Minkowski distance using 'factoextra::get_dist()' function:", dist_minkowski_10_factoextra) 
```

```{r}
#| label: minkowski-distance
#| fig-cap: "Shapes of Minkowski (Lp) distances around a point for different p values"
#| echo: false
#| message: false
#| warning: false
#| out-width: "80%"

#| label: minkowski-distance-path
#| fig-cap: "Minkowski distance (Lp) path between two points A and B"
#| echo: true
#| message: false
#| warning: false

#| label: minkowski-comparison
#| fig-cap: "Comparison of Minkowski (Lp) distances between points A and B for various p values"
#| echo: true
#| message: false
#| warning: false

library(ggplot2)

# Define points
A <- c(1, 1)
B <- c(5, 5)

# Set of p values to compare
p_values <- c(1, 2, 5, 10)
colors <- c("red", "blue", "purple", "darkgreen")

# Compute Minkowski distances
dx <- abs(B[1] - A[1])
dy <- abs(B[2] - A[2])
distances <- sapply(p_values, function(p) (dx^p + dy^p)^(1/p))
names(distances) <- paste0("p = ", p_values)

# Build the plot
g <- ggplot() +
  # Points A and B
  geom_point(aes(x = A[1], y = A[2]), size = 4, color = "black") +
  geom_point(aes(x = B[1], y = B[2]), size = 4, color = "black") +
  annotate("text", x = A[1]-0.3, y = A[2]-0.3, label = "A", fontface = "bold", size = 5) +
  annotate("text", x = B[1]+0.3, y = B[2]+0.3, label = "B", fontface = "bold", size = 5) +
  # Euclidean path (p=2, blue straight line)
  geom_segment(aes(x = A[1], y = A[2], xend = B[1], yend = B[2]),
               color = "blue", linewidth = 1.3) +
  # Manhattan (p=1, L-path)
  geom_segment(aes(x = A[1], y = A[2], xend = A[1], yend = B[2]),
               color = "red", linewidth = 1.3) +
  geom_segment(aes(x = A[1], y = B[2], xend = B[1], yend = B[2]),
               color = "red", linewidth = 1.3)

# Add Minkowski curves for p = 5, 10 (smooth approximations)
curve_positions <- data.frame(
  p = rep(c(5, 10), each = 100),
  t = rep(seq(0, 1, length.out = 100), 2)
)

# Generate smooth Minkowski-like curves (not exact, illustrative)
curve_positions <- within(curve_positions, {
  x <- A[1] + (B[1] - A[1]) * t
  y <- A[2] + (B[2] - A[2]) * (t^(1/p))
  col <- ifelse(p == 5, "purple", "darkgreen")
})

g <- g +
  geom_path(data = subset(curve_positions, p == 5), aes(x, y), color = "purple", linewidth = 1.3) +
  geom_path(data = subset(curve_positions, p == 10), aes(x, y), color = "darkgreen", linewidth = 1.3) +
  # Axes and style
  scale_x_continuous(breaks = 0:6, limits = c(0, 6)) +
  scale_y_continuous(breaks = 0:6, limits = c(0, 6)) +
  coord_equal() +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_line(color = "gray90")) +
  # Legend substitute
  annotate("text", x = 3.5, y = 2.9, label = "p = 1 (Manhattan)", color = "red", hjust = 0, size = 4) +
  annotate("text", x = 3.5, y = 2.6, label = "p = 2 (Euclidean)", color = "blue", hjust = 0, size = 4) +
  annotate("text", x = 3.5, y = 2.3, label = "p = 5", color = "purple", hjust = 0, size = 4) +
  annotate("text", x = 3.5, y = 2.0, label = "p = 10", color = "darkgreen", hjust = 0, size = 4) +
  labs(title = "Minkowski Distance Between A and B for Different p",
       subtitle = paste("Distances:",
                        paste(names(distances), round(distances, 2), collapse = " | ")),
       x = "", y = "")

g

```

#### Chebyshev Distance

Also known as the $L_\infty$ norm, this distance takes the largest coordinate difference:
$$
d_C(\mathbf{x}, \mathbf{y}) = \max_i |x_i - y_i|
$$
It measures how far apart two points are along the dimension where they differ most.
```{webr-r}
#| label: chebyshev-distance
#| autorun: true
#| warning: false
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# Chebyshev distance
d_C <- max(abs(x - y))
cat("Chebyshev distance using direct function:", d_C, "\n")

# Another way
df <- data.frame(rbind(x, y))
dist_chebyshev <- dist(df, method = 'maximum')
cat("Chebyshev distance using 'dist()' function:", dist_chebyshev, "\n")

# Using the package "factoextra" 
library(factoextra)
dist_chebyshev_factoextra <- get_dist(df, method = 'maximum')
cat("Chebyshev distance using 'factoextra::get_dist()' function:", dist_chebyshev_factoextra)
```

```{r}
#| label: chebyshev-vs-euclidean-manhattan
#| fig-cap: "Comparison of Chebyshev, Euclidean, and Manhattan distances between A and B"
#| echo: false
#| message: false
#| warning: false

library(ggplot2)

# Define the points
A <- c(1, 1)
B <- c(5, 3)

# Calculate distances
dx <- abs(B[1] - A[1])
dy <- abs(B[2] - A[2])
d_manhattan <- dx + dy
d_euclidean <- sqrt(dx^2 + dy^2)
d_chebyshev <- max(dx, dy)

# Build the plot
ggplot() +
  # Manhattan path (red L-shape)
  geom_segment(aes(x = A[1], y = A[2], xend = A[1], yend = B[2]),
               color = "red", linewidth = 1.3) +
  geom_segment(aes(x = A[1], y = B[2], xend = B[1], yend = B[2]),
               color = "red", linewidth = 1.3) +
  
  # Euclidean path (blue diagonal)
  geom_segment(aes(x = A[1], y = A[2], xend = B[1], yend = B[2]),
               color = "blue", linewidth = 1.3) +
  
  # Chebyshev path (green square)
  geom_segment(aes(x = A[1], y = A[2], xend = B[1], yend = A[2]),
               color = "green3", linetype = "dashed", linewidth = 1.2) +
  # geom_segment(aes(x = B[1], y = A[2], xend = B[1], yend = B[2]),
  #              color = "green3", linetype = "dashed", linewidth = 1.2) +
  
  # Points A and B
  geom_point(aes(x = A[1], y = A[2]), size = 4, color = "black") +
  geom_point(aes(x = B[1], y = B[2]), size = 4, color = "black") +
  
  # Labels for points
  annotate("text", x = A[1]-0.3, y = A[2]-0.3, label = "A", fontface = "bold", size = 5) +
  annotate("text", x = B[1]+0.3, y = B[2]+0.3, label = "B", fontface = "bold", size = 5) +
  
  # Axes and grid
  scale_x_continuous(breaks = 0:6, limits = c(0,6)) +
  scale_y_continuous(breaks = 0:6, limits = c(0,6)) +
  coord_equal() +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_line(color = "gray90")) +
  
  # Text annotations for distances
  annotate("text", x = 1.0, y = 5.8,
           label = paste0("Chebyshev (L∞) = ", d_chebyshev),
           color = "green3", hjust = 0, size = 4) +
  annotate("text", x = 1.0, y = 5.4,
           label = paste0("Euclidean (L2) = ", round(d_euclidean, 2)),
           color = "blue", hjust = 0, size = 4) +
  annotate("text", x = 1.0, y = 5.0,
           label = paste0("Manhattan (L1) = ", d_manhattan),
           color = "red", hjust = 0, size = 4) +
  
  labs(title = "Chebyshev vs Euclidean vs Manhattan Distance",
       subtitle = "Comparison of geometric interpretations for two points",
       x = "", y = "")
```

#### **Mahalanobis Distance**

A scale-invariant distance that accounts for correlations between variables:
$$
d_{Mah}(\mathbf{x}, \mathbf{y}) = \sqrt{(x - y)^{\top} \boldsymbol{\Sigma}^{-1} (x - y)}
$$
where $\boldsymbol{\Sigma}$ is the covariance matrix of the data.
Two points with the same Mahalanobis distance from the mean are equally likely under a multivariate normal model, regardless of the variable scales or correlations.

```{webr-r}
#| label: mahalanobis-distance
#| autorun: true
#| warning: false
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# For Mahalanobis, we need covariance matrix (example with small dataset)
X <- matrix(c(2, 4, 5, 8, 3, 7), ncol = 2, byrow = TRUE)
cov_matrix <- cov(X)
d_mahalanobis <- sqrt(t(x - y) %*% solve(cov_matrix) %*% (x - y))
d_mahalanobis
```

:::{.callout-tip}

**Choosing an Appropriate Metric**

The choice of metric depends on the **data characteristics** and the **clustering goal**:

- Use **Euclidean** when features are continuous and scaled.  
- Use **Manhattan** for grid-like or sparse data.  
- Use **Mahalanobis** when features are correlated.  
- Use **Chebyshev** for problems sensitive to maximum deviations.  
- Use **Minkowski** for flexible control between Manhattan and Euclidean.
:::

#### Cosine and Correlation Distance

For data where *direction* or *orientation* matters more than magnitude — such as text represented by TF-IDF vectors or normalized embeddings — 
Euclidean distance is not ideal. Instead, we use *cosine similarity* or *correlation distance*.

The *cosine similarity* between two vectors $\mathbf{x}$ and $\mathbf{y}$ is defined as
$$
s_{\text{cosine}}(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \, \|\mathbf{y}\|}
$$
where $\mathbf{x} \cdot \mathbf{y}$ is the **dot product** of the two vectors, and $\|\mathbf{x}\|$ and $\|\mathbf{y}\|$ are their**Euclidean norms*.  

This similarity measures the *cosine of the angle* between the two vectors in a multidimensional space.  

- A value of **1** means the vectors point in the **same direction** (perfectly similar).
   
- A value of **0** means the vectors are **orthogonal** (no similarity). 
   
- A value of **–1** means they point in **opposite directions** (perfectly dissimilar).

The corresponding *cosine distance* is:
$$
d_{\text{cosine}}(\mathbf{x}, \mathbf{y}) = 1 - s_{\text{cosine}}(\mathbf{x}, \mathbf{y})
$$

```{webr-r}
#| label: cosine-distance
#| autorun: true
#| message: false
#| warning: false

library(lsa)  # for cosine similarity

x <- c(1, 2, 3)
y <- c(2, 4, 6)
z <- c(3, 0, 0)

cos_xy <- cosine(x, y)
cos_xz <- cosine(x, z)

cat("Cosine similarity (x,y):", round(cos_xy, 3), "\n")
cat("Cosine similarity (x,z):", round(cos_xz, 3), "\n")
cat("Cosine distance (x,y):", round(1 - cos_xy, 3), "\n")
cat("Cosine distance (x,z):", round(1 - cos_xz, 3), "\n")
```

:::{.callout.tip}
Cosine-based metrics are particularly useful for *directional data*, where the *pattern* or *orientation* of features matters more than their magnitude —  
for example, in *text mining* (TF-IDF vectors), *recommendation systems*, or *image feature embeddings*.
:::

### Distances for Binary Data

When the data are **binary** (0 or 1), such as presence/absence, success/failure, or yes/no attributes,  specialized similarity and distance measures are used.

Let $o_1, o_2 \in \{0,1\}^d$ be two binary observations described by $d$ attributes.

We define:
$$
\begin{aligned}
f_{11} &= \text{number of attributes where } o_1 = 1 \text{ and } o_2 = 1, \\
f_{00} &= \text{number of attributes where } o_1 = 0 \text{ and } o_2 = 0, \\
f_{10} &= \text{number of attributes where } o_1 = 1 \text{ and } o_2 = 0, \\
f_{01} &= \text{number of attributes where } o_1 = 0 \text{ and } o_2 = 1.
\end{aligned}
$$

#### **Simple Matching Coefficient (Similarity)**

$$
s_{SMC}(o_1, o_2) = \frac{f_{11} + f_{00}}{f_{11} + f_{00} + f_{10} + f_{01}} = \frac{f_{11} + f_{00}}{d}
$$

This coefficient measures the proportion of attributes where the two observations match — whether both are 1s or both are 0s.

#### **Simple Matching Distance**
$$
d_{SMC}(o_1, o_2) = 1 - s_{SMC}(o_1, o_2) = \frac{f_{01} + f_{10}}{d}
$$
This represents the proportion of mismatches between two binary objects.

```{webr-r}
#| label: smc-distance-example
#| autorun: true
#| message: false
#| warning: false

# Two binary observations (e.g., presence/absence of features)
o1 <- c(1, 0, 1, 1, 0, 0, 1)
o2 <- c(1, 1, 0, 1, 0, 0, 0)

# Compute the components
f11 <- sum(o1 == 1 & o2 == 1)
f00 <- sum(o1 == 0 & o2 == 0)
f10 <- sum(o1 == 1 & o2 == 0)
f01 <- sum(o1 == 0 & o2 == 1)
d <- length(o1)

# Similarity and distance
s_SMC <- (f11 + f00) / d
d_SMC <- (f01 + f10) / d  # same as 1 - s_SMC

cat("f11 =", f11, "f00 =", f00, "f10 =", f10, "f01 =", f01, "\n")
cat("Simple Matching Coefficient (similarity):", round(s_SMC, 3), "\n")
cat("Simple Matching Distance:", round(d_SMC, 3), "\n")
```

:::{.callout-note}

- If two binary vectors are identical,$s_{SMC} = 1$ and $d_{sMC} = 0$.

- If they are completely opposite, $s_{SMC} = 0$  and $d_{sMC} = 1$.

- The SMC treats $0$s and $1$s symmetrically, so it is best used when both states are equally meaningful.
:::

### Distances for Categorical Data

When data contain **categorical** or **mixed-type variables**, standard numeric distances (like Euclidean) are not suitable.  
Instead, we use measures that handle **qualitative comparisons** directly.

Let  $\mathbf{x} = (x_1, \ldots, x_p)$ and $\mathbf{y} = (y_1, \ldots, y_p)$ be two observations described by $p$ categorical or mixed-type attributes.

#### **Hamming Distance**

The **Hamming distance** counts the number of mismatches between two vectors:
$$
\text{d}_{Hamming}(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{p} \delta(x_i, y_i)
$$
where
$$
\delta(x_i, y_i) =
\begin{cases}
0, & \text{if } x_i = y_i \\
1, & \text{if } x_i \neq y_i
\end{cases}
$$
It measures how many positions differ between two categorical strings or binary vectors.

```{webr-r}
#| label: hamming-distance
#| message: false
#| warning: false
#| autorun: true

# Example categorical data
x <- c("Red", "Small", "Circle")
y <- c("Blue", "Small", "Square")

# ----- Hamming Distance -----
hamming_distance <- sum(x != y)
print(paste("Hamming distance:", hamming_distance))
```


#### **Gower Distance**

The **Gower distance** [@Gower1971] allows comparing **mixed-type data** (numerical, categorical, binomial).

For two observations $i$ and $k$, and each variable $k$, Gower's method computes a partial *similarity score* $s_{ijk}$ as follows: 

- **Numeric variables (continuous or discrete)**:
$$
s_{ijk} = 1 - \frac{|x_{ik} - x_{jk}|}{R_k}
$$
where $R_k$ is the range of variable $k$.

- **Categorical (or binary) variables**:
$$
s_{ijk} = \begin{cases}
1 & \quad \text{if} \quad x_{ik} = x_{jk}\\
0 & \quad \text{if} \quad x_{ik} \neq x_{jk}
\end{cases}
$$
Then, the *overall similarity* between $i$ and $j$ is 
$$
S_{ij} = \frac{\sum_k s_{ijk} \delta_{ijk}}{\sum_k \delta_{ijk}}
$$
where $\delta_{ijk} = 1$ if variable $k$ is valid for both $i$ and $j$ (i.e, non-missing), else $0$.

Finally, to convert similarity to a distance, one often uses
$$ 
D_{ij} = 1 - S_{ij}
$$

[**Example**](https://crispinagar.github.io/blogs/gower-distance.html)

As an example, consider the following table which shows information about a number of individuals (identified by "Subject ID") with four attributes:

- *Age* --  a continuous numeric variable
- *Handedness* -- a binary variable, whether the individual is left- or right-handed.
- *Eye colour* - a categorical variable
- *Knows Python* - a dichotomous variable (while two people who know python may have a similar education, both of them not knowing Python does not imply they have similar backgrounds).

| Subject ID | Age | Handedness | Eye Colour | Knows Python |
|:-----------:|:---:|:-----------:|:------------:|:--------------:|
| 001 | 28 | Right | Blue  | Yes |
| 002 | 34 | Left  | Blue  | No  |
| 003 | 22 | Right | Green | Yes |
| 004 | 45 | Right | Hazel | No  |
| 005 | 30 | Left  | Brown | Yes |

Let us look at individuals 001 and 002, and calculate the score for each variable in turn:

- *Age*: 
$$
s_{\textrm{age}} = 1 - \frac{|28 - 34|}{23} = 0.74
$$
where the range $R_k = 23$ is the range of ages in the sample, which has a minimum of $22$ and a maximum of $45$.

- *Handedness*: 

Since the individuals have different handedness, $s_{\text{handedness}} = 0$.

- *Eye colour*: 
They have the same eyes, $s_{\text{eyes}} = 1$.

- *Knows Python*:

Individuals 001 knows python whereas 002 does not; so, $s_{\text{python}} = 0$.

We have no missing data, so all the $\delta_{ijk} = 1$. The overall similarity score between 001 and 002 is therefore
$$
S = \frac{0.74 \times 1 + 0 \times 1 + 1 \times 1 + 0 \times 1}{1 + 1 + 1 + 1} = \frac{1.74}{4} = 0.435
$$
So, the Gower distance between these two individuals is 
$$
D = 1 - 0.435 = 0.565
$$

Repeating the calculation for all pairs of indivials, we obtain the distance matrix, which can be used for clustering them into groups. 

For calculating Gower distance in R, we can use `daisy()` from the library `cluster`. However, it is important to note that always convert categorical characters to factors before using this function. 
```{webr-r}
#| label: gower-mixed-fixed
#| autorun: true
#| message: false
#| warning: false

library(cluster)
library(tibble)
library(dplyr)

# Create dataset
subjects <- tibble::tibble(
  Subject_ID = c("001", "002", "003", "004", "005"),
  Age = c(28, 34, 22, 45, 30),                
  Handedness = c("Right", "Left", "Right", "Right", "Left"),  
  Eye_Colour = c("Blue", "Blue", "Green", "Hazel", "Brown"),  
  Knows_Python = c("Yes", "No", "Yes", "No", "Yes")           
)

# Convert character columns to factors (important for Gower)
subjects <- subjects %>%
  mutate(across(where(is.character), as.factor))

# Compute Gower distance 
gower_dist <- daisy(subjects[, -1], metric = "gower")  # exclude Subject_ID

print(round(as.matrix(gower_dist), 3))
```
As you see that the individuals who are most similar are 001 and 003 $(D = 0.315)$ and the most different are 004 and 005 $(D = 0.913)$.

### Visualizing distance matrices

Datasets usually contain many observations (more than just two points).

When we compute pairwise distances between all observations, we obtain a *distance matrix* — a square table where each cell represents the dissimilarity between two observations.

Mathematically, if a dataset has $n$ observations, the distance matrix has $n \times n$ entries:
$$
D_{ij} = d(\mathbf{x}_i, \mathbf{x}_j)
$$
where $D_{ij}$ is the distance between observation $i$ and $j$.

Because this matrix contains all pairwise comparisons, it provides a complete picture of how observations relate to one another.

However, as the number of observations grows, the matrix quickly becomes difficult to interpret numerically — it is just a sea of numbers.

To extract insights, we often visualize the distance matrix as a heatmap or similarity map.

Visualizing distance matrices helps us:

- **Detect structure** -- clusters appear as blocks or dark/light patches.
  
- **Spot outliers** -- points that are dissimilar from everyone else.
  
- **Compare metrics** -- see how distance measures produce different patterns.
  
- **Build intuitio** -- for what *closeness* or *difference* means in multidimensional space.

```{webr-r}
#| label: visualize-gower
#| autorun: true
#| message: false
#| warning: false
#| fig-cap: "Visualization of Gower distance matrix using factoextra."
#| out-width: "80%"

library(cluster)
library(factoextra)
library(dplyr)
library(tibble)

subjects <- tibble(
  Subject_ID = c("001", "002", "003", "004", "005"),
  Age = c(28, 34, 22, 45, 30),
  Handedness = as.factor(c("Right", "Left", "Right", "Right", "Left")),
  Eye_Colour = as.factor(c("Blue", "Blue", "Green", "Hazel", "Brown")),
  Knows_Python = as.factor(c("Yes", "No", "Yes", "No", "Yes"))
)
subjects <- subjects %>%
  mutate(across(where(is.character), as.factor))
gower_dist <- daisy(subjects[, -1], metric = "gower")

# Visualize matrix
library(factoextra)
fviz_dist(gower_dist) 
```
The color level is proportional to the value of the dissimilarity between observations: pure red if $\text{dist}(\mathbf{x}_i, \mathbf{x}_j) = 0$ nad pure blue if $\text{dist}(\mathbf{x}_i, \mathbf{x}_j) = 1$, here. Objects belonging to the same cluster are displayed in consecutive order. 

In this example, subjects `001` and `002` are quite similar (close in age, same eye color). Subject `004` (older, hazel eyes, does not know Python) stands out as most dissimilar from others. 


::::: panel-tabset
## Exercise 1

**1. Create two 3-dimensional points and calculate the distance**

Let  $A = (1, 3, 5)$ and $B = (4, 9, 6)$.

Compute manually and in R the following distances between $A$ and $B$:

- Euclidean  

- Manhattan  

- Minkowski with $r = 3$  

- Chebyshev  

**2. Explore sensitivity to scale**

Multiply the second coordinate of both points by 10 and recompute all distances. How does this change the results?  

## Exercise 2

Two species are described by four binary characteristics:

| Feature | Species A | Species B |
|----------|------------|------------|
| Has fins | 1 | 1 |
| Lays eggs | 1 | 0 |
| Has scales | 1 | 1 |
| Warm-blooded | 0 | 1 |

1. Compute manually:  
   - \( f_{11}, f_{00}, f_{10}, f_{01} \)
   - The *Simple Matching Coefficient (SMC)*  
   - The *Simple Matching Distance (SMD)* 

2. Verify your results using R.

3. Which pair of features contributes to dissimilarity?

## Exercise 3

Two students rate three projects as “Good”, “Average”, or “Poor”:

| Project | Student 1 | Student 2 |
|----------|------------|------------|
| A | Good | Good |
| B | Poor | Average |
| C | Good | Poor |

Compute the *Hamming distance* (number of mismatches).

**Hint**: Use 
```{r}
#| eval: false
data[] <- lapply(data, function(col) if (is.character(col)) as.factor(col) else col)
```

## Exercise 3

We want to analyse the `flower` dataset which is available at library `cluseter`.

Based on the following information, a) find the best distance matrix. and b) visulize the distance matrix.

```{r}
#| label: Exercise4-Gower_distance
#| echo: true
library(cluster)
data(flower)

head(flower, 3)

#Data structure
str(flower)

```
:::::

## Partition-Based Clustering: $k$-Means and $k$-Medoids

*Partitioning clustering* are clustering methods used to classify observations, within a dataset, into multiple groups based ion their similrity. The algorithms require the analyst to specify the number of clusters to be generated.

The commonly used partitioning clustering includes: 

- $K$-means clustering [@macqueen1967] in which ,each cluster is represented by the center or means of the data points belonging to the cluster. The $K$-means method is sensitive to anomalous data points and outliers.
- $K$-medoids clustering or PAM (Partitioning Around Medoids) [@kaufman1990] in which , each cluster is reprsented by one of the objects in the cluster. PAM is less sensitive to outliers compared to $K$-means.
- CLARA (Clustering Large Applications) algorithm, which is an extension to PAM adapted for large data sets.

### $K$-Means Clustering 

K-Means aims to partition $n$ observations into $K$ clusters such that each observation belongs to the cluster with the nearest **centroid** (mean of points).

Each cluster is represented by its *centroid*, which may not be an actual data point.

**$K$-means basic ideas**

This basic idea behind $K$-means clustering consists of defining clusters so that the total intra-cluster variation (known as *total within-cluster variation*) is minimized.

There are several $K$-means algorithms available. The standard algorithm is the *Hartigan-Wong algorithm* [@hartigan1979] which defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid: 
$$
W(C_k) = \sum_{x_i \in C_k} (x_i - \mu_k)^2
$$
where $x_i$ design a data point belonging to the cluster $C_k$ and $\mu_k$ is the mean value of the points assigned to the cluster $C_k$.

Each observation ($x_i$) is assigned to a given cluster such that the sum of squares (SS) distances of the observation to their assigned cluster $\mu_k$ is minimum. 

We define the *total within-cluster* variation as follow: 
$$
\text{tot.withinss} = \sum_{k=1}^K W(C_k) = \sum_{k=1}^K \sum_{x_i \in C_k} (x_i - \mu_k)^2
$$

The *total within-cluster sum of square* measures the compactness (i.e. *goodness*) of the clustering and we want it to be as small as possible. 


**$K$-means Algorithm**

Given a dataset $\mathbf{X}= \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\}$:

1. Choose the number of clusters $K$.  
2. Initialize $k$ centroids randomly.  
3. **Assign step:** Assign each observation $\mathbf{x}_i$ to the nearest centroid based on the Euclidean distance between the object and the centroid:
4. $$
   c_i = \arg\min_k \|\mathbf{x}_i - \mu_k\|^2
   $$
   where $\mu_k$ is the $k$st centroid. 
5. **Update step:** Recalculate each centroid as the mean of points in its cluster:
   $$
   \mu_k = \frac{1}{n_k} \sum_{\mathbf{x}_i \in C_k} \mathbf{x}_i
   $$
6. Repeat steps 3–4 until centroids stop changing (convergence) or the maximum number of iterations is reached. 

For better understanding, see this [video](https://blog.dailydoseofds.com/p/an-animated-guide-to-kmeans)

```{webr-r}
#| label: kmeans-example
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

set.seed(42)

# Generate synthetic 2D data
x <- rbind(
  matrix(rnorm(100, mean = 0, sd = 0.3), ncol = 2),
  matrix(rnorm(100, mean = 3, sd = 0.3), ncol = 2)
)
colnames(x) <- c("x", "y")

# Apply k-means with 2 clusters
km <- kmeans(x, centers = 2, nstart = 20)

# Visualize results
library(ggplot2)
data <- data.frame(x, cluster = factor(km$cluster))
ggplot(data, aes(x, y, color = cluster)) +
  geom_point(size = 3) +
  geom_point(data = as.data.frame(km$centers), aes(x, y), color = "black", size = 5, shape = 8) +
  labs(title = "K-Means Clustering",
       subtitle = "Centroids shown as black stars") +
  theme_minimal()
```

#### Properties and Limitations of $K$-Means

Although $K$-means is one of the most popular clustering algorithms, its effectiveness depends on the structure and scale of the data. Understanding its limitations helps decide when to use it — and when to choose a more robust alternative like $K$-Medoids.

##### **Data Type and Cluster Shape**

$K$-Means works best when:

- All variables are numeric
-  and measured on a comparable scale. 
   
- The clusters are *spherical* (or roughly circular in 2D).  
  
- Each cluster has *similar variance* and *density*.

When clusters are elongated, overlapping, or non-spherical, $K$-Means may assign points incorrectly because it relies on *Euclidean distance*.

##### **Sensitivity to Scale**

In clustering, the distance between data points determines how groups are formed. 

If variables are measured on very different scales (for example, income in euros and age in years), the variable with the largest numerical range will dominate the distance calculation. As a result, the algorithm may ignore other variables, even if they carry meaningful structure. To correct this imbalance, we use **scaling transformations**, which adjust the magnitude or spread of each variable before computing distances.

**Normalization (Min–Max Scaling)**
Normalization rescales all variables to a fixed range, usually between $0$ and $1$:
$$
x' = \frac{x - \min(x)}{\max(x) - \min(x)}
$$
This transformation preserves the shape of the distribution but ensures all features contribute equally in magnitude to the distance measure.
It is particularly effective when all features have bounded ranges or similar distributions.

**Standardization (Z-score Scaling)**
Standardization rescales variables so they have mean 0 and standard deviation 1:
$$
x' = \frac{x - \bar{x}}{s_x}
$$
where $\bar{x}$ is the mean and $s_x$ is the standard deviation. 

This approach centers the data and adjusts for variance, which helps when features have different dispersions or units of measurement. 
Standardization is preferred when features are expected to follow roughly Gaussian distributions, or when outliers might distort min–max scaling.

:::{.callout-important}
**Why It Matters for $k$-Means and Similar Algorithms**

- $K$-means relies on Euclidean distance, which is sensitive to scale. Without scaling, one feature can dominate, producing distorted clusters.

- After scaling or standardization, all variables contribute more fairly, and the clusters tend to reflect combined variation across features.

- Scaling can change the cluster boundaries, but it does not change the underlying data relationships — it only ensures that distances are comparable.
:::

We will show the effect of normalization and standardization on the data. 

```{webr-r}
#| label: kmeans-step1
#| fig-cap: "Original data (different scales)"
#| autorun: true
#| out-width: "80%"
#| warnings: false 
#| message: false
library(ggplot2)
library(dplyr)

set.seed(42)
x <- rnorm(1000) / 2
y <- c(rnorm(500) + 5, rnorm(500)) / 10
data <- data.frame(x, y)

ggplot(data, aes(x, y)) +
  geom_point(size = 1, color = "black") +
  theme_minimal(base_size = 14) +
  labs(title = "Original Data (different scales)")
```
Now, we apply $K$-means on the raw dataset. 

```{webr-r}
#| label: kmeans-step2
#| fig-cap: "Non-normalized K-Means clustering"
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

km_raw <- kmeans(data, centers = 2)
data$cluster_raw <- as.factor(km_raw$cluster)

ggplot(data, aes(x, y, color = cluster_raw)) +
  geom_point(size = 1) +
  scale_color_manual(values = c("tomato", "steelblue")) +
  theme_minimal(base_size = 14) +
  labs(title = "Non-normalized K-Means")
```
Now, we normalize the dataset and apply $k$-means. 
```{webr-r}
#| label: kmeans-step3
#| fig-cap: "Normalized (0–1) K-Means clustering"
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

normalize <- function(v) (v - min(v)) / (max(v) - min(v))
data_norm <- data %>% mutate(xn = normalize(x), yn = normalize(y))
km_norm <- kmeans(data_norm[, c("xn", "yn")], centers = 2)
data_norm$cluster_norm <- as.factor(km_norm$cluster)

ggplot(data_norm, aes(xn, yn, color = cluster_norm)) +
  geom_point(size = 1) +
  scale_color_manual(values = c("tomato", "steelblue")) +
  theme_minimal(base_size = 14) +
  labs(title = "Normalized (0–1) K-Means")
```
It is the time to standarize the dataset before applyong $k$-means. 
```{webr-r}
#| label: kmeans-step4
#| fig-cap: "Standardized (Z-score) K-Means clustering"
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

standardize <- function(v) (v - mean(v)) / sd(v)
data_std <- data %>% mutate(xs = standardize(x), ys = standardize(y))
km_std <- kmeans(data_std[, c("xs", "ys")], centers = 2)
data_std$cluster_std <- as.factor(km_std$cluster)

ggplot(data_std, aes(xs, ys, color = cluster_std)) +
  geom_point(size = 1) +
  scale_color_manual(values = c("tomato", "steelblue")) +
  theme_minimal(base_size = 14) +
  labs(title = "Standardized (Z-score) K-Means")
```

#### Measures of Cluster Quality

To evaluate and compare clustering results, we rely on *internal validation measures* — those that assess how compact and well-separated the clusters are, using only the data itself.

The most common measures include:

##### **Within-Cluster Sum of Squares (WSS)**

The *Within-Cluster Sum of Squares* measures how tightly the data points in a cluster are grouped around their centroid.
$$
\text{WSS} = \sum_{k=1}^{K} \sum_{\mathbf{x}_i \in C_k} \|\mathbf{x}_i - \boldsymbol{\mu}_k\|^2
$$
where $C_k$ is the set of points in cluster $k$, $\boldsymbol{\mu}_k$ is the centroid of cluster $k$, and $\|\cdot\|^2$ shows the squared Euclidean distance between a point and its centroid.  

A smaller WSS means *more compact clusters*.

##### **Between-Cluster Sum of Squares (BSS)**

The *Between-Cluster Sum of Squares* measures how far apart the cluster centroids are from the overall mean of the dataset.
$$
\text{BSS} = \sum_{k=1}^{K} n_k \| \boldsymbol{\mu}_k - \boldsymbol{\mu} \|^2
$$
where $n_k$ is the number of points in cluster $k$, $\boldsymbol{\mu}_k$ is the centroid of cluster $k$, and $\boldsymbol{\mu}$ is the global mean of all data points. 

A larger BSS means *better separation* between clusters.

##### **Total Sum of Squares (TSS)**

The *Total Sum of Squares* represents the overall variation in the dataset:
$$
\text{TSS} = \sum_{i=1}^{n} \|\mathbf{x}_i - \boldsymbol{\mu}\|^2
$$

This relationship always holds:
$$
\text{TSS} = \text{BSS} + \text{WSS}
$$

This decomposition allows us to express clustering performance as a proportion of explained variance:
$$
\text{Ratio} = \frac{\text{BSS}}{\text{TSS}}
$$

A higher ratio means that the clusters explain a greater proportion of the data’s variance.

##### **Silhouette Coefficient**

The *Silhouette Coefficient* combines *cohesion* (how close points are within a cluster) and *separation* (how far points are from other clusters):
$$
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
$$
where $a(i)$ is the average distance from point $i$ to all other points in its cluster, and $b(i)$ is the smallest average distance from point $i$ to points in another cluster.  

Values range from $–1$ to $1$, where values **close to $1$** indicate that observations are well-clustered, values **around $0$** suggest that points lie near cluster boundaries, and  **negative** values indicate that observations are likely misclassified or assigned to the wrong cluster.

:::{.callout-note}
**Summary**

| Measure | Description | Ideal Value |
|:---------|:-------------|:-------------|
| **WSS** | Compactness within clusters | Low |
| **BSS** | Separation between clusters | High |
| **BSS / TSS** | Proportion of explained variance | High |
| **Silhouette** | Cohesion and separation combined | Close to 1 |
:::

:::{.callout-important}
Good clustering minimizes **WSS** and maximizes **BSS**, producing compact, well-separated groups.  
The **Elbow Method** helps find a balance between simplicity (few clusters) and accuracy (tight, distinct groups).
:::

**Implementation in R**

The standard R function for $K$-means clustering is `stats::kmeans()` which simplified format is as follow:
```{r}
#| eval: false
kmeans(x, centers, iter.max = 10, nstart = 1)
```
where:
- `x`: numeric matrix, numeric data frame or a numeric vevtor
- `centers`: possible values are the number of clusters ($k$) or a set of initial (distinct) cluster centers. If a number, a random set of (distinct) rows in $x$ is chosen as the initial centers. 
- `iter.max`: the maximum number of iterations allowed. Default value if $10$.
- `nstart`: the number of random starting partitions when centers is a number. Trying `nstart > 1` is often recommended. 

To create a beautiful graph of the clusters generated with the `kmeans()` function, will use the `factoextra` package.

**Computing $K$-means clustering**

As $K$-means clustering algorithm starts with $K$ randomly selected centroids, it is always recommended to use the `set.seed()` function in order to set a seed for `R`'s random number generator. The aim of using this function is to make reproducible the results. 

The `R` code below performs $K$-means clustering with $K = 4$:
```{r}
#| eval: false
set.seed(123)
km.res <- kmeans(df, k = 4, nstart = 25)
```

:::{.callout-tip}
Actually, $K$-means starts by randomly choosing $K$ points as *initial centroids*. Depending on these starting points, the algorithm can converge to *different local minima* — meaning the results may vary across runs. So, `nstart` means number of random initial sets of centroids to try and then, R will run the algorithm that many times and keep the best result (the one with the lowest total within-cluster sum of squares). 

The default value of `nstart` in R is one. But, it is strongly recommended to compute $K$-means clustering with a large value of `nstart` such as $25$ or $50$, in order to have a more stable result.
:::

With the function `print()`, the results of `kmeans()` that for example in this note, saved in `km.res` 
```{r}
#| eval: false
print(km.res)
```
The printed output displys:
- the cluster means or centers, that is a matrix which rows are cluster number and columns are variables. 
- the clustering vector which is a vector of integers (from 1 to $K$) indicating the cluster to which each point is allocated. 

:::{.callout-note}
It is possible to compute the mean of each variable in the dataset by clusters using the original dataset.
```{r}
#| eval: false 
aggregate(df, by = list(cluster=km.res$cluster), mean)
```
:::

:::{.callout-note}
It the user is intersted in adding the clusters to the original data, use the following R code
```{r}
#| eval: false 
cbind(df, cluster = km.res$cluster)
```
:::

`kmeans()` function returns a list of components, including:
- *cluster*: a vector of integers (from $1$ to $k$) indicating the cluster to which each point is allocated. 
- *centers*: a matrix of cluster centers (cluster means)
- *totss*: the total sum of squares (TSS). It measures the total variance in the data. 
- *withinss*: vector of within-cluster sum of squares, one component per cluster
- *tot.withinss*: total within-cluster sum of squares, i.e., $\text{sum}(withinss)$
- *betweenss*: the between-cluster sum of squares, i.e., *totss - tot.withinss*
- *size*: the number of observations in each cluster

These components can be accessed as follow:
```{r}
#| eval: false 
# Cluster number for each of the observations
km.res$cluster 

# Cluster size
km.res$size

# Cluster means
km.res$centers
```

**Visualizing $K$-means Clusters**

It is a good idea to plot the cluster results. These can be used to assess the choice of the number of clusters as well as comparing two different cluster analyses.

The idea is to visualize the data in scatter plot with coloring tach data point according to its cluster assignment. 

The problem is that the data contains more than 2 variables and the question is what variables to choose for the $x$-$y$ scatter plot. A **solution** is to reduce the number of dimensions by applying a dimensionality reduction algorithm, such as *principal component analysis (PCA)*. 

In other words, if we have a multi-dimensional dataset, a solution is to perform *PCA* and to plot data points according to the first two principal components coordinates.

The function `factoextra::fviz_cluster()` can be used to easily visulize $K$-means clusters. It takes $K$-means results and the original data as arguments. In the resulting plot, observations are represented by points, using orincipal components if the number of variables is greater than $2$. It is also possible to draw concentration ellipse around each cluster. 

```{r}
#| eval: false 
fviz_cluster(km.res, data = df,
palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"), #Define colors
ellipse.type = "euclid", # Concentration ellipse
star.plot = TRUE, # Add segments from centroids to items
repel = TRUE, # Avoid label overplotting (slow)
ggtheme = theme_minimal()
)
```

:::{.callout-important}
$K$-means clustering is very simple and fast algorithm. It can efficiently deal with very large data sets. 
:::


:::{.callout-important}
$K$-means has some weaknesses, including:

- It assumes prior knowledge of the data and requires the analyst to choose the appropriate number of cluster ($k$) in advance. 

*Solution*: Compute $K$-means for a range of $K$ values, for example by varying $k$ between $2$ and $10$. Then choose the best $k$ by comparing the clustering results obtained for different $k$ values.

- The final results obtained is sensitive to the initial random selection of cluster centers. *Why is this a problem?* Because, for every different run of the algorithm on the same data set, you may choose different set of initial centers. This may lead to different clustering results on different runs of the algorithm.

*Solution*: Compute $K$-means algorithm several times with different initial cluster centers. The run with the lowest total within-cluster sum of square is selected as the final clustering solution.

- It is sensitive to outliers.

*Solution*: To avoid distortions caused by excessive outliers, it is possible to use PAM algorithm, which is less sensitive to outliers.

:::

#### Analysis of `iris` Dataset

The classic `iris` dataset contains $150$ flower observations from three species (*setosa*, *versicolor*, *virginica*).  Each observation has four numeric features: `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`

```{webr-r}
#| label: iris-prepare
#| autorun: true
#| message: false
#| warning: false

# Load dataset
data("iris")

# Select only numeric features for clustering
iris_data <- iris[, 1:4]
```

Suppose that we do not know that there are three groups and the goal is to determine the optimal number of clusters using *Elbow method*

```{webr-r}
#| label: iris-elbow
#| autorun: true
#| message: false
#| warning: false
#| out-width: "90%"

library(ggplot2)

# Compute WSS for k = 1:10

wss <- sapply(1:10, function(k) {
kmeans(iris_data, centers = k, nstart = 20)$tot.withinss
})

# Create Elbow plot

elbow_df <- data.frame(k = 1:10, WSS = wss)

ggplot(elbow_df, aes(k, WSS)) +
geom_line(color = "steelblue", linewidth = 1.2) +
geom_point(size = 3, color = "tomato") +
scale_x_continuous(breaks = 1:10) +
labs(title = "Elbow Method — Iris Dataset",
x = "Number of clusters (k)",
y = "Total Within-Cluster Sum of Squares (WSS)") +
theme_minimal()
```

The plot represents the variance within the clusters. It decreases as $k$ increases, but it can be seen a bend (or *elbow*) at $k = 3$, matching the three known species.

Another way to visualize the elbow method for choosing the optimal number of clusters is by using the function `factoextra::fviz_nbclust()`. We can also add a vertical dashed line to indicate the chosen value of $k$.

```{webr-r}
#| label: elbow-fviz 
#| autorun:true 
#| messages: false 
#| warning: false 
#| out-width: "80%"

library(factoextra)

fviz_nbclust(iris_data, kmeans, method = 'wss') + 
  geom_vline(xintercept = 3, linetype = 2) +
labs(
title = "Elbow Method for Determining Optimal k",
subtitle = "Dashed line indicates the chosen number of clusters (k = 4)"
)
```

Now, we apply $k$-means algorithm with $k = 3$.

```{webr-r}
#| label: iris-kmeans
#| autorun: true
#| message: false
#| warning: false

set.seed(42)
km_iris <- kmeans(iris_data, centers = 3, nstart = 20)

# Print the results
print(km_iris)
```

```{webr-r}
#| label: iris-kmeans
#| autorun: true
#| message: false
#| warning: false

# Add cluster assignments to the dataset
iris_clustered <- cbind(iris, cluster = factor(km_iris$cluster))

# Show first few results
head(iris_clustered)
```

We compute cluster quality measures.

```{webr-r}
#| label: iris-measures
#| autorun: true
#| message: false
#| warning: false

# Total, within, and between-cluster sums of squares

TSS <- sum(scale(iris_data, scale = FALSE)^2)
WSS <- km_iris$tot.withinss
BSS <- TSS - WSS
ratio <- BSS / TSS

cat("TSS:", round(TSS, 2), "\n")
cat("WSS:", round(WSS, 2), "\n")
cat("BSS:", round(BSS, 2), "\n")
cat("Explained variance (BSS/TSS):", round(ratio, 3), "\n")
```

:::{.callout-note}

**What are “discriminative features”?**

*Discriminative features* are those that *best separate groups or clusters* in your data. They are the variables that most strongly differentiate one cluster (or class) from another.

There are two most famous approach to find discriminative features:

- Exploratory approach
  
- Statistical approach

**Exploratory approach**
```{webr-r}
#| label: eda
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

library(GGally)
ggpairs(iris, aes(color = Species))
```
In this plot, you will notice that:

- Setosa is perfectly separated in the `Petal.Length`–`Petal.Width` plane.

- Versicolor and Virginica overlap slightly but still show visible separation.

That is why we often choose `Petal.Length` and `Petal.Width` for visualization — they are the most discriminative pair of features.

**Statistical approach**
We can quantify which features are most discriminative by computing, for each variable:
- The between-group variance (BSS)
- The within-group variance (WSS)
and comparing their ratio.

```{webr-r}
#| label: discriminative-measures
#| autorun: true
#| message: false
#| warning: false

# Compute ratio of between- to within-group variance for each feature
library(dplyr)

iris_var <- iris %>%
  group_by(Species) %>%
  summarise(across(where(is.numeric), mean))

# Between-class variance
overall_means <- colMeans(iris[, 1:4])
between_var <- colSums((as.matrix(iris_var[, -1]) - overall_means)^2)

# Within-class variance
within_var <- sapply(iris[, 1:4], function(v) tapply(v, iris$Species, var))
within_var <- colMeans(within_var)

# Ratio
discriminative_ratio <- between_var / within_var
discriminative_ratio
```
The higher this ratio, the better that feature separates the species.

In this results, `Petal.Width` is clearly the most discriminative feature.

At first glance, it seems that `Sepal.Width` (124.96) should be more discriminative than `Petal.Length` (79.51) because its ratio is higher — but this needs context

**What the ratio measures?** The ratio 
BSS/WSS
BSS/WSS for each feature is:
$$
\text{Ratio} = \frac{Between-Class Variance}{Within-Class Variance}
$$
This tells you, for that one variable alone, how distinct the species means are compared to the variability within each species.

So, mathematically, if `Sepal.Width` has a higher ratio, it means that species differ more in `Sepal.Width` (on average) than they do within themselves. **But "discriminative" depends on combined separability**

The tricky part is that `Sepal.Width` alone does not visually separate the species well, even if its numeric ratio looks high.
Let us check the data visually:

```{webr-r}
#| label: sepal-length-plots
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

library(ggplot2)

# Compare Sepal.Width vs Petal.Length
ggplot(iris, aes(Sepal.Width, fill = Species)) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution of Sepal.Width by Species") +
  theme_minimal()

ggplot(iris, aes(Petal.Length, fill = Species)) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution of Petal.Length by Species") +
  theme_minimal()
```
You see that:
- For `Sepal.Width`, there is a lot of overlap between Versicolor and Virginica.
The means differ, but not enough to separate groups clearly.

- For `Petal.Length`, Setosa is completely separated, and Versicolor/Virginica overlap less.

So even though `Sepal.Width`’s ratio is high, its overlap pattern means it is less useful for distinguishing all three species.

:::{.callout-tip}
The *BSS/WSS ratio* is a helpful indicator, but not the whole story.
Always complement numerical measures with *visual inspection*.
:::
:::

Now, based on the most discriminative features, `Petal.Length` and `Petal.Width`, we visualize the clusters.

```{webr-r}
#| label: iris-plots
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

ggplot(iris_clustered, aes(Petal.Length, Petal.Width, color = cluster)) +
geom_point(size = 3) +
labs(title = "K-Means Clustering of Iris Data (k = 3)",
subtitle = "Color shows cluster assignment") +
theme_minimal()
```


Another way to visulize $K$-means clusters is using the function `fviz_cluster()` that used the PCA (the `iris_data` has 4 variables) with the following code 

```{webr-r}
#| label: fviz_clust_iris
#| autorun: true
#| warnings: false 
#| message: false
#| out-width: "80%"
fviz_cluster(km_iris, data = iris_data,
palette = c("#2E9FDF", "#00AFBB", "#E7B800"),
ellipse.type = "euclid", # Concentration ellipse
star.plot = FALSE, # Add segments from centroids to items
repel = TRUE, # Avoid label overplotting (slow)
ggtheme = theme_minimal()
)
```




We can also evaluat  clustering with the Silhouette coefficient

```{webr-r}
#| label: silhouette-iris
#| autorun: true
#| message: false
#| warning: false

library(cluster)
set.seed(42)

# K-means result for k = 3 (from previous section)
km_iris <- kmeans(iris[, 1:4], centers = 3, nstart = 20)

# Compute silhouette width
sil_iris <- silhouette(km_iris$cluster, dist(iris[, 1:4]))

# Display summary statistics
summary(sil_iris)
```

```{webr-r}
#| label: silhouette-plot
#| autorun: true
#| fig-cap: "Silhouette plot for K-Means clustering (k = 3) on the Iris dataset"
#| out-width: "80%"
plot(sil_iris, border = NA, main = "Silhouette Plot — Iris Dataset (k = 3)")
```
The *average silhouette width *(displayed in the summary) gives an overall quality score.

- Values > $0.5$ indicate clear, well-separated clusters.

- Values around $0.25$–$0.5$ indicate overlapping clusters.

- Values < $0.25$ indicate poor structure or misclassification.

In the `iris` dataset, the average silhouette is typically around $0.55$–$0.60$,
confirming that 
$k=3$ is a reasonable choice.

- *Setosa* usually has near-perfect silhouette values, while *Versicolor* and *Virginica* show some overlap — as expected.

We can compute the average silhouette width for different numbers of clusters.

```{webr-r}
#| label: silhouette-elbow
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

sil_widths <- sapply(2:10, function(k) {
km <- kmeans(iris[, 1:4], centers = k, nstart = 20)
ss <- silhouette(km$cluster, dist(iris[, 1:4]))
mean(ss[, "sil_width"])
})

sil_df <- data.frame(k = 2:10, Silhouette = sil_widths)

library(ggplot2)
ggplot(sil_df, aes(k, Silhouette)) +
geom_line(color = "steelblue", linewidth = 1.2) +
geom_point(size = 3, color = "tomato") +
scale_x_continuous(breaks = 2:10) +
labs(title = "Average Silhouette Width by Number of Clusters",
x = "Number of clusters (k)",
y = "Average Silhouette Width") +
theme_minimal()

library(factoextra)

fviz_nbclust(iris_data, kmeans, method = 'silhouette')
```

Based on the Silhouette plots, the highest average silhouette width occurs at $k = 2$. This means that, mathematically, dividing the data into two clusters gives the tightest and most well-separated grouping according to the Silhouette metric.

However, **the best numerical score does not always mean the most meaningful clustering**. 

At $k =2$, the algorithm merges two of the real Iris species (versicolor and virginica) into a single cluster. The result has high compactness, but low interpretive accuracy.

At $k = 3$, the average silhouette width is slightly lower, but the clusters correspond more closely to the true biological species. This result is more meaningful, even if slightly less compact.

Thus, the *Silhouette Coefficient* and the *Elbow Method* together suggest that: 
- $k=2$ yields the most compact clustering,
- but $k=3$ represents the optimal trade-off between cluster compactness and real-world interpretability


::::: panel-tabset
## Exercise 1

Use the built-in dataset `USArrests`, which contains statistics on violent crime rates in the 1970s for 50 U.S. states.
```{r}
#| label: ex1
#| eval: false

data("USArrests")
```
Standardize all numeric features before applying $k$-Means using this R code:
```{r}
#| label: ex1-standard 
#| eval: false

us_data <- scale(USArrests)
```
**Elbow Method**

- Compute the total within-cluster sum of squares (WSS) for $k=1$ to $10$.
  
- Plot the results.
  
- Identify the "elbow" point.

**Silhouette Method**

- Compute the average silhouette width for $k=2$ to $10$.
  
- Plot the results using the same approach as in the Iris example.
  
- Identify the $k$ that maximizes the silhouette width.

## Exercise  2
This exercise demonstrates how $k$-Means clustering can be used beyond data analysis — for image compression.

Each color in an image can be treated as a data point in RGB space, and clustering reduces the number of colors while preserving the main structure.

**Load and visualize an image.**

```{r}
#| eval: false 
library(jpeg)
library(ggplot2)
library(dplyr)


# Read example image from R base (or provide your own file path)
url <- "https://upload.wikimedia.org/wikipedia/commons/3/3f/JPEG_example_flower.jpg"
temp <- tempfile(fileext = ".jpg")
download.file(url, temp, mode = "wb")
img <- readJPEG(temp)


# Convert to data frame
img_df <- data.frame(
R = as.vector(img[,,1]),
G = as.vector(img[,,2]),
B = as.vector(img[,,3])
)

# Sample only a few thousand pixels to avoid memory explosion
set.seed(123)
img_sample <- img_df %>% sample_n(5000)
```

**Determine the optimal number of clusters.**

Apply $k$-means clustering to the RGB values of the image (`img_sample`) for $k = 2$ to $10$, using `nstart = 10`.  Then, use both the **Elbow Method** and the **Silhouette Coefficient** to decide which $k$ gives the best balance between cluster compactness and separation. Do both methods suggest the same number of clusters? 

When you decide about the optimal $k$, with the following code, you can compare the original and compressed image. 

```{r}
#| eval: false 

set.seed(123)


k <- 'put the optimal value you found'
km_colors <- kmeans(img_sample, centers = k, nstart = 10)


# Replace each pixel with its cluster centroid color
compressed_img <- km_colors$centers[km_colors$cluster, ]
compressed_img <- array(compressed_img, dim = dim(img))

par(mfrow = c(1, 2))
plot(0, type = "n", xlim = c(0, 1), ylim = c(0, 1),
xlab = "", ylab = "", axes = FALSE, main = "Original Image")
rasterImage(img, 0, 0, 1, 1)


plot(0, type = "n", xlim = c(0, 1), ylim = c(0, 1),
xlab = "", ylab = "", axes = FALSE, main = paste("Compressed Image (k =", k, ")"))
rasterImage(compressed_img, 0, 0, 1, 1)
```

:::::

### $k$-Medoids Clustering

The $k$-medoids algorithm is a partitioning clustering method closely related to $k$-means, but instead of using the *mean (centroid)* of points to represent a cluster, it uses an *actual observation (medoid)* — the most centrally located data point within that cluster.

Formally, a medoid is the point whose average dissimilarity to all other points in the cluster is minimal. i.e,
$$
m_j = \arg\min_{x_i \in C_j} \sum_{x_k \in C_j} d(x_i,x_k)
$$
where:
- $C_j$ is the set of points assigned to cluster $j$
- $d(x_i,x_k)$ is a chosen distance or dissimilarity measure (e.g., Euclidean, Manhattan, etc.)

**Algorithm (PAM — Partitioning Around Medoids)**

The most common implementation of k-medoids is the PAM algorithm, which works as follows:
1. **Initialize:**
Select $k$ representative objects (medoids) randomly from the dataset.
2. **Assign:**
Assign each remaining point to the nearest medoid based on the chosen distance metric.
3. **Update:**
For each cluster, try swapping the medoid with another point from the same cluster and compute the total cost:
$$
\text{Cost} = \sum_{i=1}^n \min_j d(x_i, m_j)
$$
If a swap reduces the cost, keep it.
4. **Iterate:** 
Repeat assignment and update steps until the medoids no longer change.

:::{.callout-important}
The $k$-medoids algorithm minimizes a **sum of dissimilarities**, not squared distances.
Thus, the cluster representative is always a real observation, which makes the method robust when data contain noise, outliers, or mixed types.
:::

```{webr-r}
#| label: pam-basic
#| autorun: true
#| message: false
#| warning: false
#| fig-cap: "k-Medoids clustering using the PAM algorithm on the iris dataset."
#| out-width: "80%"

library(cluster)
library(ggplot2)

# Prepare data (use only numeric columns)
iris_data <- iris[, 1:4]

# Apply k-medoids clustering (PAM = Partitioning Around Medoids)
set.seed(123)
pam_result <- pam(iris_data, k = 3, metric = "euclidean")

# Add cluster labels to data
iris_pam <- iris
iris_pam$cluster <- as.factor(pam_result$clustering)

# Visualize clusters (Petal.Length vs Petal.Width)
ggplot(iris_pam, aes(Petal.Length, Petal.Width, color = cluster, shape = Species)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_point(data = as.data.frame(pam_result$medoids), 
             aes(Petal.Length, Petal.Width), 
             color = "black", size = 5, shape = 8) +
  labs(title = "k-Medoids Clustering (PAM)", 
       subtitle = "Medoids shown as black stars",
       x = "Petal Length", y = "Petal Width") +
  theme_minimal(base_size = 14)
```

As you see, the black stars represent the *medoids* — actual data points from the dataset.

Unlike $k$-means, the medoid is a real observation, not an artificial centroid.


**Visual Comparison: k-Means vs k-Medoids**
```{webr-r}
#| label: kmeans-vs-pam
#| autorun: true
#| fig-cap: "Comparison between k-Means and k-Medoids clustering on the same dataset."
#| out-width: true

set.seed(123)
kmeans_result <- kmeans(iris_data, centers = 3, nstart = 20)

iris_compare <- iris
iris_compare$kmeans_cluster <- as.factor(kmeans_result$cluster)
iris_compare$kmedoids_cluster <- as.factor(pam_result$clustering)

# Combine plots side by side
library(patchwork)

p1 <- ggplot(iris_compare, aes(Petal.Length, Petal.Width, color = kmeans_cluster)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_point(data = as.data.frame(kmeans_result$centers),
             aes(Petal.Length, Petal.Width),
             color = "black", size = 5, shape = 8) +
  labs(title = "k-Means", subtitle = "Centroids (means) shown as black stars") +
  theme_minimal(base_size = 13)

p2 <- ggplot(iris_compare, aes(Petal.Length, Petal.Width, color = kmedoids_cluster)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_point(data = as.data.frame(pam_result$medoids),
             aes(Petal.Length, Petal.Width),
             color = "black", size = 5, shape = 8) +
  labs(title = "k-Medoids (PAM)", subtitle = "Medoids (real points) shown as black stars") +
  theme_minimal(base_size = 13)

p1 + p2
```

#### Analysis of Heart Disease Dataset

The dataset has a mix of numerical, ordinal, and categorical features:
`Age`, `Sex`, `ChestPain`, `Thal`, `RestBP`, `Chol`, `MaxHR`, `AHD` (disease outcome), etc.

We compute Gower distance and run $k$-medoids.

```{webr-r}
#| label: pam-heart-optimal
#| fig-cap: "Finding optimal k and discriminative features in the Heart Disease dataset using PAM (Gower distance)."
#| autorun: true
#| message: false
#| warning: false

library(cluster)
library(ggplot2)
library(dplyr)
library(factoextra)

# Load dataset
data("heart")

# Compute Gower distance for mixed data
diss_heart <- daisy(heart, metric = "gower")

#-----------------------------
# 1️⃣ Find optimal number of clusters using silhouette
#-----------------------------
sil_width <- c()

for (k in 2:6) {
  pam_fit <- pam(diss_heart, k = k, diss = TRUE)
  sil_width[k] <- pam_fit$silinfo$avg.width
}

# Find best k (maximum silhouette width)
best_k <- which.max(sil_width)
best_k

# Plot silhouette curve
plot(2:6, sil_width[2:6], type = "b", pch = 19,
     xlab = "Number of clusters (k)",
     ylab = "Average silhouette width",
     main = "Choosing optimal number of clusters (PAM, Gower)")
abline(v = best_k, lty = 2, col = "red")
text(best_k + 0.2, max(sil_width, na.rm = TRUE) - 0.01,
     labels = paste("Best k =", best_k), col = "red")
```

```{webr-r}
#| label: pam-heart-optimal-1"
#| autorun: true
#| message: false
#| warning: false

set.seed(123)
pam_heart <- pam(diss_heart, k = best_k, diss = TRUE)

heart_clustered <- heart %>%
  mutate(Cluster = as.factor(pam_heart$clustering))
```


