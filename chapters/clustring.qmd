---
engine: knitr
filters: 
  - webr
---

```{r setup, include=FALSE}
required_packages <- c("cluster", "lsa")

for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, repos = "https://cloud.r-project.org")
    library(pkg, character.only = TRUE)
  }
}
```

# Clustering

Cluster analysis, or *Clustring*, is a technique used to find groups of objects such that the objects within the same group are similar (or closely realted) to one another, while the objects from different groups are dissimilar (or unrelated).

## Applications of Clustering

Clustering techniques are widely used across multiple disciplines to discover patterns, structure data, and support decision-making.  Below are some common and illustrative applications.

**Customer Segmentation**

In marketing and business analytics, clustering helps identify groups of customers with similar behaviors, preferences, or purchasing habits.  This segmentation allows companies to optimize advertising strategies, personalize product offerings, and design campaigns for specific focus groups.

**Example**: 
Grouping supermarket customers based on purchase frequency, product categories, and spending level.

**Web Visitor Segmentation**

Web analytics platforms use clustering to classify website visitors according to their browsing patterns, time spent on pages, or interaction behaviors. This segmentation supports personalized content delivery and improved user experience.

**Example**: 
Optimizing web navigation or recommendations for distinct *user segments* such as new visitors vs. returning users.

**Data Aggregation and Reduction**

Clustering can be used to represent large datasets by a smaller set of representative elements (centroids or medoids).  
This reduces computational complexity and facilitates data visualization.

**Example**:  
Reducing the color palette of an image to *k representative colors* using algorithms like *k-means*.

**Text Collection Organization**

In natural language processing (NLP), clustering is applied to group similar documents or texts into topics or themes.  
It is particularly useful when the number or nature of topics is *unknown in advance*.

**Example**:  
Grouping news articles, research abstracts, or emails into topic clusters.

**Biology and Taxonomy**

In biological sciences, clustering supports the classification of living organisms based on genetic, morphological, or behavioral similarities.  
It forms the basis for hierarchical structures such as *kingdom, phylum, class, order, family, genus,* and *species*.

**Example**:  
Clustering DNA sequences to identify genetic relationships among species.

**Information Retrieval**

In computer science and library systems, clustering helps organize large document collections to improve search and retrieval efficiency. 
By grouping related documents, search engines can return more *contextually relevant* results.

**Example**:  
Document clustering for semantic search or grouping research papers by field of study.

## Distance and Similarity in Clustering

The notion of *distance* or *similarity* lies at the heart of clustering. Since clustering aims to group similar observations together, we must have a way to measure how close or far two observations are from each other in the feature space. These measures quantify the degree of resemblance (similarity) or difference (dissimilarity) between data points.

> The main goal is to partition a set of data points into clusters where **intra-cluster** distances (distances between points within the same cluster) are **minimized**, and **inter-cluster** distances (distances between points from different clusters) are **maximized**.

Thus, the main goal of clustering can be rewritten as:

> The main goal is to partition a set of data points into clusters where intra-cluster similarities (similarities between points within the same cluster) are maximized, and inter-cluster similarities (similarities between points from different clusters) are minimized.

To avoid confusion, remember that *distance* and *similarity* are inverse concepts: when two objects are close (small distance), they are considered similar (high similarity).  
The table below summarizes their relationship and how each measure behaves in clustering.

| **Concept** | **Meaning** | **High Value Indicates** | **Low Value Indicates** | **Goal in Clustering** |
|:-------------|:------------|:--------------------------|:--------------------------|:------------------------|
| **Distance** | A measure of dissimilarity between two objects | Objects are far apart (dissimilar) | Objects are close together (similar) | **Minimize intra-cluster distances** and **maximize inter-cluster distances** |
| **Similarity** | A measure of closeness or resemblance between two objects | Objects are close together (similar) | Objects are far apart (dissimilar) | **Maximize intra-cluster similarities** and **minimize inter-cluster similarities** |

```{r}
#| label: similarity-vs-distance
#| fig-cap: "Relationship between distance and similarity — points in the same cluster have high similarity (low distance)."
#| echo: false
#| message: false
#| warning: false

library(ggplot2)
set.seed(123)

# Generate two clusters

n <- 30
cluster1 <- data.frame(x = rnorm(n, 2, 0.3), y = rnorm(n, 2, 0.3), cluster = "Cluster A")
cluster2 <- data.frame(x = rnorm(n, 6, 0.3), y = rnorm(n, 6, 0.3), cluster = "Cluster B")
df <- rbind(cluster1, cluster2)

# Pick two points to illustrate distance and similarity

p1 <- df[5, ]
p2 <- df[6, ]
p3 <- df[40, ]

# Plot

ggplot(df, aes(x, y, color = cluster)) +
geom_point(size = 3) +
geom_segment(aes(x = p1$x, y = p1$y, xend = p2$x, yend = p2$y),
color = "darkgreen", linewidth = 1.2,
arrow = arrow(length = unit(0.15, "inches"), ends = "both")) +
geom_segment(aes(x = p1$x, y = p1$y, xend = p3$x, yend = p3$y),
color = "red3", linewidth = 1.2,
arrow = arrow(length = unit(0.15, "inches"), ends = "both")) +
annotate("text", x = (p1$x + p2$x)/2, y = (p1$y + p2$y)/2 + 0.99,
label = "Intra-Cluster\nHigh similarity\n(Low distance)", color = "darkgreen", size = 4) +
annotate("text", x = (p1$x + p3$x)/2, y = (p1$y + p3$y)/2 + 0.9,
label = "Inter-Cluster\nLow similarity\n(High distance)", color = "red3", size = 4) +
theme_minimal() +
labs(title = "Distance vs Similarity",
subtitle = "Closer points → higher similarity (smaller distance)")
```

A **distance metric** $d(\mathbf{x}, \mathbf{y})$ measures the dissimilarity between two points $\mathbf{x}=(x_1, x_2, \ldots, x_p)$ and $\mathbf{y} = (y_1, y_2, \ldots, y_p)$.  Formally, a function $d: X \times X \rightarrow \mathbb{R}$ is a *metric* if it satisfies:
$$
\begin{aligned}
1.\;& d(\mathbf{x}, \mathbf{y}) \ge 0 && \text{(non-negativity)} \\
2.\;& d(\mathbf{x}, \mathbf{y}) = 0 \iff \mathbf{x} = \mathbf{y} && \text{(identity)} \\
3.\;& d(\mathbf{x}, \mathbf{y}) = d(\mathbf{y}, \mathbf{x}) && \text{(symmetry)} \\
4.\;& d(\mathbf{x}, \mathbf{y}) \le d(\mathbf{x}, \mathbf{z}) + d(\mathbf{z}, \mathbf{y}) && \text{(triangle inequality)}
\end{aligned}
$$

A **similarity measure** $s(\mathbf{x}, \mathbf{y})$ expresses how close or related two points are.  
It usually satisfies:
$$
\begin{aligned}
1.\;& s(\mathbf{x}, \mathbf{y}) \ge 0 && \text{(non-negativity)} \\
2.\;& s(\mathbf{x}, \mathbf{y}) = s(\mathbf{y}, \mathbf{x}) && \text{(symmetry)} \\
3.\;& s(\mathbf{x}, \mathbf{y}) \in [0, 1] && \text{(boundedness)} \\
4.\;& s(\mathbf{x}, \mathbf{x}) = 1 && \text{(maximum self-similarity)}
\end{aligned}
$$
A simple transformation links both concepts:
$$
s(\mathbf{x}, \mathbf{y}) = \frac{1}{1 + d(\mathbf{x}, \mathbf{y})}, \quad \text{where } s(\mathbf{x}, \mathbf{y}) \in [0, 1].
$$

### Common Distance Metrics

Different clustering algorithms may behave very differently depending on the metric used.  
Below are the most widely used **distance measures**.

#### **Euclidean Distance**

The most common metric, representing the straight-line distance between two points in $\mathbb{R}^p$:
$$
d_E(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{p} (x_i - y_i)^2}
$$

:::{.callout-note}
Euclidean distance is sensitive to scale; therefore, variables should usually be standardized before applying it.
:::

```{webr-r}
#| label: euclidean-distance
#| autorun: true 
#| warning: false 
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# Euclidean distance
d_E <- sqrt(sum((x - y)^2))
d_E
```
#### **Manhattan (or City-Block) Distance**
This distance sums the absolute differences across all coordinates:
$$
d_M(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{p} |x_i - y_i|
$$

:::{.callout-note}
The Manhattan distance is more robust to outliers and suitable when features represent grid-like or discrete steps.
:::

```{webr-r}
#| label: manhattan-distance
#| autorun: true
#| warning: false
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# Manhattan distance
d_M <- sum(abs(x - y))
d_M
```

#### **Minkowski Distance**

A generalization of Euclidean and Manhattan distances, controlled by a parameter $r$:
$$
d_r(\mathbf{x}, \mathbf{y}) = \left( \sum_{i=1}^{p} |x_i - y_i|^r \right)^{1/r}
$$

- When $r = 1$, it becomes Manhattan distance. 
   
- When $r = 2$, it becomes Euclidean distance.  

- Larger values of $r$ emphasize large coordinate differences.

```{webr-r}
#| label: manhattan-distance
#| autorun: true
#| warning: false
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# Minkowski distance
d_r <- function(x, y, r) {
  (sum(abs(x - y)^r))^(1/r)
}
cat('Manhattan distance:', d_r(x, y, r = 1), 'that should be equal to d_M which is', d_M, '\n')
cat('Euclidean distance:', d_r(x, y, r = 2), 'that should be equal to d_E which is', d_E, '\n')
cat('Minkowski Distance', d_r(x, y, r = 3))
```

#### **Chebyshev Distance**

Also known as the $L_\infty$ norm, this distance takes the largest coordinate difference:
$$
d_C(\mathbf{x}, \mathbf{y}) = \max_i |x_i - y_i|
$$
It measures how far apart two points are along the dimension where they differ most.
```{webr-r}
#| label: chebyshev-distance
#| autorun: true
#| warning: false
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# Chebyshev distance
d_C <- max(abs(x - y))
d_C
```

#### **Mahalanobis Distance**

A scale-invariant distance that accounts for correlations between variables:
$$
d_{Mah}(\mathbf{x}, \mathbf{y}) = \sqrt{(x - y)^{T} \boldsymbol{\Sigma}^{-1} (x - y)}
$$
where $\boldsymbol{\Sigma}$ is the covariance matrix of the data.
Two points with the same Mahalanobis distance from the mean are equally likely under a multivariate normal model, regardless of the variable scales or correlations.

```{webr-r}
#| label: mahalanobis-distance
#| autorun: true
#| warning: false
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# For Mahalanobis, we need covariance matrix (example with small dataset)
X <- matrix(c(2, 4, 5, 8, 3, 7), ncol = 2, byrow = TRUE)
cov_matrix <- cov(X)
d_mahalanobis <- sqrt(t(x - y) %*% solve(cov_matrix) %*% (x - y))
d_mahalanobis
```

:::{.callout-tip}

**Choosing an Appropriate Metric**

The choice of metric depends on the **data characteristics** and the **clustering goal**:

- Use **Euclidean** when features are continuous and scaled.  
- Use **Manhattan** for grid-like or sparse data.  
- Use **Mahalanobis** when features are correlated.  
- Use **Chebyshev** for problems sensitive to maximum deviations.  
- Use **Minkowski** for flexible control between Manhattan and Euclidean.
:::

#### Cosine and Correlation Distance

For data where *direction* or *orientation* matters more than magnitude — such as text represented by TF-IDF vectors or normalized embeddings — 
Euclidean distance is not ideal. Instead, we use *cosine similarity* or *correlation distance*.

The *cosine similarity* between two vectors $\mathbf{x}$ and $\mathbf{y}$ is defined as
$$
s_{\text{cosine}}(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \, \|\mathbf{y}\|}
$$
where $\mathbf{x} \cdot \mathbf{y}$ is the **dot product** of the two vectors, and $\|\mathbf{x}\|$ and $\|\mathbf{y}\|$ are their**Euclidean norms*.  

This similarity measures the *cosine of the angle* between the two vectors in a multidimensional space.  

- A value of **1** means the vectors point in the **same direction** (perfectly similar).
   
- A value of **0** means the vectors are **orthogonal** (no similarity). 
   
- A value of **–1** means they point in **opposite directions** (perfectly dissimilar).

The corresponding *cosine distance* is:
$$
d_{\text{cosine}}(\mathbf{x}, \mathbf{y}) = 1 - s_{\text{cosine}}(\mathbf{x}, \mathbf{y})
$$

```{webr-r}
#| label: cosine-distance
#| autorun: true
#| message: false
#| warning: false

library(lsa)  # for cosine similarity

x <- c(1, 2, 3)
y <- c(2, 4, 6)
z <- c(3, 0, 0)

cos_xy <- cosine(x, y)
cos_xz <- cosine(x, z)

cat("Cosine similarity (x,y):", round(cos_xy, 3), "\n")
cat("Cosine similarity (x,z):", round(cos_xz, 3), "\n")
cat("Cosine distance (x,y):", round(1 - cos_xy, 3), "\n")
cat("Cosine distance (x,z):", round(1 - cos_xz, 3), "\n")
```

:::{.callout.tip}
Cosine-based metrics are particularly useful for *directional data*, where the *pattern* or *orientation* of features matters more than their magnitude —  
for example, in *text mining* (TF-IDF vectors), *recommendation systems*, or *image feature embeddings*.
:::

### Distances for Binary Data

When the data are **binary** (0 or 1), such as presence/absence, success/failure, or yes/no attributes,  specialized similarity and distance measures are used.

Let $o_1, o_2 \in \{0,1\}^d$ be two binary observations described by $d$ attributes.

We define:
$$
\begin{aligned}
f_{11} &= \text{number of attributes where } o_1 = 1 \text{ and } o_2 = 1, \\
f_{00} &= \text{number of attributes where } o_1 = 0 \text{ and } o_2 = 0, \\
f_{10} &= \text{number of attributes where } o_1 = 1 \text{ and } o_2 = 0, \\
f_{01} &= \text{number of attributes where } o_1 = 0 \text{ and } o_2 = 1.
\end{aligned}
$$

#### **Simple Matching Coefficient (Similarity)**

$$
s_{SMC}(o_1, o_2) = \frac{f_{11} + f_{00}}{f_{11} + f_{00} + f_{10} + f_{01}} = \frac{f_{11} + f_{00}}{d}
$$

This coefficient measures the proportion of attributes where the two observations match — whether both are 1s or both are 0s.

#### **Simple Matching Distance**
$$
d_{SMC}(o_1, o_2) = 1 - s_{SMC}(o_1, o_2) = \frac{f_{01} + f_{10}}{d}
$$
This represents the proportion of mismatches between two binary objects.

```{webr-r}
#| label: smc-distance-example
#| autorun: true
#| message: false
#| warning: false

# Two binary observations (e.g., presence/absence of features)
o1 <- c(1, 0, 1, 1, 0, 0, 1)
o2 <- c(1, 1, 0, 1, 0, 0, 0)

# Compute the components
f11 <- sum(o1 == 1 & o2 == 1)
f00 <- sum(o1 == 0 & o2 == 0)
f10 <- sum(o1 == 1 & o2 == 0)
f01 <- sum(o1 == 0 & o2 == 1)
d <- length(o1)

# Similarity and distance
s_SMC <- (f11 + f00) / d
d_SMC <- (f01 + f10) / d  # same as 1 - s_SMC

cat("f11 =", f11, "f00 =", f00, "f10 =", f10, "f01 =", f01, "\n")
cat("Simple Matching Coefficient (similarity):", round(s_SMC, 3), "\n")
cat("Simple Matching Distance:", round(d_SMC, 3), "\n")
```

:::{.callout-note}

- If two binary vectors are identical,$s_{SMC} = 1$ and $d_{sMC} = 0$.

- If they are completely opposite, $s_{SMC} = 0$  and $d_{sMC} = 1$.

- The SMC treats $0$s and $1$s symmetrically, so it is best used when both states are equally meaningful.
:::

### Distances for Categorical Data

When data contain **categorical** or **mixed-type variables**, standard numeric distances (like Euclidean) are not suitable.  
Instead, we use measures that handle **qualitative comparisons** directly.

Let  $\mathbf{x} = (x_1, \ldots, x_p)$ and $\mathbf{y} = (y_1, \ldots, y_p)$ be two observations described by $p$ categorical or mixed-type attributes.

#### **Hamming Distance**

The **Hamming distance** counts the number of mismatches between two vectors:
$$
\text{d}_{Hamming}(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{p} \delta(x_i, y_i)
$$
where
$$
\delta(x_i, y_i) =
\begin{cases}
0, & \text{if } x_i = y_i \\
1, & \text{if } x_i \neq y_i
\end{cases}
$$
It measures how many positions differ between two categorical strings or binary vectors.

#### **Gower Distance**

The **Gower distance** allows comparing **mixed-type data** (numerical, categorical, ordinal). For each variable $X_i$, a partial distance is computed, and then averaged:
$$
\text{d}_{Gower}(\mathbf{x}, \mathbf{y}) =
\frac{1}{p} \sum_{i=1}^{p} \delta_i(x_i, y_i)
$$
with
$$
\delta_i(x_i, y_i) =
\begin{cases}
0, & \text{if } x_i = y_i \\
1, & \text{if } X_i \text{ is categorical and } x_i \neq y_i \\
\frac{|x_i - y_i|}{R_i}, & \text{if } X_i \text{ is interval scale} \\
\frac{|\text{rank}(x_i) - \text{rank}(y_i)|}{n - 1}, & \text{if } X_i \text{ is ordinal scale}
\end{cases}
$$
This flexibility makes Gower distance especially useful in real datasets with mixed variable types.

```{webr-r}
#| label: categorical-distances
#| message: false
#| warning: false
#| autorun: true

# Example categorical data
x <- c("Red", "Small", "Circle")
y <- c("Blue", "Small", "Square")

# ----- Hamming Distance -----
hamming_distance <- sum(x != y)
print(paste("Hamming distance:", hamming_distance))

# ----- Gower Distance -----
library(cluster)

data <- data.frame(
  Color = c("Red", "Blue"),
  Size = c("Small", "Small"),
  Shape = c("Circle", "Square")
)

# Convert all character columns to factors
data[] <- lapply(data, as.factor)

# Compute Gower distance
gower_dist <- daisy(data, metric = "gower")
gower_matrix <- round(as.matrix(gower_dist), 3)

print("Gower distance matrix:")
print(gower_matrix)

```



::::: panel-tabset
## Exercise 1

**1. Create two 3-dimensional points and calculate the distance**

Let  $A = (1, 3, 5)$ and $B = (4, 9, 6)$.

Compute manually and in R the following distances between $A$ and $B$:

- Euclidean  

- Manhattan  

- Minkowski with $r = 3$  

- Chebyshev  

**2. Explore sensitivity to scale**

Multiply the second coordinate of both points by 10 and recompute all distances. How does this change the results?  

## Exercise 2

Two species are described by four binary characteristics:

| Feature | Species A | Species B |
|----------|------------|------------|
| Has fins | 1 | 1 |
| Lays eggs | 1 | 0 |
| Has scales | 1 | 1 |
| Warm-blooded | 0 | 1 |

1. Compute manually:  
   - \( f_{11}, f_{00}, f_{10}, f_{01} \)
   - The *Simple Matching Coefficient (SMC)*  
   - The *Simple Matching Distance (SMD)* 

2. Verify your results using R.

3. Which pair of features contributes to dissimilarity?

## Exercise 3

Two students rate three projects as “Good”, “Average”, or “Poor”:

| Project | Student 1 | Student 2 |
|----------|------------|------------|
| A | Good | Good |
| B | Poor | Average |
| C | Good | Poor |

Compute the *Hamming distance* (number of mismatches).


## Exercise 4

Two students rate three projects as “Good”, “Average”, or “Poor”:

| Project | Student 1 | Student 2 |
|----------|------------|------------|
| A | Good | Good |
| B | Poor | Average |
| C | Good | Poor |

Compute the *Hamming distance* (number of mismatches).

**Hint**: Use 
```{r}
#| eval: false
data[] <- lapply(data, function(col) if (is.character(col)) as.factor(col) else col)
```

:::::


## Partition-Based Clustering: K-Means and K-Medoids

### $k$-Means Clustering

K-Means aims to partition $n$ observations into $k$ clusters such that each observation belongs to the cluster with the nearest **centroid** (mean of points).

Each cluster is represented by its *centroid*, which may not be an actual data point.

**Algorithm**

Given a dataset $\mathbf{X}= \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\}$:

1. Choose the number of clusters $k$.  
2. Initialize $k$ centroids randomly.  
3. **Assign step:** Assign each observation $x_i$ to the nearest centroid:
4. $$
   c_i = \arg\min_j \|x_i - \mu_j\|^2
   $$
5. **Update step:** Recalculate each centroid as the mean of points in its cluster:
   $$
   \mu_j = \frac{1}{n_j} \sum_{x_i \in C_j} x_i
   $$
6. Repeat steps 3–4 until centroids stop changing (convergence).

For better understanding, see this [video](https://blog.dailydoseofds.com/p/an-animated-guide-to-kmeans)

```{webr-r}
#| label: kmeans-example
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

set.seed(42)

# Generate synthetic 2D data
x <- rbind(
  matrix(rnorm(100, mean = 0, sd = 0.3), ncol = 2),
  matrix(rnorm(100, mean = 3, sd = 0.3), ncol = 2)
)
colnames(x) <- c("x", "y")

# Apply k-means with 2 clusters
km <- kmeans(x, centers = 2, nstart = 20)

# Visualize results
library(ggplot2)
data <- data.frame(x, cluster = factor(km$cluster))
ggplot(data, aes(x, y, color = cluster)) +
  geom_point(size = 3) +
  geom_point(data = as.data.frame(km$centers), aes(x, y), color = "black", size = 5, shape = 8) +
  labs(title = "K-Means Clustering",
       subtitle = "Centroids shown as black stars") +
  theme_minimal()
```

### Properties and Limitations of $k$-Means

Although $k$$-means is one of the most popular clustering algorithms, its effectiveness depends on the structure and scale of the data. Understanding its limitations helps decide when to use it — and when to choose a more robust alternative like $k$-Medoids.

##### ** Data Type and Cluster Shape**

$k$-Means works best when:

- All variables are **
- numeric**
-  and measured on a comparable scale. 
   
- The clusters are *spherical* (or roughly circular in 2D).  
  
- Each cluster has *similar variance* and *density*.

When clusters are elongated, overlapping, or non-spherical, $k$-Means may assign points incorrectly because it relies on *Euclidean distance*.

##### **Sensitivity to Scale**

Since Euclidean distance depends directly on the magnitude of the variables, features with larger numerical ranges will dominate the distance calculation.

For example, in a dataset with *income* (in euros) and *age* (in years), income will completely overshadow age unless both are standardized.

Always *standardize or normalize* variables before running $k$-Means:
```{r}
#| label: kmeans-scale
#| eval: false
#| message: false
#| warning: false

# Standardization example
x_scaled <- scale(x)
kmeans(x_scaled, centers = 2)
```

### Effect of Scaling on $k$-Means Clustering

$k$-means relies on *Euclidean distance*, which is sensitive to the magnitude of each variable. If one variable has a much larger scale than others, it can dominate the clustering process — even if it’s less informative.

The following example illustrates this issue.

#### Example: Unscaled vs. Scaled Data

We will create a simple dataset with two features:

- `income` (in euros, large values)

- `age` (in years, small values)

```{webr-r}
#| label: kmeans-scaling-demo
#| autorun: true
#| message: false
#| warning: false

set.seed(123)
library(ggplot2)
library(patchwork)

# Create dataset
data_scale <- data.frame(
  income = c(rnorm(50, 30000, 4000), rnorm(50, 60000, 4000)),
  age    = c(rnorm(50, 25, 2), rnorm(50, 45, 2))
)

# --- (1) K-Means without scaling ---
km_unscaled <- kmeans(data_scale, centers = 2, nstart = 20)
data_scale$cluster_unscaled <- factor(km_unscaled$cluster)

# --- (2) K-Means with scaling ---
data_scaled <- scale(data_scale[, c("income", "age")])
km_scaled <- kmeans(data_scaled, centers = 2, nstart = 20)

# Save scaled cluster results back to original coordinates
data_scale$cluster_scaled <- factor(km_scaled$cluster)

p1 <- ggplot(data_scale, aes(income, age, color = cluster_unscaled)) +
geom_point(size = 3) +
labs(title = "K-Means Without Scaling",
subtitle = "Clusters dominated by 'income' variable") +
theme_minimal() +
theme(legend.position = "none")




p2 <- ggplot(data_scale, aes(income, age, color = cluster_scaled)) +
geom_point(size = 3) +
labs(title = "K-Means With Scaling",
subtitle = "Clusters reflect both 'income' and 'age'") +
theme_minimal() +
theme(legend.position = "none")




p1 + p2
```

**Without scaling**:
The `income` variable dominates, because it has a much larger numeric range.
The clusters are separated mostly along the horizontal (income) axis.

**With scaling**:
Both `income` and `age` contribute equally to the distance measure.
The clusters now reflect genuine structure in both dimensions.