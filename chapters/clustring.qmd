---
engine: knitr
filters: 
  - webr
---

```{r setup, include=FALSE}
required_packages <- c("cluster", "lsa", "GGally")

for (pkg in required_packages) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, repos = "https://cloud.r-project.org")
    library(pkg, character.only = TRUE)
  }
}
```

# Clustering

Cluster analysis, or *Clustring*, is a technique used to find groups of objects such that the objects within the same group are similar (or closely realted) to one another, while the objects from different groups are dissimilar (or unrelated).

## Applications of Clustering

Clustering techniques are widely used across multiple disciplines to discover patterns, structure data, and support decision-making.  Below are some common and illustrative applications.

**Customer Segmentation**

In marketing and business analytics, clustering helps identify groups of customers with similar behaviors, preferences, or purchasing habits.  This segmentation allows companies to optimize advertising strategies, personalize product offerings, and design campaigns for specific focus groups.

**Example**: 
Grouping supermarket customers based on purchase frequency, product categories, and spending level.

**Web Visitor Segmentation**

Web analytics platforms use clustering to classify website visitors according to their browsing patterns, time spent on pages, or interaction behaviors. This segmentation supports personalized content delivery and improved user experience.

**Example**: 
Optimizing web navigation or recommendations for distinct *user segments* such as new visitors vs. returning users.

**Data Aggregation and Reduction**

Clustering can be used to represent large datasets by a smaller set of representative elements (centroids or medoids).  
This reduces computational complexity and facilitates data visualization.

**Example**:  
Reducing the color palette of an image to *k representative colors* using algorithms like *k-means*.

**Text Collection Organization**

In natural language processing (NLP), clustering is applied to group similar documents or texts into topics or themes.  
It is particularly useful when the number or nature of topics is *unknown in advance*.

**Example**:  
Grouping news articles, research abstracts, or emails into topic clusters.

**Biology and Taxonomy**

In biological sciences, clustering supports the classification of living organisms based on genetic, morphological, or behavioral similarities.  
It forms the basis for hierarchical structures such as *kingdom, phylum, class, order, family, genus,* and *species*.

**Example**:  
Clustering DNA sequences to identify genetic relationships among species.

**Information Retrieval**

In computer science and library systems, clustering helps organize large document collections to improve search and retrieval efficiency. 
By grouping related documents, search engines can return more *contextually relevant* results.

**Example**:  
Document clustering for semantic search or grouping research papers by field of study.

## Distance and Similarity in Clustering

The notion of *distance* or *similarity* lies at the heart of clustering. Since clustering aims to group similar observations together, we must have a way to measure how close or far two observations are from each other in the feature space. These measures quantify the degree of resemblance (similarity) or difference (dissimilarity) between data points.

> The main goal is to partition a set of data points into clusters where **intra-cluster** distances (distances between points within the same cluster) are **minimized**, and **inter-cluster** distances (distances between points from different clusters) are **maximized**.

Thus, the main goal of clustering can be rewritten as:

> The main goal is to partition a set of data points into clusters where intra-cluster similarities (similarities between points within the same cluster) are maximized, and inter-cluster similarities (similarities between points from different clusters) are minimized.

To avoid confusion, remember that *distance* and *similarity* are inverse concepts: when two objects are close (small distance), they are considered similar (high similarity).  
The table below summarizes their relationship and how each measure behaves in clustering.

| **Concept** | **Meaning** | **High Value Indicates** | **Low Value Indicates** | **Goal in Clustering** |
|:-------------|:------------|:--------------------------|:--------------------------|:------------------------|
| **Distance** | A measure of dissimilarity between two objects | Objects are far apart (dissimilar) | Objects are close together (similar) | **Minimize intra-cluster distances** and **maximize inter-cluster distances** |
| **Similarity** | A measure of closeness or resemblance between two objects | Objects are close together (similar) | Objects are far apart (dissimilar) | **Maximize intra-cluster similarities** and **minimize inter-cluster similarities** |

```{r}
#| label: similarity-vs-distance
#| fig-cap: "Relationship between distance and similarity — points in the same cluster have high similarity (low distance)."
#| echo: false
#| message: false
#| warning: false

library(ggplot2)
set.seed(123)

# Generate two clusters

n <- 30
cluster1 <- data.frame(x = rnorm(n, 2, 0.3), y = rnorm(n, 2, 0.3), cluster = "Cluster A")
cluster2 <- data.frame(x = rnorm(n, 6, 0.3), y = rnorm(n, 6, 0.3), cluster = "Cluster B")
df <- rbind(cluster1, cluster2)

# Pick two points to illustrate distance and similarity

p1 <- df[5, ]
p2 <- df[6, ]
p3 <- df[40, ]

# Plot

ggplot(df, aes(x, y, color = cluster)) +
geom_point(size = 3) +
geom_segment(aes(x = p1$x, y = p1$y, xend = p2$x, yend = p2$y),
color = "darkgreen", linewidth = 1.2,
arrow = arrow(length = unit(0.15, "inches"), ends = "both")) +
geom_segment(aes(x = p1$x, y = p1$y, xend = p3$x, yend = p3$y),
color = "red3", linewidth = 1.2,
arrow = arrow(length = unit(0.15, "inches"), ends = "both")) +
annotate("text", x = (p1$x + p2$x)/2, y = (p1$y + p2$y)/2 + 0.99,
label = "Intra-Cluster\nHigh similarity\n(Low distance)", color = "darkgreen", size = 4) +
annotate("text", x = (p1$x + p3$x)/2, y = (p1$y + p3$y)/2 + 0.9,
label = "Inter-Cluster\nLow similarity\n(High distance)", color = "red3", size = 4) +
theme_minimal() +
labs(title = "Distance vs Similarity",
subtitle = "Closer points → higher similarity (smaller distance)")
```

A **distance metric** $d(\mathbf{x}, \mathbf{y})$ measures the dissimilarity between two points $\mathbf{x}=(x_1, x_2, \ldots, x_p)$ and $\mathbf{y} = (y_1, y_2, \ldots, y_p)$.  Formally, a function $d: X \times X \rightarrow \mathbb{R}$ is a *metric* if it satisfies:
$$
\begin{aligned}
1.\;& d(\mathbf{x}, \mathbf{y}) \ge 0 && \text{(non-negativity)} \\
2.\;& d(\mathbf{x}, \mathbf{y}) = 0 \iff \mathbf{x} = \mathbf{y} && \text{(identity)} \\
3.\;& d(\mathbf{x}, \mathbf{y}) = d(\mathbf{y}, \mathbf{x}) && \text{(symmetry)} \\
4.\;& d(\mathbf{x}, \mathbf{y}) \le d(\mathbf{x}, \mathbf{z}) + d(\mathbf{z}, \mathbf{y}) && \text{(triangle inequality)}
\end{aligned}
$$

A **similarity measure** $s(\mathbf{x}, \mathbf{y})$ expresses how close or related two points are.  
It usually satisfies:
$$
\begin{aligned}
1.\;& s(\mathbf{x}, \mathbf{y}) \ge 0 && \text{(non-negativity)} \\
2.\;& s(\mathbf{x}, \mathbf{y}) = s(\mathbf{y}, \mathbf{x}) && \text{(symmetry)} \\
3.\;& s(\mathbf{x}, \mathbf{y}) \in [0, 1] && \text{(boundedness)} \\
4.\;& s(\mathbf{x}, \mathbf{x}) = 1 && \text{(maximum self-similarity)}
\end{aligned}
$$
A simple transformation links both concepts:
$$
s(\mathbf{x}, \mathbf{y}) = \frac{1}{1 + d(\mathbf{x}, \mathbf{y})}, \quad \text{where } s(\mathbf{x}, \mathbf{y}) \in [0, 1].
$$

### Common Distance Metrics

Different clustering algorithms may behave very differently depending on the metric used.  
Below are the most widely used **distance measures**.

#### **Euclidean Distance**

The most common metric, representing the straight-line distance between two points in $\mathbb{R}^p$:
$$
d_E(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{p} (x_i - y_i)^2}
$$

:::{.callout-note}
Euclidean distance is sensitive to scale; therefore, variables should usually be standardized before applying it.
:::

```{webr-r}
#| label: euclidean-distance
#| autorun: true 
#| warning: false 
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# Euclidean distance
d_E <- sqrt(sum((x - y)^2))
d_E
```
#### **Manhattan (or City-Block) Distance**
This distance sums the absolute differences across all coordinates:
$$
d_M(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{p} |x_i - y_i|
$$

:::{.callout-note}
The Manhattan distance is more robust to outliers and suitable when features represent grid-like or discrete steps.
:::

```{webr-r}
#| label: manhattan-distance
#| autorun: true
#| warning: false
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# Manhattan distance
d_M <- sum(abs(x - y))
d_M
```

#### **Minkowski Distance**

A generalization of Euclidean and Manhattan distances, controlled by a parameter $r$:
$$
d_r(\mathbf{x}, \mathbf{y}) = \left( \sum_{i=1}^{p} |x_i - y_i|^r \right)^{1/r}
$$

- When $r = 1$, it becomes Manhattan distance. 
   
- When $r = 2$, it becomes Euclidean distance.  

- Larger values of $r$ emphasize large coordinate differences.

```{webr-r}
#| label: manhattan-distance
#| autorun: true
#| warning: false
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# Minkowski distance
d_r <- function(x, y, r) {
  (sum(abs(x - y)^r))^(1/r)
}
cat('Manhattan distance:', d_r(x, y, r = 1), 'that should be equal to d_M which is', d_M, '\n')
cat('Euclidean distance:', d_r(x, y, r = 2), 'that should be equal to d_E which is', d_E, '\n')
cat('Minkowski Distance', d_r(x, y, r = 3))
```

#### **Chebyshev Distance**

Also known as the $L_\infty$ norm, this distance takes the largest coordinate difference:
$$
d_C(\mathbf{x}, \mathbf{y}) = \max_i |x_i - y_i|
$$
It measures how far apart two points are along the dimension where they differ most.
```{webr-r}
#| label: chebyshev-distance
#| autorun: true
#| warning: false
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# Chebyshev distance
d_C <- max(abs(x - y))
d_C
```

#### **Mahalanobis Distance**

A scale-invariant distance that accounts for correlations between variables:
$$
d_{Mah}(\mathbf{x}, \mathbf{y}) = \sqrt{(x - y)^{T} \boldsymbol{\Sigma}^{-1} (x - y)}
$$
where $\boldsymbol{\Sigma}$ is the covariance matrix of the data.
Two points with the same Mahalanobis distance from the mean are equally likely under a multivariate normal model, regardless of the variable scales or correlations.

```{webr-r}
#| label: mahalanobis-distance
#| autorun: true
#| warning: false
#| message: false

x <- c(2, 4)
y <- c(5, 8)

# For Mahalanobis, we need covariance matrix (example with small dataset)
X <- matrix(c(2, 4, 5, 8, 3, 7), ncol = 2, byrow = TRUE)
cov_matrix <- cov(X)
d_mahalanobis <- sqrt(t(x - y) %*% solve(cov_matrix) %*% (x - y))
d_mahalanobis
```

:::{.callout-tip}

**Choosing an Appropriate Metric**

The choice of metric depends on the **data characteristics** and the **clustering goal**:

- Use **Euclidean** when features are continuous and scaled.  
- Use **Manhattan** for grid-like or sparse data.  
- Use **Mahalanobis** when features are correlated.  
- Use **Chebyshev** for problems sensitive to maximum deviations.  
- Use **Minkowski** for flexible control between Manhattan and Euclidean.
:::

#### Cosine and Correlation Distance

For data where *direction* or *orientation* matters more than magnitude — such as text represented by TF-IDF vectors or normalized embeddings — 
Euclidean distance is not ideal. Instead, we use *cosine similarity* or *correlation distance*.

The *cosine similarity* between two vectors $\mathbf{x}$ and $\mathbf{y}$ is defined as
$$
s_{\text{cosine}}(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{\|\mathbf{x}\| \, \|\mathbf{y}\|}
$$
where $\mathbf{x} \cdot \mathbf{y}$ is the **dot product** of the two vectors, and $\|\mathbf{x}\|$ and $\|\mathbf{y}\|$ are their**Euclidean norms*.  

This similarity measures the *cosine of the angle* between the two vectors in a multidimensional space.  

- A value of **1** means the vectors point in the **same direction** (perfectly similar).
   
- A value of **0** means the vectors are **orthogonal** (no similarity). 
   
- A value of **–1** means they point in **opposite directions** (perfectly dissimilar).

The corresponding *cosine distance* is:
$$
d_{\text{cosine}}(\mathbf{x}, \mathbf{y}) = 1 - s_{\text{cosine}}(\mathbf{x}, \mathbf{y})
$$

```{webr-r}
#| label: cosine-distance
#| autorun: true
#| message: false
#| warning: false

library(lsa)  # for cosine similarity

x <- c(1, 2, 3)
y <- c(2, 4, 6)
z <- c(3, 0, 0)

cos_xy <- cosine(x, y)
cos_xz <- cosine(x, z)

cat("Cosine similarity (x,y):", round(cos_xy, 3), "\n")
cat("Cosine similarity (x,z):", round(cos_xz, 3), "\n")
cat("Cosine distance (x,y):", round(1 - cos_xy, 3), "\n")
cat("Cosine distance (x,z):", round(1 - cos_xz, 3), "\n")
```

:::{.callout.tip}
Cosine-based metrics are particularly useful for *directional data*, where the *pattern* or *orientation* of features matters more than their magnitude —  
for example, in *text mining* (TF-IDF vectors), *recommendation systems*, or *image feature embeddings*.
:::

### Distances for Binary Data

When the data are **binary** (0 or 1), such as presence/absence, success/failure, or yes/no attributes,  specialized similarity and distance measures are used.

Let $o_1, o_2 \in \{0,1\}^d$ be two binary observations described by $d$ attributes.

We define:
$$
\begin{aligned}
f_{11} &= \text{number of attributes where } o_1 = 1 \text{ and } o_2 = 1, \\
f_{00} &= \text{number of attributes where } o_1 = 0 \text{ and } o_2 = 0, \\
f_{10} &= \text{number of attributes where } o_1 = 1 \text{ and } o_2 = 0, \\
f_{01} &= \text{number of attributes where } o_1 = 0 \text{ and } o_2 = 1.
\end{aligned}
$$

#### **Simple Matching Coefficient (Similarity)**

$$
s_{SMC}(o_1, o_2) = \frac{f_{11} + f_{00}}{f_{11} + f_{00} + f_{10} + f_{01}} = \frac{f_{11} + f_{00}}{d}
$$

This coefficient measures the proportion of attributes where the two observations match — whether both are 1s or both are 0s.

#### **Simple Matching Distance**
$$
d_{SMC}(o_1, o_2) = 1 - s_{SMC}(o_1, o_2) = \frac{f_{01} + f_{10}}{d}
$$
This represents the proportion of mismatches between two binary objects.

```{webr-r}
#| label: smc-distance-example
#| autorun: true
#| message: false
#| warning: false

# Two binary observations (e.g., presence/absence of features)
o1 <- c(1, 0, 1, 1, 0, 0, 1)
o2 <- c(1, 1, 0, 1, 0, 0, 0)

# Compute the components
f11 <- sum(o1 == 1 & o2 == 1)
f00 <- sum(o1 == 0 & o2 == 0)
f10 <- sum(o1 == 1 & o2 == 0)
f01 <- sum(o1 == 0 & o2 == 1)
d <- length(o1)

# Similarity and distance
s_SMC <- (f11 + f00) / d
d_SMC <- (f01 + f10) / d  # same as 1 - s_SMC

cat("f11 =", f11, "f00 =", f00, "f10 =", f10, "f01 =", f01, "\n")
cat("Simple Matching Coefficient (similarity):", round(s_SMC, 3), "\n")
cat("Simple Matching Distance:", round(d_SMC, 3), "\n")
```

:::{.callout-note}

- If two binary vectors are identical,$s_{SMC} = 1$ and $d_{sMC} = 0$.

- If they are completely opposite, $s_{SMC} = 0$  and $d_{sMC} = 1$.

- The SMC treats $0$s and $1$s symmetrically, so it is best used when both states are equally meaningful.
:::

### Distances for Categorical Data

When data contain **categorical** or **mixed-type variables**, standard numeric distances (like Euclidean) are not suitable.  
Instead, we use measures that handle **qualitative comparisons** directly.

Let  $\mathbf{x} = (x_1, \ldots, x_p)$ and $\mathbf{y} = (y_1, \ldots, y_p)$ be two observations described by $p$ categorical or mixed-type attributes.

#### **Hamming Distance**

The **Hamming distance** counts the number of mismatches between two vectors:
$$
\text{d}_{Hamming}(\mathbf{x}, \mathbf{y}) = \sum_{i=1}^{p} \delta(x_i, y_i)
$$
where
$$
\delta(x_i, y_i) =
\begin{cases}
0, & \text{if } x_i = y_i \\
1, & \text{if } x_i \neq y_i
\end{cases}
$$
It measures how many positions differ between two categorical strings or binary vectors.

#### **Gower Distance**

The **Gower distance** allows comparing **mixed-type data** (numerical, categorical, ordinal). For each variable $X_i$, a partial distance is computed, and then averaged:
$$
\text{d}_{Gower}(\mathbf{x}, \mathbf{y}) =
\frac{1}{p} \sum_{i=1}^{p} \delta_i(x_i, y_i)
$$
with
$$
\delta_i(x_i, y_i) =
\begin{cases}
0, & \text{if } x_i = y_i \\
1, & \text{if } X_i \text{ is categorical and } x_i \neq y_i \\
\frac{|x_i - y_i|}{R_i}, & \text{if } X_i \text{ is interval scale} \\
\frac{|\text{rank}(x_i) - \text{rank}(y_i)|}{n - 1}, & \text{if } X_i \text{ is ordinal scale}
\end{cases}
$$
This flexibility makes Gower distance especially useful in real datasets with mixed variable types.

```{webr-r}
#| label: categorical-distances
#| message: false
#| warning: false
#| autorun: true

# Example categorical data
x <- c("Red", "Small", "Circle")
y <- c("Blue", "Small", "Square")

# ----- Hamming Distance -----
hamming_distance <- sum(x != y)
print(paste("Hamming distance:", hamming_distance))

# ----- Gower Distance -----
library(cluster)

data <- data.frame(
  Color = c("Red", "Blue"),
  Size = c("Small", "Small"),
  Shape = c("Circle", "Square")
)

# Convert all character columns to factors
data[] <- lapply(data, as.factor)

# Compute Gower distance
gower_dist <- daisy(data, metric = "gower")
gower_matrix <- round(as.matrix(gower_dist), 3)

print("Gower distance matrix:")
print(gower_matrix)

```



::::: panel-tabset
## Exercise 1

**1. Create two 3-dimensional points and calculate the distance**

Let  $A = (1, 3, 5)$ and $B = (4, 9, 6)$.

Compute manually and in R the following distances between $A$ and $B$:

- Euclidean  

- Manhattan  

- Minkowski with $r = 3$  

- Chebyshev  

**2. Explore sensitivity to scale**

Multiply the second coordinate of both points by 10 and recompute all distances. How does this change the results?  

## Exercise 2

Two species are described by four binary characteristics:

| Feature | Species A | Species B |
|----------|------------|------------|
| Has fins | 1 | 1 |
| Lays eggs | 1 | 0 |
| Has scales | 1 | 1 |
| Warm-blooded | 0 | 1 |

1. Compute manually:  
   - \( f_{11}, f_{00}, f_{10}, f_{01} \)
   - The *Simple Matching Coefficient (SMC)*  
   - The *Simple Matching Distance (SMD)* 

2. Verify your results using R.

3. Which pair of features contributes to dissimilarity?

## Exercise 3

Two students rate three projects as “Good”, “Average”, or “Poor”:

| Project | Student 1 | Student 2 |
|----------|------------|------------|
| A | Good | Good |
| B | Poor | Average |
| C | Good | Poor |

Compute the *Hamming distance* (number of mismatches).


## Exercise 4

Two students rate three projects as “Good”, “Average”, or “Poor”:

| Project | Student 1 | Student 2 |
|----------|------------|------------|
| A | Good | Good |
| B | Poor | Average |
| C | Good | Poor |

Compute the *Hamming distance* (number of mismatches).

**Hint**: Use 
```{r}
#| eval: false
data[] <- lapply(data, function(col) if (is.character(col)) as.factor(col) else col)
```

:::::


## Partition-Based Clustering: $k$-Means and $k$-Medoids

### $k$-Means Clustering

K-Means aims to partition $n$ observations into $k$ clusters such that each observation belongs to the cluster with the nearest **centroid** (mean of points).

Each cluster is represented by its *centroid*, which may not be an actual data point.

**Algorithm**

Given a dataset $\mathbf{X}= \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\}$:

1. Choose the number of clusters $k$.  
2. Initialize $k$ centroids randomly.  
3. **Assign step:** Assign each observation $x_i$ to the nearest centroid:
4. $$
   c_i = \arg\min_j \|x_i - \mu_j\|^2
   $$
5. **Update step:** Recalculate each centroid as the mean of points in its cluster:
   $$
   \mu_j = \frac{1}{n_j} \sum_{x_i \in C_j} x_i
   $$
6. Repeat steps 3–4 until centroids stop changing (convergence).

For better understanding, see this [video](https://blog.dailydoseofds.com/p/an-animated-guide-to-kmeans)

```{webr-r}
#| label: kmeans-example
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

set.seed(42)

# Generate synthetic 2D data
x <- rbind(
  matrix(rnorm(100, mean = 0, sd = 0.3), ncol = 2),
  matrix(rnorm(100, mean = 3, sd = 0.3), ncol = 2)
)
colnames(x) <- c("x", "y")

# Apply k-means with 2 clusters
km <- kmeans(x, centers = 2, nstart = 20)

# Visualize results
library(ggplot2)
data <- data.frame(x, cluster = factor(km$cluster))
ggplot(data, aes(x, y, color = cluster)) +
  geom_point(size = 3) +
  geom_point(data = as.data.frame(km$centers), aes(x, y), color = "black", size = 5, shape = 8) +
  labs(title = "K-Means Clustering",
       subtitle = "Centroids shown as black stars") +
  theme_minimal()
```

#### Properties and Limitations of $k$-Means

Although $k$$-means is one of the most popular clustering algorithms, its effectiveness depends on the structure and scale of the data. Understanding its limitations helps decide when to use it — and when to choose a more robust alternative like $k$-Medoids.

##### **Data Type and Cluster Shape**

$k$-Means works best when:

- All variables are numeric
-  and measured on a comparable scale. 
   
- The clusters are *spherical* (or roughly circular in 2D).  
  
- Each cluster has *similar variance* and *density*.

When clusters are elongated, overlapping, or non-spherical, $k$-Means may assign points incorrectly because it relies on *Euclidean distance*.

##### **Sensitivity to Scale**

Since Euclidean distance depends directly on the magnitude of the variables, features with larger numerical ranges will dominate the distance calculation.

For example, in a dataset with *income* (in euros) and *age* (in years), income will completely overshadow age unless both are standardized.

Always *standardize or normalize* variables before running $k$-Means:
```{r}
#| label: kmeans-scale
#| eval: false
#| message: false
#| warning: false

# Standardization example
x_scaled <- scale(x)
kmeans(x_scaled, centers = 2)
```


#### Measures of Cluster Quality

To evaluate and compare clustering results, we rely on *internal validation measures* — those that assess how compact and well-separated the clusters are, using only the data itself.

The most common measures include:

##### **Within-Cluster Sum of Squares (WSS)**

The *Within-Cluster Sum of Squares* measures how tightly the data points in a cluster are grouped around their centroid.
$$
\text{WSS} = \sum_{j=1}^{k} \sum_{x_i \in C_j} \|x_i - \mu_j\|^2
$$

- $C_j$: set of points in cluster $j$  
- $\mu_j$: centroid of cluster $j$  
- $\|x_i - \mu_j\|^2$: squared Euclidean distance between a point and its centroid  

A smaller WSS means *more compact clusters*.

**Used in:**  
The **Elbow Method**, where we plot WSS vs. $k$ and look for the “*elbow*” — the point beyond which adding more clusters yields diminishing improvement.

##### **Between-Cluster Sum of Squares (BSS)**

The *Between-Cluster Sum of Squares* measures how far apart the cluster centroids are from the overall mean of the dataset.
$$
\text{BSS} = \sum_{j=1}^{k} n_j \| \mu_j - \mu \|^2
$$
where:
- $n_j$: number of points in cluster $j$  
- $\mu_j$: centroid of cluster $j$ 
- $\mu$: global mean of all data points  

A larger BSS means *better separation* between clusters.

##### **Total Sum of Squares (TSS)**

The *Total Sum of Squares* represents the overall variation in the dataset:
$$
\text{TSS} = \sum_{i=1}^{n} \|x_i - \mu\|^2
$$

This relationship always holds:
$$
\text{TSS} = \text{BSS} + \text{WSS}
$$

This decomposition allows us to express clustering performance as a proportion of explained variance:
$$
\text{Ratio} = \frac{\text{BSS}}{\text{TSS}}
$$

A higher ratio means that the clusters explain a greater proportion of the data’s variance.

##### **Silhouette Coefficient**

The *Silhouette Coefficient* combines *cohesion* (how close points are within a cluster) and *separation* (how far points are from other clusters):
$$
s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}
$$
where:  
- $a(i)$: average distance from point $i$ to all other points in its cluster  
- $b(i)$: smallest average distance from point $i$ to points in another cluster  

Values range from $–1$ to $1$:

- **Close to $1$:** well-clustered  
- **Around $0$:** point near cluster boundary  
- **Negative:** likely misclassified

:::{.callout-note}
**Summary**

| Measure | Description | Ideal Value |
|:---------|:-------------|:-------------|
| **WSS** | Compactness within clusters | Low |
| **BSS** | Separation between clusters | High |
| **BSS / TSS** | Proportion of explained variance | High |
| **Silhouette** | Cohesion and separation combined | Close to 1 |
:::

:::{.callout-important}
Good clustering minimizes **WSS** and maximizes **BSS**, producing compact, well-separated groups.  
The **Elbow Method** helps find a balance between simplicity (few clusters) and accuracy (tight, distinct groups).
:::

#### Analysis of `iris` Dataset

The classic `iris` dataset contains $150$ flower observations from three species (*setosa*, *versicolor*, *virginica*).  Each observation has four numeric features: `Sepal.Length`, `Sepal.Width`, `Petal.Length`, and `Petal.Width`

```{webr-r}
#| label: iris-prepare
#| autorun: true
#| message: false
#| warning: false

# Load dataset
data("iris")

# Select only numeric features for clustering
iris_data <- iris[, 1:4]
```

Suppose that we do not know that there are three groups and the goal is to determine the optimal number of clusters using *Elbow method*

```{webr-r}
#| label: iris-elbow
#| autorun: true
#| message: false
#| warning: false
#| out-width: "90%"

library(ggplot2)

# Compute WSS for k = 1:10

wss <- sapply(1:10, function(k) {
kmeans(iris_data, centers = k, nstart = 20)$tot.withinss
})

# Create Elbow plot

elbow_df <- data.frame(k = 1:10, WSS = wss)

ggplot(elbow_df, aes(k, WSS)) +
geom_line(color = "steelblue", linewidth = 1.2) +
geom_point(size = 3, color = "tomato") +
scale_x_continuous(breaks = 1:10) +
labs(title = "Elbow Method — Iris Dataset",
x = "Number of clusters (k)",
y = "Total Within-Cluster Sum of Squares (WSS)") +
theme_minimal()
```

We observe that the *elbow* appears around $k = 3$, matching the three known species.

:::{.callout-tip}
In the $k$-means function
```{r}
#| eval: false 
kmeans(iris_data, centers = 3, nstart = 20)
```
the argument `nstart` controls how many random initializations the algorithm tries before selecting the best solution.

Actually, $k$-means starts by randomly choosing $k$ points as *initial centroids*. Depending on these starting points, the algorithm can converge to *different local minima* — meaning the results may vary across runs. So, `nstart` means number of random initial sets of centroids to try and then, R will run the algorithm that many times and keep the best result (the one with the lowest total within-cluster sum of squares). 
:::

Now, we apply $k$-means algorithm with $k = 3$.

```{webr-r}
#| label: iris-kmeans
#| autorun: true
#| message: false
#| warning: false

set.seed(42)
km_iris <- kmeans(iris_data, centers = 3, nstart = 20)

# Add cluster assignments to the dataset
iris_clustered <- cbind(iris, cluster = factor(km_iris$cluster))

# Show first few results
head(iris_clustered)
```

We compute cluster quality measures.

```{webr-r}
#| label: iris-measures
#| autorun: true
#| message: false
#| warning: false

# Total, within, and between-cluster sums of squares

TSS <- sum(scale(iris_data, scale = FALSE)^2)
WSS <- km_iris$tot.withinss
BSS <- TSS - WSS
ratio <- BSS / TSS

cat("TSS:", round(TSS, 2), "\n")
cat("WSS:", round(WSS, 2), "\n")
cat("BSS:", round(BSS, 2), "\n")
cat("Explained variance (BSS/TSS):", round(ratio, 3), "\n")
```

:::{.callout-note}

**What are “discriminative features”?**

*Discriminative features* are those that *best separate groups or clusters* in your data. They are the variables that most strongly differentiate one cluster (or class) from another.

There are two most famous approach to find discriminative features:

- Exploratory approach
  
- Statistical approach

**Exploratory approach**
```{webr-r}
#| label: eda
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

library(GGally)
ggpairs(iris, aes(color = Species))
```
In this plot, you will notice that:

- Setosa is perfectly separated in the `Petal.Length`–`Petal.Width` plane.

- Versicolor and Virginica overlap slightly but still show visible separation.

That is why we often choose `Petal.Length` and `Petal.Width` for visualization — they are the most discriminative pair of features.

**Statistical approach**
We can quantify which features are most discriminative by computing, for each variable:
- The between-group variance (BSS)
- The within-group variance (WSS)
and comparing their ratio.

```{webr-r}
#| label: discriminative-measures
#| autorun: true
#| message: false
#| warning: false

# Compute ratio of between- to within-group variance for each feature
library(dplyr)

iris_var <- iris %>%
  group_by(Species) %>%
  summarise(across(where(is.numeric), mean))

# Between-class variance
overall_means <- colMeans(iris[, 1:4])
between_var <- colSums((as.matrix(iris_var[, -1]) - overall_means)^2)

# Within-class variance
within_var <- sapply(iris[, 1:4], function(v) tapply(v, iris$Species, var))
within_var <- colMeans(within_var)

# Ratio
discriminative_ratio <- between_var / within_var
discriminative_ratio
```
The higher this ratio, the better that feature separates the species.

In this results, `Petal.Width` is clearly the most discriminative feature.

At first glance, it seems that `Sepal.Width` (124.96) should be more discriminative than `Petal.Length` (79.51) because its ratio is higher — but this needs context

**What the ratio measures?** The ratio 
BSS/WSS
BSS/WSS for each feature is:
$$
\text{Ratio} = \frac{Between-Class Variance}{Within-Class Variance}
$$
This tells you, for that one variable alone, how distinct the species means are compared to the variability within each species.

So, mathematically, if `Sepal.Width` has a higher ratio, it means that species differ more in `Sepal.Width` (on average) than they do within themselves. **But "discriminative" depends on combined separability**

The tricky part is that `Sepal.Width` alone does not visually separate the species well, even if its numeric ratio looks high.
Let us check the data visually:

```{webr-r}
#| label: sepal-length-plots
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

library(ggplot2)

# Compare Sepal.Width vs Petal.Length
ggplot(iris, aes(Sepal.Width, fill = Species)) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution of Sepal.Width by Species") +
  theme_minimal()

ggplot(iris, aes(Petal.Length, fill = Species)) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution of Petal.Length by Species") +
  theme_minimal()
```
You see that:
- For `Sepal.Width`, there is a lot of overlap between Versicolor and Virginica.
The means differ, but not enough to separate groups clearly.

- For `Petal.Length`, Setosa is completely separated, and Versicolor/Virginica overlap less.

So even though `Sepal.Width`’s ratio is high, its overlap pattern means it is less useful for distinguishing all three species.

:::{.callout-tip}
The *BSS/WSS ratio* is a helpful indicator, but not the whole story.
Always complement numerical measures with *visual inspection*.
:::
:::

Now, based on the most discriminative features, `Petal.Length` and `Petal.Width`, we visualize the clusters.

```{webr-r}
#| label: iris-plots
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

ggplot(iris_clustered, aes(Petal.Length, Petal.Width, color = cluster)) +
geom_point(size = 3) +
labs(title = "K-Means Clustering of Iris Data (k = 3)",
subtitle = "Color shows cluster assignment") +
theme_minimal()
```

Even, we can compare with true species 
```{webr-r}
#| label: iris-comparison
#| autorun: true
#| message: false
#| warning: false

table(Cluster = iris_clustered$cluster, Species = iris_clustered$Species)
```

As you see, *Setosa* is perfectly separated, while *versicolor* and *virginica* overlap.


We can also evaluat  clustering with the Silhouette coefficient

```{webr-r}
#| label: silhouette-iris
#| autorun: true
#| message: false
#| warning: false

library(cluster)
set.seed(42)

# K-means result for k = 3 (from previous section)
km_iris <- kmeans(iris[, 1:4], centers = 3, nstart = 20)

# Compute silhouette width
sil_iris <- silhouette(km_iris$cluster, dist(iris[, 1:4]))

# Display summary statistics
summary(sil_iris)
```

```{webr-r}
#| label: silhouette-plot
#| echo: true
#| fig-cap: "Silhouette plot for K-Means clustering (k = 3) on the Iris dataset"
#| out-width: "80%"
plot(sil_iris, border = NA, main = "Silhouette Plot — Iris Dataset (k = 3)")
```
The *average silhouette width *(displayed in the summary) gives an overall quality score.

- Values > $0.5$ indicate clear, well-separated clusters.

- Values around $0.25$–$0.5$ indicate overlapping clusters.

- Values < $0.25$ indicate poor structure or misclassification.

In the `iris` dataset, the average silhouette is typically around $0.55$–$0.60$,
confirming that 
$k=3$ is a reasonable choice.

- *Setosa* usually has near-perfect silhouette values, while *Versicolor* and *Virginica* show some overlap — as expected.

We can compute the average silhouette width for different numbers of clusters.

```{webr-r}
#| label: silhouette-elbow
#| autorun: true
#| message: false
#| warning: false
#| out-width: "80%"

sil_widths <- sapply(2:10, function(k) {
km <- kmeans(iris[, 1:4], centers = k, nstart = 20)
ss <- silhouette(km$cluster, dist(iris[, 1:4]))
mean(ss[, "sil_width"])
})

sil_df <- data.frame(k = 2:10, Silhouette = sil_widths)

library(ggplot2)
ggplot(sil_df, aes(k, Silhouette)) +
geom_line(color = "steelblue", linewidth = 1.2) +
geom_point(size = 3, color = "tomato") +
scale_x_continuous(breaks = 2:10) +
labs(title = "Average Silhouette Width by Number of Clusters",
x = "Number of clusters (k)",
y = "Average Silhouette Width") +
theme_minimal()
```

As you see, the highest silhouette score occurs around $k = 3$, matching the known number of species. $k = 2$ gives larger compact clusters but merges two real species. While $k > 3$ begins splitting natural groups unnecessarily (overfitting). 
You see that the *Silhouette Coefficient* confirms the Elbow method’s result

::::: panel-tabset
## Exercise 1

Use the built-in dataset `USArrests`, which contains statistics on violent crime rates in the 1970s for 50 U.S. states.
```{r}
#| label: ex1
#| eval: false

data("USArrests")
```
Standardize all numeric features before applying $k$-Means using this R code:
```{r}
#| label: ex1-standard 
#| eval: false

us_data <- scale(USArrests)
```
**Elbow Method**

- Compute the total within-cluster sum of squares (WSS) for $k=1$ to $10$.
  
- Plot the results.
  
- Identify the "elbow" point.

**Silhouette Method**

- Compute the average silhouette width for $k=2$ to $10$.
  
- Plot the results using the same approach as in the Iris example.
  
- Identify the $k$ that maximizes the silhouette width.

## Exercise  2
This exercise demonstrates how $k$-Means clustering can be used beyond data analysis — for image compression.

Each color in an image can be treated as a data point in RGB space, and clustering reduces the number of colors while preserving the main structure.

**Load and visualize an image.**

```{r}
#| eval: false 
library(jpeg)
library(ggplot2)
library(dplyr)


# Read example image from R base (or provide your own file path)
url <- "https://upload.wikimedia.org/wikipedia/commons/3/3f/JPEG_example_flower.jpg"
temp <- tempfile(fileext = ".jpg")
download.file(url, temp, mode = "wb")
img <- readJPEG(temp)


# Convert to data frame
img_df <- data.frame(
R = as.vector(img[,,1]),
G = as.vector(img[,,2]),
B = as.vector(img[,,3])
)
```

**Determine the optimal number of clusters.**

Apply $k$-means clustering to the RGB values of the image (`img_df`) for $k = 2$ to $10$, using `nstart = 10`.  Then, use both the **Elbow Method** and the **Silhouette Coefficient** to decide which $k$ gives the best balance between cluster compactness and separation. Do both methods suggest the same number of clusters? 


When you decide about the optimal $k$, with the following code, you can compare the original and compressed image. 

```{r}
#| eval: false 

set.seed(123)


k <- 'put the optimal value you found'
km_colors <- kmeans(img_df, centers = k, nstart = 10)


# Replace each pixel with its cluster centroid color
compressed_img <- km_colors$centers[km_colors$cluster, ]
compressed_img <- array(compressed_img, dim = dim(img))

par(mfrow = c(1, 2))
plot(0, type = "n", xlim = c(0, 1), ylim = c(0, 1),
xlab = "", ylab = "", axes = FALSE, main = "Original Image")
rasterImage(img, 0, 0, 1, 1)


plot(0, type = "n", xlim = c(0, 1), ylim = c(0, 1),
xlab = "", ylab = "", axes = FALSE, main = paste("Compressed Image (k =", k, ")"))
rasterImage(compressed_img, 0, 0, 1, 1)
```

:::::