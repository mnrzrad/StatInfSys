[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics and Information Systems",
    "section": "",
    "text": "1 Note\nThis interactive book is designed as a companion to the course Systems Information and Statistics, taught at the NOVA School of Science and Technology (NOVA FCT) in Lisbon, Portugal.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Note</span>"
    ]
  },
  {
    "objectID": "index.html#emails",
    "href": "index.html#emails",
    "title": "Statistics and Information Systems",
    "section": "1.1 Emails",
    "text": "1.1 Emails\nMina Norouzirad\nMiguel Fonseca\nGracinda Guerreiro",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Note</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html",
    "href": "chapters/chapter4_EDA.html",
    "title": "2  Exploratory data analysis (EDA)",
    "section": "",
    "text": "2.1 Plan the Study\nStatProcess\n\ncluster_problem\n\n\ncluster_plan\n\n\ncluster_data\n\n\ncluster_analysis\n\n\ncluster_conclusion\n\n\n\nP1\n\nP\n\n\n\nBOX1\n\n\n\n\n\n\n\nProblem\n• Write down study objectives\n• Identify target/sample population\n• Define the variates\n\n\n\nP1-&gt;BOX1\n\n\n\n\nP2\n\nP\n\n\n\nP1-&gt;P2\n\n\n\n\n\nBOX2\n\n\n\n\n\n\n\nPlan\n• Plan data collection methods\n• Calculate sample size\n• Consider analysis options\n\n\n\nP2-&gt;BOX2\n\n\n\n\nD\n\nD\n\n\n\nP2-&gt;D\n\n\n\n\n\nBOX3\n\n\n\n\n\n\n\nData\n• Collect data per plan\n• Note any deviations\n\n\n\nD-&gt;BOX3\n\n\n\n\nA\n\nA\n\n\n\nD-&gt;A\n\n\n\n\n\nBOX4\n\n\n\n\n\n\n\nAnalysis\n• Analyze data per plan\n\n\n\nA-&gt;BOX4\n\n\n\n\nC\n\nC\n\n\n\nA-&gt;C\n\n\n\n\n\nBOX5\n\n\n\n\n\n\n\nConclusion\n• Draw conclusion in context\n\n\n\nC-&gt;BOX5",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html#after-collecting-the-data",
    "href": "chapters/chapter4_EDA.html#after-collecting-the-data",
    "title": "2  Exploratory data analysis (EDA)",
    "section": "2.2 After collecting the data",
    "text": "2.2 After collecting the data\n\n\n\n\n\n\n\nAfterCollecting\n\n\nS1\n\nReview the hypotheses\n•\nReview the research questions\n•\nConsider sub-questions\n\n\n\nS2\n\nProcess the raw data\n•\nSelect a suitable statistics software\n•\nConvert the data into an acceptable format\n\n\n\nS1-&gt;S2\n\n\n\n\n\nS3\n\nExplore the data\n•\nDescriptive summaries\n•\nData visualization\n\n\n\nS2-&gt;S3\n\n\n\n\n\nS4\n\nAnalyze the data\n•\nInferential analysis\n•\nPrediction\n\n\n\nS3-&gt;S4\n\n\n\n\n\nS5\n\nReport the results\n•\nInterpret the results in the context of the study\n\n\n\nS4-&gt;S5",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html#what-is-eda",
    "href": "chapters/chapter4_EDA.html#what-is-eda",
    "title": "2  Exploratory data analysis (EDA)",
    "section": "2.3 What is EDA?",
    "text": "2.3 What is EDA?\nExploratory data analysis (EDA) was first formally introduced by Tukey (1977) in his influential book, Exploratory Data Analysis.\n\n\n\n\n\n\n\n\n\n\nSince then, its importance has grown significantly, especially in recent years, for several key reasons (Midway 2022):\ni) data is being produced faster and in larger volumes than ever before,\nii) modern computing and software tools make it easier to explore, clean, and visualize data in meaningful ways,\niii) contemporary statistical models are often complex and assumption-dependent, requiring us to thoroughly understand the data before applying formal techniques.\nThe EDA may not be fully described in concrete terms, but most data analysts and statisticians know it when they see it.\n\n\n\n\n\n\nImportant\n\n\n\nThe EDA is important because it helps researchers make thoughtful decisions about which ideas are worth exploring. Sometimes, the data clearly show that a particular question does not have enough support to be studied further—at least not with the current evidence.\n\n\nThe main goals of EDA are:\n\nTo suggest hypotheses about what might be causing the patterns or relationships observed in the data,\nTo guide the choice of appropriate statistical tools or models by helping you understand the structure of the data,\nTo assess key assumptions that must be checked before applying formal statistical analysis (e.g., linearity, normality, independence),\nTo provide a basis for further data collection, by highlighting gaps, inconsistencies, or areas where more information is needed.\n\n\n\n\n\n\n\nNote\n\n\n\nEDA is not typically the final stage of analysis. Rather, it serves as a transitional step between raw data and formal modeling. The insights gained through EDA guide decisions about which models to use, which variables to consider, and which data issues to address.\n\n\n\n\n\n\n\n\nImportantEDA Checklist\n\n\n\nRoger D. Peng in his book (Peng 2012) provide this checklist for conducting EDA.\n\nStart with a clear question: Before you begin EDA, take time to define exactly what you want to find out. A clear question or hypothesis gives your analysis purpose and helps you stay focused along the way.\nLoad your data carefully: Make sure your dataset is fully and correctly loaded into your analysis tool (e.g., R or Python). This first step is essential—it sets the stage for everything you will do next.\nTake a first look at the data: Check that the file type, structure, and layout are what you expected. Make sure everything is organized in a way that works for your analysis.\nUse str() to Peek Inside the Dataset: In R, it will get a quick summary: number of observations (rows), number and names of variables (columns), variable types (e.g., numeric, character, factor), and a preview of the data values\nLook at the Beginning and End of Your Data: Use functions such as head() and tail() to view the first and last few rows. This visual check can help to detect issues such as incorrect headers, blank rows, or unusual formatting.\nCheck the Number of Rows (“\\(n\\)“): Make sure to verify how many observations (rows) are in your dataset. Compare this to what you expected from the original. source. If the number is too high or too low, there may be missing values, duplicate entries, or extra rows (e.g. duplicates or blank lines).\nValidate with an External Source: When possible, compare part of your dataset with a trusted external source, such as official statistics or published reports. This helps confirm the accuracy and reliability of your data.\nTry the Simple Solution First: Start by basic methods–such as summaries, tables, or visualizations– to explore and answer your question. Simple tools can often reveal key patterns or issues. Use more complex techniques only of necessary.\nChallenge Your Findings: Once you find a result, pause and ask yourself: Does this make sense? Check your assumptions and consider possible errors or missing information. Being critical helps make your results stronger and more trustworthy.\nDecide What to Do Next: Use the insights from your EDA to guide your next steps. You may decide to collect more data, use new methods, or refine your question. Sometimes , your initial findings are already enough to answer your main question.\n\nIn Chapter 4 of Midway (2022), interested reader can find some comments for each step.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nEDA is the single most important task to conduct at the beginning of every data science project.\n\n\n\n\n\n\n\n\nTip\n\n\n\nEDA is like exploring a new place – you do not know what you will find until you start looking.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html#enter-the-world-of-eda",
    "href": "chapters/chapter4_EDA.html#enter-the-world-of-eda",
    "title": "2  Exploratory data analysis (EDA)",
    "section": "2.4 Enter the world of EDA",
    "text": "2.4 Enter the world of EDA\n\n\n\n\n\n\n\nEDA\n\n\nEDA\n\nExploratory Data Analysis\n\n\n\nView\n\nView data\n\n\n\nEDA-&gt;View\n\n\n\n\n\nSummary\n\nSummary\nstatistics\n\n\n\nEDA-&gt;Summary\n\n\n\n\n\nGraphs\n\nBasic\nGraphs\n\n\n\nEDA-&gt;Graphs\n\n\n\n\n\nTests\n\nBasic\nTests\n\n\n\nEDA-&gt;Tests\n\n\n\n\n\nV1\n\nObservation number\nVariable number\nVariable type\nVariable category\nStructure\n\n\n\nView-&gt;V1\n\n\n\n\n\nS1\n\nMean\nMedian\nMode\nRange\nVariance\nStandard deviation\nOutliers\nMissing data\n\n\n\nSummary-&gt;S1\n\n\n\n\n\nG1\n\nHistogram\nBar plot\nBoxplot\nScatter-plot\nQQ-plot\n\n\n\nGraphs-&gt;G1\n\n\n\n\n\nT1\n\nCheck assumptions\nT-tests\nCorrelations\nANOVA\nLinear model\n\n\n\nTests-&gt;T1\n\n\n\n\n\n\n\n\n\n\nNow it is time to start working with data directly. To do that, we must first understand how data are structured and what types of variables we are dealing with.\nMost datasets are organized in a rectangular format — like a spreadsheet — where each row represents one observation (e.g. a person, object, or experiment), and each column represents a variable (e.g. name, age, group, result).\n\n\n\nAdapted from Slides of ‘EDA Module I: A Bird’s Eye View’ by Dr. Mark Williamson\n\n\n\nAs an example, let us explore the mpg dataset with a few simple commands in R:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n2.4.1 Types of Variables\nVariables represent the information we collect in data analysis. They can be broadly divided into categorical and numeric types.\n\n2.4.1.1 Categorical Variables\nCategorical variables describe qualities or characteristics. They answer questions like “what type?” or “which group?”.\n\nAlso called qualitative variables, and they produce qualitative data.\nThey do not have numeric meaning (even if represented by numbers)\n\nThey fall into two main groups:\n\nNominal Variables\n\nCategories have no logical order.\nThey are simply labels or names.\n\n\nFor example, Gender (male, female), Blood group (A, B, AB, O), City (Lisbon, Porto, Faro, … ), color (red, blue, green), the types of drinks at Starbucks, a person’s eye color.\n\nOrdinal Variables\n\nCategories can be ordered or ranked.\nHowever, the distance between categories is not exact or consistent\n\n\nFor example, Satisfaction (low, medium, high), education (primary, secondary, university), rank (1st, 2nd, 3rd), Academic grades (A, B, C)\n\n\n\n\n\n\nNote\n\n\n\nAvoid coding categories with numbers (e.g., Male = 1, Female = 2), as this may wrongly suggest order or allow meaningless calculations.\nTip: Use clear text labels like \"Male\" and \"Female\".\n\n\nIn R, categorical variables are usually stored as factors. But sometimes they might appear as character or even numeric, especially if the dataset is not clean or comes from an external file.\nYou can check whether a variable is a factor using:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo see the possible categories (called levels) of a factor, use:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIf a variable is not a factor but should be treated as one, you can convert it like this:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nTip\n\n\n\nSometimes, datasets are not ready for analysis and categorical variables may be incorrectly coded as numeric or character. Always check and convert them to factor if needed before analysis or plotting.\n\n\nYou can use the following commands to help:\nis.factor(mpg$class)\nlevels(mpg$class)\nmpg$class &lt;- factor(mpg$class)\n\nExerciseHintSolutions\n\n\nThe mpg dataset contains information about different car models, including manufacturer, engine size, fuel efficiency, and more.\nYour task is to:\n\nList all the categorical variables in the dataset.\nUse is.factor() to check which variables are already treated as factors (categorical).\nIf a variable is not a factor but should be (e.g., class or trans), convert it using factor().\nUse levels() to inspect the categories.\nFinally, fill in the table to classify each categorical variable as nominal or ordinal.\n\nFill Table to Complete\n\n\n\n\n\n\n\n\n\nVariable\nIs it categorical? (Yes/No)\nScale (Nominal / Ordinal)\nJustify your answer\n\n\n\n\nmanufacturer\n\n\n\n\n\nmodel\n\n\n\n\n\ntrans\n\n\n\n\n\ndrv\n\n\n\n\n\nfl\n\n\n\n\n\nclass\n\n\n\n\n\nyear\n\n\n\n\n\ndispl\n\n\n\n\n\n\n\n\n\n\nUse `str(mpg)` to inspect all variable types.\nUse `is.factor()` to check factor status.\nUse `levels()` to explore categories.\nVariables like `class` and `fl` are good candidates for factors.\nNominal= unordered categories; Ordinal= categories with a meaningful order.\n\n\n\n\nDo it yourself :)\n\n\n\n\n\n\n2.4.1.2 Numerical variables\nNumeric variables describe quantities that can be measured. They answer questions like “how many?” or “how much?”.\n\nAlso called quantitative variables, and they produce quantitative data.\nThey are numbers we can measure, and we can meaningfully perform mathematical operations on them.\nUsually, they have a measurement unit.\n\nNumerical (quantitative) variables can be classified in two complementary ways:\n\nContinuous vs. Discrete – How values occur\nRatio vs. Interval – How the scale is defined\n\nContinuous vs. Discrete\n\nContinuous Variables\n\nCan take any value on a number line, including decimals.\nRepresent real-world quantities.\nThey may be restricted to positivevalues (e.g., mass) or include negatives (e.g., temperature changes).\n\nSome examples are: mass, age, temperature, time.\nDiscrete Variables\n\nTake only whole number values (no decimals).\nBased on counting.\nCannot take values between integers (e.g., 2.5 students does not make sense).\n\n\nSome examples are: number of students, number of offspring, number of infected individuals\nRatio vs. Interval\nNumerical variables can also be distinguished by the scale of measurement, which affects how we interpret differences, proportions, and ratios.\n\nRatio Variables\n\nHave a true zero point (zero means “none”).\nAllows all mathematical operations, including ratios and proportions.\nInterpretation: “Tree A is twice as tall as Tree B.”\n\n\nSome examples are: Height, Weight, Age, Income, mass.\n\nInterval Variables\n\nDo not have a true zero (zero is arbitrary).\nAllows meaningful differences, but not ratios.\nInterpretation: “The difference between 20 °C and 10 °C is 10 degrees,” but not “20 °C is twice as hot as 10 °C.”\n\n\nSome examples are Temperature (in Celsius of Fahrenheit), IQ scores, Calendar dates (e.g. year 1000 vs. 2000)\n\n\n\n\n\n\nNote\n\n\n\nSame Quantity, Different Scales\nThe distinction between ratio and interval depends on how the variable is measured, not on what is being measured.\nFor example,\n\nTemperature in Celsius → interval scale (arbitrary zero).\nTemperature in Kelvin → ratio scale (absolute zero).\n\nSo, the same phenomenon (temperature) can be an interval variable (in °C), or a ratio variable (in K).\n\n\n\n\n\n\n\n\nTip\n\n\n\nVariable Types Are Not Always Fixed\nYou cannot determine a variable’s type just from its name — it depends on how the data is recorded.\nExample: Age\n\nIf recorded as an exact value (e.g., 25, 35.5, 80), it is a numerical variable.\nIf recorded in categories (e.g., &lt;20, 21–25, 80+), it is a categorical variable.\n\n\n\nIn R, numeric variables include both:\n\nIntegers (whole numbers, like 1, 2, 3), and\nDoubles or floats (decimal values, like 3.14, 5.0)\n\nYou can check a variable’s numeric type using:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn R, most numeric data are stored as double even if they look like integers.\nYou can convert to numeric explicitly if needed:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExerciseHintSolutions\n\n\nThe mpg dataset includes several numeric variables related to fuel economy and engine size.\nYour task is to:\n\nIdentify the numeric variables in the dataset using is.numeric().\nCheck if the numeric variable is an integer (is.integer()) or a float (is.double()).\nComplete the table and classify each variable as:\n\nDiscrete (countable values)\nContinuous (can take any value within a range)\n\nComment on any variables that could be treated as categorical instead, depending on context.\n\n\n\n\n\nUse str(mpg) and summary(mpg) to explore data.\nUse is.numeric() to detect numeric variables.\nUse is.integer() and is.double() to check type.\nDiscrete = exact counts; Continuous = measured values\n\n\n\n\nDo it yourself :)\n\n\n\n\nSummary of Variable Types\n\n\n\nSource: https://datatab.net/tutorial/level-of-measurement\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Type\nScale\nOrdered?\nCan Measure Distance?\nCan Divide?\nExamples\n\n\n\n\nNominal\nCategorical\n❌ No\n❌ No\n❌ No\nGender, City, Color\n\n\nOrdinal\nCategorical\n✅ Yes\n❌ No (not exact)\n❌ No\nRank, Education, Likert\n\n\nInterval\nNumeric\n✅ Yes\n✅ Yes\n❌ No\nTemperature, Year\n\n\nRatio\nNumeric\n✅ Yes\n✅ Yes\n✅ Yes\nAge, Weight, Income\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are not sure about a variable, ask:\n\nDoes the variable have a natural order?\nCan we count it or measure it?\nDoes it have a true zero?\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKnowing the types of data matters because:\n\nDifferent statistical methods apply to different variable types.\nSome graphs and visualizations only make sense for some types of variables.\n\n\n\n\n\n\n2.4.2 Types of EDA\nEDA helps us answer important questions like:\n\nWhat are the most common values of a variable?\nHow much do observations differ from each other?\nIs one variable related to another?\n\nTo answer these questions, EDA is generally cross-classified into two ways:\n\n\n\n\n\n\n\n\n\nNon-graphical (numbers/tables)\nGraphical (plots)\n\n\n\n\nUnivariate (one)\nUnivariate × Non-graphical\nUnivariate × Graphical\n\n\nMultivariate (≥2*)\nMultivariate × Non-graphical\nMultivariate × Graphical\n\n\n\n* Often bivariate.\nIn EDA, non-graphical methods are simply the numbers—the collection of descriptive statistics that summarize data (center, spread, shape, position, and association), while graphical methods are the visual counterparts that show those same properties (e.g., histograms/boxplots for distribution, scatterplots for association), helping us verify, compare, and spot patterns or anomalies that the numbers alone might hide.\nUnivariate methods examine a single variable at a time, whereas multivariate methods consider two or more variables to explore relationships—most often bivariate, though sometimes three or more—and it is generally best practice to first perform univariate EDA on each variable before moving on to multivariate analysis.\nBeyond the four cells of the cross-classification, EDA choices also depend on each variable’s role (outcome vs explanatory) and type (categorical vs quantitative)—and there is an element of art that grows with practice.\n\n\n\n\n\n\n2.4.2.1 Descriptive Statistics\n\n\n\n\n\n\nNote\n\n\n\nWhen we measure the same characteristic (e.g., age, gender, task speed, response to a stimulus) on all subjects in a sample, the resulting values form the sample distribution of that variable. This sample distribution is our imperfect window onto the population distribution.\nDescriptive statistics aims to describe the sample distribution—its location, spread, shape, and unusual observations—and to make tentative statements about which population distributions could plausibly have generated it.\n\n\nDescriptive statistics give numerical summaries of a dataset.\nBefore diving into categorical and numerical summaries, first check for missing values. Missingness is part of the data story and can be informative: count missing values and look for patterns (are they concentrated in certain variables or groups?). In EDA, always report the number (and share) of missing values per variable. We will return to strategies for handling missingness later, since it merits more attention and advanced methods.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nTipR Tip: any(), all(), and anyNA()\n\n\n\n\nany(x) returns TRUE if at least one value in x is TRUE.\nall(x) returns TRUE only if every value in x is TRUE.\nanyNA(x) is a shortcut to check if any value is NA (missing) in a vector or data frame.\n\nExamples\nx &lt;- c(1, 2, 3, NA)\n\nany(is.na(x))     # TRUE – there is at least one NA\nall(is.na(x))     # FALSE – not all are NA\nanyNA(x)          # TRUE – a fast way to check for NA, \nUse any() when you care if a problem exists, and all() when you want to know if everything passes a check.\n\n\n\n\n\n\n\n\nNote\n\n\n\nsummaries like mean/SD ignore NA by default only if you set na.rm = TRUE; silently dropping cases can bias results.\n\n\nUnivariate Case\n\nQualitative Data\n\nFor a categorical variable, the key features are the set of categories (the possible values) and how often each occurs (frequency or relative frequency).\nThe primary tool is a frequency table: list each category with its count and proportion/percent. Always include the total \\(N\\) (and any missing) to verify that every recruited subject is represented. As a quick quality check, proportions should sum to \\(1.00\\) (or \\(100\\%\\)). Once you are comfortable, reporting either proportion or percent is sufficient—they are equivalent.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQuantitative Data\n\nA quantitative variable’s population distribution is described by its center (typical value), spread (how much values vary), modality (how many peaks), shape (including how long or heavy the tails are), and outliers (unusual values). The data we observe are just one random sample among many we could have drawn, so the sample’s features matter only because they help us learn about the population they come from.\nWhat we observe for a given variable is the sample distribution—how the values happen to fall in this particular sample. If we drew another sample from the same population, the distribution (and the numbers we compute) would likely be different because of random sampling (and, in experiments, possibly different treatment assignments or conditions). From each sample we can calculate sample statistics—mean, variance/standard deviation, skewness, kurtosis—but these vary from sample to sample, so they are not exact truths; they are noisy estimates that provide uncertain information about the population distribution and its parameters.\nIf a quantitative variable has only a few distinct values, a simple frequency table (as with categorical data) can be a useful tool. More often, though, we rely on numerical summaries—sample statistics like the mean, median, variance, standard deviation, skewness, and kurtosis. These are best understood as estimates of their corresponding population parameters. We aim to learn what we can about those parameters from a random sample, knowing we can’t know them exactly, because the parameters are “secrets of nature.”\n\n\n\n\n\n\nNote\n\n\n\nQuantitative vs categorical summaries\nFor quantitative variables, it is meaningful to report a variable’s central tendency (mean/median), spread (SD/IQR), skewness, and kurtosis. For categorical variables, these summaries do not make sense—use counts, proportions/percentages, number of levels, mode, and missingness instead.\n\n\nThey describe the key features of a variable:\n\nCenter (e.g., mean or median) — a typical value\nSpread (e.g., standard deviation or interquartile range) — how much values vary\nShape (e.g., skewness) — symmetry or lean and tail behavior\nExtremes (e.g., minimum, maximum, or outliers) — unusual low/high values\n\nCentral Tendency\nA measure of central tendency tells us what a “typical” or “middle” value looks like in the data.\nThe most common measures are the (arithmetic) mean, the median, and sometimes the mode.\n\n\n\n\n\n\nCaution\n\n\n\nIn special situations you may also see geometric, harmonic, trimmed, or Winsorized means used as measures of centrality.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMost authors use average to mean the arithmetic mean, though some use average more broadly to include these other means.\n\n\nIf we have \\(n\\) values \\(x_1\\), \\(x_2\\), \\(\\ldots\\), \\(x_n\\), the sample (arithmetic) mean is\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\nIn words: add up all the values and divide by the number of values.\n\n\n\n\n\n\nTip\n\n\n\nA helpful picture is the fair-share idea—put everything into one pot and split it equally; the amount each person would get is the mean.\n\n\nFor most descriptive quantities, there is a population version and a sample version. In a fixed finite population—or in an idealized infinite population defined by a probability model—there is a single population mean, a fixed (often unknown) parameter. By contrast, the sample mean changes from one sample to another; it is a random variable. The distribution of the sample mean over all possible samples is its sampling distribution. This reflects the idea that, at least in principle, we could repeat the sampling many times and recompute the statistic each time, getting different values. Under suitable assumptions, probability theory lets us derive the exact (or approximate) form of this sampling distribution.\n\n\n\n\n\n\nWarning\n\n\n\nThe mean is sensitive to outliers\nBe careful: the mean can be pulled toward extreme values. For example, using the mean to describe income is often misleading because a few very high earners raise the average.\nWhat to do instead: for skewed data, report the median (and IQR) or show both mean and median; consider trimmed/Winsorized means; always add a quick plot (histogram/boxplot).\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe median is another measure of central tendency. To find it, first sort the \\(n\\) values from smallest to largest. If \\(n\\) is odd, the median is the middle value; if \\(n\\) is even, it is the average of the two middle values. When there are ties around the middle, statistical software applies standard rules so you still get a single number. (For some discrete variables there may not be a unique theoretical median, but software returns a conventional choice.)\nIn the 2004 U.S. Census Bureau Economic Survey, the median family income was $43,318, meaning half of families earned less and half earned more. The mean (average) was $60,828, the amount each family would have if total income were split equally. The gap between these two numbers is large because a few very high incomes pull the mean upward.\n\n\n\n\n\n\nTip\n\n\n\nRobustness of the median\nThe median is robust, a few very large or very small values usually do not change it. You can move the highest or lowest observations far away and, as long as the middle order does not change, the median stays the same.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe mode is the most frequent value. For quantitative data it is rarely a good “typical” summary because with continuous or rounded variables the result depends on binning/rounding, may be non-unique, and many samples have no repeated value. We mostly use mode as shape language—unimodal (one peak), bimodal (two), multimodal (many). In practice, it is conceptually useful but hard to estimate robustly for numeric variables, and is used more for categorical or discrete data.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nMean vs. median\nThe most common measure of central tendency is the mean. When outliers are a concern, the median may be preferred.\n\n\nSpread (Dispersion)\nSpread tells us how far values tend to lie from the center and how different they are from one another. A dataset with very similar values has low dispersion; one with values scattered widely has high dispersion. Three common measures:\n\nVariance — the average squared distance from the mean. Useful mathematically, but harder to interpret because it is in squared units.\nStandard deviation (SD) — the square root of the variance. Same units as the data; think of it as a typical distance from the mean. For bell-shaped data, most observations sit within a few SDs of the mean.\nInterquartile range (IQR) — the distance between the 25th and 75th percentiles; the middle 50% of the data. Robust to outliers, and usually paired with the median.\n\nOther quick checks: the range (max − min) is easy but very sensitive to extremes; MAD (median absolute deviation) is a robust alternative to SD.\nThe MAD can be defined as\n\\[\n\\mathrm{MAD} = \\mathop{\\mathrm{median}}_{1\\leq i \\leq n} |x_i - \\bar{x}|\n\\]\n\n\n\n\n\n\nTip\n\n\n\nWhich measure when?\nUse SD when the distribution is roughly symmetric and free of extreme outliers. Use IQR (often with the median) when the distribution is skewed or contains outliers.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nOutliers inflate variance/SD.\nA few extreme values can make the variance and SD look large even if most data are tightly clustered. Always check a plot and consider IQR/MAD for robustness.\n\n\nTo describe spread, start with each value’s deviation from the mean (value − mean). If you add up all these signed deviations, they cancel and sum to zero. To avoid that cancellation and to give extra weight to big departures, we square the deviations and then average them—that average of squared deviations is the variance.\nThe population variance is the mean of the squared deviations. For a sample, the variance is an estimate of it and will change from sample to sample. For \\(n\\) observations (\\(x_1\\), \\(\\ldots\\) , \\(x_n\\)), define each deviation as \\((x_i - \\bar{x})\\) (negative if below the mean, positive if above), and thus,\n\\[\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\n\\]As a sample statistic, (\\(s^2\\)) varies from sample to sample; we use it to estimate the single, fixed population variance (\\(\\sigma^2\\)). The \\((n-1)\\) in the denominator ensures unbiasedness: across many random samples, the average of \\(s^2\\) equals \\(\\sigma^2\\).\n\n\n\n\n\n\n\nNote\n\n\n\nWhy does variance matter?\nVariance is an important quantity in statistics. Many statistical tests use changes in variance to compare groups or detect effects.\n\n\nBecause we square deviations, the variance is always non-negative and its units are squared. That is why a temperature measured in degrees has variance in degrees², and an area measured in km² would have variance in km⁴. This can feel odd, but it is also why we often report the standard deviation (the square root of variance) when we want a spread measure back in the original units.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nCaution\n\n\n\nDoes var() compute the sample variance or population variance?\n\n\nThe standard deviation is the square root of the variance, so it is in the same units as the data, which makes it easier to interpret. We usually write the sample standard deviation as \\(s\\) (and the population SD as \\(\\sigma\\) ).\n\n\n\n\n\n\nNote\n\n\n\nWhy use standard deviation?\n\nIt is on the same scale as the variable\nIt gives us a sense of spread we can understand\nit is more commonly used in EDA than variance\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSD is sensitive (like the mean)\nThe standard deviation is not robust: it can be distorted by skewed distributions and outliers (extreme values).\nWhen this happens: prefer median + IQR or MAD.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo understand the interquantile range (IQR), first we define quartiles. Quartiles split the data into four equal parts: the first quartile (Q1) is the smallest value below which 25% of observations fall, the second quartile (Q2) is the median (50%), and the third quartile (Q3) marks 75%. These three cut points summarize where the lower quarter ends, where the middle lies, and where the upper quarter begins, which is especially helpful when the distribution is not symmetric.\nThe interquartile range (IQR) is the distance between the third and first quartiles, ($ = Q3 - Q1$ ). It measures how spread out the middle 50% of the data are: a wider IQR means the central values are more dispersed, and a narrower IQR means they are more tightly clustered. Like the standard deviation, the IQR is expressed in the same units as the original data, but unlike the standard deviation it is robust—a few extreme highs or lows have little effect on it because it depends only on the central half of the distribution.\nIn contrast, the range (maximum minus minimum) is very sensitive to outliers and can change dramatically from sample to sample; it is useful for quick checks and for spotting obvious data-entry errors when you know plausible bounds, but it is not a stable measure of spread. By comparison, the variance/standard deviation fluctuate less than the range, and the IQR fluctuates less than either of them.\n\n\n\n\n\n\nNote\n\n\n\nWhy is the IQR useful?\n\nIt does not depend on extreme values.\nIt gives a good summary of the central spread\nIt is preferred in EDA for skewed or messy data.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nShape (Skewness and kurtosis)\nThe normal distribution is a foundational concept in statistics. It is perfectly symmetric: the left and right sides of its bell-shaped curve are mirror images. This means the mean, median, and mode of a normal distribution are all equal.\n\n\n\n\n\nBut real-world data rarely follow perfect symmetry. That’s where skewness comes in.\nSkewness describes asymmetry. A positive skew means a long right tail (very high values are more common than very low ones); a negative skew means a long left tail.\n\n\n\n[source:https://towardsdatascience.com/skewness-kurtosis-simplified-1338e094fc85/]\n\n\nOne common (sample-based) formula is\n\\[\n\\text{Skewness} = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i - \\bar{x}}{s} \\right)^3\n\\]\nwhere \\(\\bar{x}\\) is sample mean, \\(s\\) is sample standard deviations and \\(n\\) is number of observations.\nWhen you don’t have access to the full dataset but know summary values like the mean, median, or mode, skewness can be approximated using\n\nPearson’s First Coefficient:\n\n\\[\n\\text{Skewness} = \\frac{\\bar{x} - \\text{Mode}}{s}\n\\]\n\nPearson’s Second Coefficient:\n\n\\[\n\\text{Skewness} = \\frac{3(\\bar{x} - \\text{Median})}{s}\n\\]\nThese are easier to compute and often agree on the direction (sign) of skew, even if the magnitude differs.\n\n\n\n\n\n\nNote\n\n\n\nHow to Interpret Skewness\n\nSkewness = 0 → The distribution is symmetric.\nSkewness &gt; 0 → The distribution is right-skewed (long tail on the right).\nSkewness &lt; 0 → The distribution is left-skewed (long tail on the left).\n\n\n\nWhen we compute these from a sample, we get estimates with standard errors. As a rough rule: if an estimate is within about \\(\\pm 2\\) standard errors of zero, there is no strong evidence of skewness or extra tail weight; if it is more than \\(2\\) standard errors away, that suggests positive/negative skew or heavier/lighter tails in that direction.\nIn R, example is\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nKurtosis describes tail weight (and peak shape) relative to a Normal distribution. Positive kurtosis (“heavy tails”) means more extreme values and a sharper center peak; negative kurtosis (“light tails”) means fewer extremes and a flatter, broader peak.\n\n\n\n[Source: https://towardsdatascience.com/skewness-kurtosis-simplified-1338e094fc85/]\n\n\nThe most common formula used to calculate raw kurtosis is\n\\[\n\\text{Kurtosis} = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i - \\bar{x}}{s} \\right)^4\n\\]\nA normal distribution has a kurtosis of 3. This value serves as a benchmark. To make comparisons easier, we often use a version called excess kurtosis, which is defined as:\n\\[\n\\text{Excess Kurtosis} = \\text{Kurtosis} - 3\n\\]\nThis transformation re-centers the scale so that the normal distribution has excess kurtosis equal to 0.\nWhen excess kurtosis is positive, the distribution is called leptokurtic. This means the distribution has heavier tails and a sharper peak than a normal distribution—indicating that extreme values are more likely. When excess kurtosis is negative, the distribution is referred to as platykurtic, meaning it has lighter tails and a flatter peak, with fewer extreme values than a normal distribution.\nThus, kurtosis tells us not just about the peak of the distribution, but more importantly, about how likely we are to observe values that are far from the mean.\nExamples in R are\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn short, skewness tells us whether a distribution is symmetric or lopsided, while kurtosis tells us whether the distribution has unusual peaks or heavy/light tails compared to a normal distribution.\n\n\n\n[Source: https://medium.com/@the_social_byte/why-should-we-care-about-skewness-and-kurtosis-in-data-analysis-bf6bff4b4ab6]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nUse with care. Skewness and kurtosis are sensitive to outliers and can be unstable in small samples.\nTip: Treat them as supporting evidence, and always pair with plots (histogram/density and a QQ-plot) before drawing conclusions.\n\n\nSee this link for more examples in R about skewness and kurtosis.\nExtremes and outliers\nExtremes are simply the minimum and maximum (and other high/low quantiles). They’re expected in any sample.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOutliers are observations unusually far from the bulk relative to a rule or model (context matters).\nTukey’s rule flags an observation as a mild outlier if it lies below Q1 − 1.5×IQR or above Q3 + 1.5×IQR, and as an extreme outlier if it lies beyond Q1 − 3×IQR or Q3 + 3×IQR.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExerciseSolutions\n\n\nIn the dataset mpg,\n\nCompute the mean and median of cty. Which is larger? What does that suggest about skew?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFor displ (engine displacement), report SD and IQR. Which would you prefer if the distribution is skewed? Why?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFind the modal category of drv (f/r/4). Also report proportions.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCompute skewness and kurtosis for hwy, cty, and displ. Which looks most skewed? Which has the heaviest tails?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFor cty, compute min/max and identify outliers using the 1.5×IQR rule. How many outliers?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nDo it yourself! :) \n\n\n\nMultivariate Case\nCross-tabulation is the basic non-graphical way to study the relationship between two (or more) variables. For two categorical variables, make a two-way table of counts, and (depending on your question) add row % (each row sums to 100), column % (each column sums to 100), and/or cell % (table sums to 100). You can extend this to a third variable by producing one two-way table at each level of the third variable (or by using a three-way contingency table).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAssociations\nIn EDA, we often want to ask Is one variable associated with another?\nThis can be for:\n\nTwo numeric variables\nTwo categorical variables\nOne numeric and one categorical\n\nPairs of Numeric Variables\nThe most common way to measure association between two numeric variables is:\nPearson’s Correlation Coefficient\n\nMeasures Strength and direction of a linear relationship.\nValues range from \\(-1\\) to \\(+1\\):\n\n\\(+1\\): Perfect positive linear association\n\\(-1\\): Perfect negative linear association\n\\(0\\): no linear relationship\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPerson’s \\(r\\) only measures linear relationships.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIf the relationship is curved, \\(r\\) can be misleading. See Anscombe’s Quartet for famous examples.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis figure is Anscombe’s quartet—four tiny datasets that share (nearly) the same summary statistics (means of \\(x\\) and \\(y\\), variances, Pearson correlation) but look very different once you plot them.\n\nDataset I (top-left): A textbook linear relationship with ordinary scatter around the line. Here a linear model is sensible.\nDataset II (top-right): A clear curved (non-linear) pattern. The fitted straight line and the correlation suggest “strong linearity,” but that’s misleading—a linear model is inappropriate.\nDataset III (bottom-left): Almost perfectly linear except for one outlier in \\(y\\). That single point inflates the correlation and affects the fit; without it the pattern is very regular.\nDataset IV (bottom-right): Most \\(x\\) values are the same (a vertical stack); a single high-leverage point at large \\(x\\) drags the regression line to create a high correlation/“good” fit. The model is being driven by one point.\n\n\n\n\n\n\n\nTip\n\n\n\nNumbers alone can fool you. Correlation, means, and regression coefficients can look “good” even when the relationship is non-linear or dominated by outliers/high-leverage points. Always plot your data, and follow up with diagnostics (residual plots, leverage/Cook’s distance) before trusting a model.\n\n\nThe population correlation (\\(\\rho\\)) is defined as\n\\[\n\\rho = \\frac{Cov(X, Y)}{\\sqrt{\\sigma_X \\sigma_Y}} =\\frac{E[(X - \\mu_X)(Y - \\mu_Y)]}{\\sqrt{E[(X - \\mu_X)^2E[(Y - \\mu_Y)^2}}\n\\]\nwhere \\(\\mu_X\\) and \\(\\mu_Y\\) are the population means, \\(\\sigma_X\\) and \\(\\sigma_Y\\) are the population standard deviations.\nIn practical, we do not know \\(\\mu\\) or \\(\\sigma\\) , so we replace them with their sample counterparts and compute the Pearson’s \\(r\\) (sample) as\n\\[\nr=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i-\\bar{y})^2}}\n\\]\nRank Correlations: For Non-linear Relationships\nIf the relationship is not linear, use a rank correlation:\n\n\n\nMethod\nNotes\n\n\n\n\nSpearman’s \\(\\rho\\)\nMore sensitive to outliers\n\n\nKendall’s \\(\\tau\\)\nBetter for ordinal data\n\n\n\nThese methods:\n\nconvert data into ranks (1st, 2nd, 3rd, …)\nMeasure how well the ranks agree between two variables\n\nThey are interpreted like Pearson’s \\(r\\):\n\nClose to \\(\\pm 1\\) means strong association\n\\(0\\) means no association\n\nPairs of Categorical Variables\nFor two categorical variables, we ask Are certain combinations of categories more or less common than expected?\nThe most basic tool is the contingency table. It counts how many times each combination of categories appears in the data.\nThis table helps us to answer questions like:\n\nDo some combinations occur more often than others?\nAre the two categorical variables associated?\n\nExample: Penguins Species and Islands\nImagine we observed \\(344\\) penguins from three species on three islands. We record how many penguins of each species were found on each island.\n\n\n\nSpecies\nBiscoe\nDream\nTorgersen\nTotal\n\n\n\n\nAdelie\n44\n56\n52\n152\n\n\nChinstrap\n0\n68\n0\n68\n\n\nGentoo\n124\n0\n0\n124\n\n\nTotal\n168\n124\n52\n344\n\n\n\nThe formula\nSummary of Correlation and Association Measures in R\nThis table shows which association or correlation measure to use depending on the types of variables, along with the appropriate R function.\n\n\n\n\n\n\n\n\n\nVariable Types\nMeasure\nInterpretation\nR Function / Package\n\n\n\n\nNumeric – Numeric\nPearson’s r\nStrength of linear relationship\ncor(x, y, method = \"pearson\")\n\n\nNumeric – Numeric\nSpearman’s ρ\nStrength of monotonic relationship\ncor(x, y, method = \"spearman\")\n\n\nNumeric – Numeric\nKendall’s τ\nRank-based measure; robust to ties\ncor(x, y, method = \"kendall\")\n\n\nOrdinal – Ordinal\nSpearman’s ρ or Kendall’s τ\nAssociation between ranks\nSame as above\n\n\nNominal – Nominal (2×2)\nPhi (φ) coefficient\nAssociation between two binary variables\npsych::phi(table)\n\n\nNominal – Nominal (k×m)\nCramér’s V\nStrength of association in contingency table\nrcompanion::cramerV(table) or DescTools::CramerV(table)\n\n\nNominal – Numeric\nEta (η) coefficient\nHow much variance in numeric var is explained by group\nDescTools::EtaSq(aov(y ~ group))\n\n\nBinary – Binary\nTetrachoric correlation\nFor binary variables assumed to reflect latent continuous variables\npsych::tetrachoric(table)$rho\n\n\nOrdinal – Nominal\n(No standard measure)\nUse visual summaries or grouped statistics\ntable(), ggplot2::geom_bar(), mosaicplot()\n\n\n\n\nNotes\n\nSpearman’s ρ and Kendall’s τ both work well for ranked data and are more robust to non-linear trends than Pearson’s r.\nCramér’s V is used for nominal data, and its values range from 0 to 1.\nEta squared (η²) measures how much of the variance in the numeric variable is explained by the group (nominal) variable.\nFor ordinal + nominal, no widely accepted coefficient exists; focus on visual summaries (e.g., bar charts or stacked plots).\n\n\nExample: What Should You Use?\n\n\n\nSituation\nUse\n\n\n\n\nHeight vs Weight\nPearson’s r\n\n\nExam Rank vs Satisfaction Level\nKendall’s τ\n\n\nEye Color vs Blood Type\nCramér’s V\n\n\nGender vs Income\nEta\n\n\nPlant Type vs Size Category\nCross-tab\n\n\n\nThese summaries help us compare groups and form initial impressions (for example, the mean suggests a “typical” value). But a few numbers can not tell the whole story—different datasets can share the same mean and spread yet look very different, so always pair numbers with plots and context.\n\nExerciseSolutions\n\n\nIn the dataset mtcars,\n\nCompute the Pearson correlation between mpg (miles per gallon) and hp (horsepower).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCompute the Spearman correlation between the same two variables.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nNow, answer these questions:\n\nIs the relationship positive or negative?\nDo Pearson’s r and Spearman’s \\(\\rho\\) give similar results? Why or why not?\n\n\n\nDo it yourself! :)\n\n\n\n\nExerciseSolutions\n\n\nIn the dataset Titanic,\n\nCreate a contingency table of passenger Class vs Survived.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCompute appropriate bivariate association for this association\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nNow, answer this question:\n\nHow strong is the association according to the used coefficient?\n\n\n\nDo it yourself! :)\n\n\n\n\nExerciseSolutions\n\n\nIn the dataset palmerpenguins::penguins,\n\nCompute the correlation between bill_length_mm and bill_depth_mm.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBuild a contingency table of species and island and compute Cramér’s V.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nDo it yourself! :)\n\n\n\n\nExerciseSolutions\n\n\nIn the dataset iris,\nCompute the appropriate coefficient of bi-variate association between Sepal.Length and Species.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nDo it yourself! :)\n\n\n\nComprehensive Exercise: Exploring the airquality Dataset\nThe dataset airquality (built into R) contains daily air quality measurements in New York, May–September 1973.\nVariables include:\n\nOzone (ppb),\nSolar.R (solar radiation),\nWind (mph),\nTemp (°F),\nMonth (5 = May, …, 9 = September),\nDay (day of month).\n\n\nDescriptive StatisticsAssociations\n\n\n\nInspect the dataset: How many rows and columns does it have? Are there missing values?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCompute the mean, median, and standard deviation of Ozone, Wind, and Temp.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nFor Month, create a frequency table. Which month has the most observations?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCompute the Pearson correlation between Ozone and Temp.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCompute the Spearman correlation betweenOzone and Solar.R.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCompute the correlation between Ozone and Month\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhich factor (Temp, Wind, Solar.R, or Month) seems most strongly associated with Ozone levels?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n2.4.2.2 Graphical Summaries\nGraphs help us see patterns, spot outliers, and understand distributions.\nCommon EDA plots include: - Histograms – show the shape and spread of a numeric variable - Boxplots – show center, spread, and outliers - Bar charts – show frequencies of categories - Scatterplots – show relationships between two numeric variables\nGraphs are easier to understand than tables of numbers. They help us and others see what’s going on in the data.\n\n\n2.4.2.2.1 Principles of Analytic Graphics\nBased on (Tufte 2006), there are six key principles for designing informative and effective graphs.\nPrinciple 1 – Show Comparison\n\n\n\n\n\n\nShowing comparisons is really the basis of all good scientific investigation. (Peng 2012)\n\n\n\nGood data analysis always involves comparing things. A single number or result doesn’t mean much on its own. We need to ask:\n\n\n\n\n\n\nCompared to what?\n\n\n\nFor example, if we see that children with air cleaners had more symptom-free days, that sounds good. But how do we know the air cleaner made the difference? We only know that by comparing to another group of children who didn’t get the air cleaner. When we add that comparison, we can see that the control group didn’t improve — so the improvement likely came from the air cleaner.\n\n\n\nChange in symptom-free days with air cleaner. Source: @Peng2012\n\n\n\n\n\nChange in symptom-free days with air cleaner. Source: @Peng2012\n\n\n\n\n\nChange in symptom-free days by treatment group. Source: @Peng2012\n\n\nGood data graphics should always show at least two things so we can compare and understand what’s really happening.\nPrinciple 2: Show Causality and Explanation\nWhen making a data graphics, it is helpful to show why you think something is happening – not just what is happening. Even if you can not prove a cause, you can show your hypothesis of idea about how one thing might lead to another.\n\n\n\n\n\n\nIf possible, it is always useful to show your causal framework for thinking about a question. (Peng 2012)\n\n\n\n\nFor example, in #figEDA2 we saw that children with an air cleaner had more symptom–free days. But that alone does not explain why. A good follow-up question is: “Why did the air cleaner help?“ One possible reason is that air cleaners reduce fine particles in the air – especially in homes with smokers. Breathing in these particles can make asthma worse, so removing them might help children feel better. To show this, we can make a new plot.\n\n\n\nChange in symptom-free days and change in PM2.5 levels in-home. Source: Peng (2012)\n\n\nFrom the plot, we can see:\n\nChildren with air cleaners had more symptom-free days.\nTheir homes also had less PM2.5 after six months.\nIn contrast, the control group had little improvement.\n\nThis pattern supports the idea that air cleaners work by reducing harmful particles — but it is not final proof. Other things might also cause the change, so more data and careful studies are needed to confirm.\nPrinciple 3: Show Multivariate Data\n\n\n\n\n\n\nThe real world is multivariate. (Peng 2012)\n\n\n\nIn real life, most problems involve more than one or two variables. We call this multivariate data. Good data graphics should try to show these multiple variables at the same time, instead of reducing everything to just one number or a simple trend.\nLet us look at an example.\nThe mtcars dataset contains information about 32 car models from the 1970s. Each row is a car and each column is a variable. Some of these variables are: mpg: miles per gallon (fuel efficiency), wt: weight (in 1000 lbs), cyl: number of cylanders (engine size), hp: horse power, qsec: 1/4 mile time (acceleration), am: Transmission (0 = auto, 1 = manual). These variables help us to explore relationship between engin size, weight, fuel use, and more.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe want to know how a car’s weight affects its fuel efficiency (miles per gallon). We look at a simple scatter plot of these two variables.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHeavier cars tend to have lower fuel efficiency. But is weight the only thing affecting fuel use?\nCars also have different engine sizes, measured by cylinders (cyl). This affects both weight and fuel efficiency. To understand the relationship better, we add cyl as a third variable by coloring points by the number of cylinders.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, we see that cars with 4, 6, and 8 cylinders have different trends. The overall pattern changes once we include this third variable.\n\n\n\n\n\n\nCaution\n\n\n\nThe number of cylinders cofounds the relationship – it influences both weight and MPG.\n\n\nTo understand how the number of cylinders changes the relationship between weight and fuel efficiency, we can make separate plots for each cylinder group. This is called faceting.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nImportant\n\n\n\nSometimes, looking at groups separately helps us find clearer patterns that get lost in a big mixed dataset.\n\n\nEven, we can add the variable transmission(am) as an additional variable by using shapes or facet.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, each panel shows either automatic automatic or manual cars. Within each panel, colors show the number of cylinders.\nEven we can use facet_grid that allows us to split the plot into rows and columns based on two categorical variables – perfect for showing how relationships vary across combinations. We will use am in rows and cyl in columns.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis way, we can see how the relationship between weight and MPG changes in each group combination.\nPrinciple 4: Integrate Evidence — Keep the message clear\nWhen you make a graph, do not rely on points and lines to show your idea. You can also use: Numbers to give exact values, Words or short labels to explain what is happening, Pictures or diagrams to give context.\n\n\n\n\n\n\nImportant\n\n\n\nA good graph tells a complete story.\n\n\nUse all tools you need – not just the ones your software gives you easily.\n\n\n\n\n\n\nImportant\n\n\n\nThe goal is not to just make a nice picture, but to help people understand your message clearly.\n\n\nPrinciple 5: Describe and Document the Evidence\nA good graph tells a story – clearly and completely. That means it should include:\n\nA clear title\nLabels for the x-axis and y-axis\nUnits for measurement (e.g. ‘weights in 1000 lbs’)\nTime scale if needed (e.g. ‘daily’, ‘monthly’)\nwhere the data comes from (e.g., ‘New York’, ‘EPA’)\nsource of the data\n\n\n\n\n\n\n\nTip\n\n\n\nImagine someone looking only at your plot without reading anything else. Can they understand the main idea? if yes – your plot is doing a good job.\n\n\n\n\n\n\n\n\nNote\n\n\n\nEven if your graph is not final, it is a good habit to label things early. It helps you and others understand what is going on.\n\n\nFor example, instead of using\ntry this\nwhen we use ggplot2, it is better to add labs() like we did until now.\nPrinciple 6: Content is King\n\n\n\n\n\n\nImportant\n\n\n\nA beautiful plot means nothing if the question is weak or the data is poor.\n\n\nData graphics are only powerful when:\n\nthe question is clear and important\nthe data is high quality and relevant\nthe evidence supports the question\n\nNo chart or fancy design can fix a bad question or messy data. That is why it is crucial to start with a strong idea and only show what really matters to answer that idea.\n\n\n\n\n\n\nTip\n\n\n\nDo not just decorate your data – focus on the message!",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html#airquailty-dataset",
    "href": "chapters/chapter4_EDA.html#airquailty-dataset",
    "title": "2  Exploratory data analysis (EDA)",
    "section": "2.5 Airquailty dataset",
    "text": "2.5 Airquailty dataset\nWe will explore the airquality dataset, which contains daily measurement of air pollutants and weather conditions in New York City May to September 1973.\nStep 1 – How do ozone levels vary across different months in New York during the summer in 1973?\nThis question is specific and focused: One location (New York), One variable (ozone), one year and a defined time window (May to September)\nStep 2 – The airquality dataset is built into R. Load it with:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 3 – Check the size and dimension of the dataset\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 4 – run str()\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nCaution\n\n\n\nThere are some missing values (NA , Not Available) in the dataset.\n\n\nStep 5 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 6 –\nFirst, we count how many rows (i.e., records or observations) are in the dataset.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMissing values (denoted as NA in R) can lead to incorrect calculations or unexpected results. It is good practice to check how many missing values there are per variable.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nUsing `dplyr`\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis command uses across() to apply the same function to all columns, and sum(is.na(.)) to count the number of missing values per column.\n\n\n\n\n\n\nTip\n\n\n\nWhat is ~ ?\nThe ~ introduces an anonymous function — a function written inline without a name.\nThis\n\n~ sum(is.na(.))\n\n~sum(is.na(.))\n\n\nis a shorthand for\n\nfunction(x) sum(is.na(x))\n\nfunction (x) \nsum(is.na(x))\n\n\n\n\nOr, we can check how many rows are from July:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ncheck how many missing values are just in August.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 7 – According to the U.S. EPA, ozone levels above 70 parts per billion (ppb) may be considered unhealthy.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBased on the output, many days have safe levels and some days have extremely high ozone levels.\nStep 8 – Let us check the average ozone level by month:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis gives a quick overview of how ozone levels change over the summer. Let visualize it:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBased on the plot, we observe that July and August have the highest median ozone levels, showing that typical ozone concentrations were significantly higher during mid-summer. This suggests a seasonal pattern, likely driven by higher temperatures and increased sunlight, which promote the formation of ozone. The taller boxes and whiskers for these months reflect a greater variability in ozone levels, including several high outliers. In contrast, May and September show lower median values and less variation, possibly due to cooler temperatures and different atmospheric conditions. The median ozone level is June is slightly higher than in May but lower than in July, with a moderate level of variability. Overall, this seasonal trend is consistent with environmental science – ozone forms more easily in strong sunlight and warm conditions, which are more prevalent in July and August.\nStep 9 – Let us examine whether temperature or wind speed might help explain ozone patterns.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou may observe that there is a positive relationship between temperature and ozone and a negative relationship between wind speed and ozone. These patterns support common environmental science findings.\nStep 10 – Based on our findings, we might fit a simple regression model\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe might also refine our question: On which days was the ozone level unusually high, and what were the weather conditions on those days?\nAlternative question\nStep 1 – Which weather factor—temperature, wind, or solar radiation—has the strongest relationship with ozone levels during the summer of 1973 in New York?\nThis question is more analytical than descriptive, and focused on relationships between variables. So, we need to examine how ozone changes with respect to other variables.\nStep 2 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 3 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 4 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 5 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 6 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 7 –\nCheck how often ozone exceeds EPA’s 70 ppb guideline:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 8 –\nSince dplyr does not include a cor() function, we will compute correlations manually using summarise() and cor() from base R, keeping within tidy pipelines:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMore advanced:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBut sometimes, a simple code gives the more informative output:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nvisualize the relationships:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 9 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhen we check p-values and coefficients, we see Temp has strong and significant positive effect, Wind has strong and significant negative effect, and Solar.R has weaker effect but still contributes.\nStep 10 –\nNow we know temperature has the strongest effect on ozone levels:\n\nShould we check for nonlinear effects (e.g., does ozone spike at high temps)?\nWould a time-based model (e.g. by day or month) add insight?\nWhat is the temperature threshold where ozone exceeds 70 ppb?\nAre there interactions between variables (e.g. high temp + low wind)?\nDo results change if the we include time (e.g. month)?\n\nWe might now refine our question: How much does temperature need to rise before ozone levels exceed the 70 ppb threshold?\nAs Tukey emphasized, EDA is about “detecting the unexpected” and learning from the data before attempting to explain it.\n\n\n\n\n\n\nTip\n\n\n\nAt the beginning, it is hard to ask the perfect question because you do not know the date well yet. But here is the secret:\n\n\n\n\n\n\nThe more questions you ask, the better your questions become.\n\n\n\n\n\nEach time you explore one idea, it leads to a new question. That is how you:\n\nDiscover patterns\nNotice surprises\nUnderstand your data more deeply\n\n\n\n\n\n\n\nCaution\n\n\n\nGood EDA is like this\n\nAsk a question\nMake a plot or summary\nLook at the result\nAsk a new, better question\nRepeat!\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDo not wait for the perfect question - just start. Exploration will lead you to insight.\n\n\n\n\n\n\nMidway, Steve. 2022. Data Analysis in r. https://bookdown.org/steve_midway/DAR/.\n\n\nPeng, Roger D. 2012. Exploratory Data Analysis with R. https://bookdown.org/rdpeng/exdata/.\n\n\nTufte, Edward. 2006. Beautiful Evidence. Graphics Press LLC.\n\n\nTukey, John Wilder. 1977. Exploratory Data Analysis. Addison-Wesley Publishing Company.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html",
    "href": "chapters/chapter3_dplyr.html",
    "title": "3  Data Wrangling",
    "section": "",
    "text": "3.1 Filter observations - filter()\nThe dplyr package is a part of the R tidyverse : an ecosystem of several libraries designed to work together by representing data in common formats.\nThe data frame is a key data structure in statistics and in R. The basic structure of a data frame is that there is one observation per ro and each column represents a variable, a measure, feature, or characteristic of that observation. Given the importance of managing data frames, it is important that we have good tools for dealing with them.\nThe dplyr package is a relatively new R package that allows you to do all kinds of analyses quickly and easily.\nOne important contribution of the dplyr package is that it provides a “grammar” (in particular, verbs) for data manipulating and for operating on data frames. With this grammar, you can sensibly communicate what it is that you are doing to a data frame that other people can understand (assuming they also know the grammar). This is useful because it provides an abstraction for data manipulation that previously did not exist.\nSome of the key “verbs” provided by the dplyr package are\nTo install the dplyr package from CRAN, just run\nAfter installing the package, it is important to load it into the R session.\nTo better understanding, let us continue within analyzing a dataset, penguins, available within the palmerpenguins package. we need to load the dataset and if it is necessary, we need to install the package.\n‌The data frame contain data for \\(344\\) penguins and \\(8\\) variables describing the species (species), the island (island), some measurements of the size of the bill (bill_length_mm and bill_depth_mm), flipper (flipper_length_mm) and body mass (body_mass_g), the sex (sex) and the study year (year). More information about the data frame can be found by running ?penguins .\nYou can see some basic characteristics of the dataset with the dim() and str() functions.\nThe first and last 6 rows can be displayed by head() and tail(), respectively.\nThe summary information of data frame can be found by summary() .\nThis function works on both quantitative and qualitative variables.\nYou can combine multiple conditions using & if all conditions must be true (cumulative), or | if at least one condition must be true (alternative). For example,\nAs you can see, the filter() functions require the name of the data frames as the first argument, then the condition (with the usual logical operators &gt;, &lt;, &gt;=, &lt;=, ==, !=, %in%, etc.) as second argument.\nSo with the pipe operator, the code above becomes:\nInstead of listing the data frame’s name as the initial argument within functions like filter() (or other {dplyr} functions), you simply specify the data frame once, then use the pipe operator to connect it to the desired function.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#filter-observations---filter",
    "href": "chapters/chapter3_dplyr.html#filter-observations---filter",
    "title": "3  Data Wrangling",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nVariable names should be used directly, without enclosing them in single or double quotation marks (' or \").\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nTo use any functions in the dplyr package, you must specify the data frame’s name as the first argument. Alternatively, you can use the pipe operator (|&gt; or %&gt;% ) to avoid explicitly naming the data frame within each function.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe keyboard shortcut for the pipe operator is ctrl+shift+M on Windows or cmd + shift + M on Mac. By default, this will produce %&gt;%, but if you have configured RStudio to use the native pipe operator, it will print |&gt; .\n\n\n\n\n\n\nthe %&gt;% pipe, originating from the magrittr package (included in the tidyverse), was superseded in R 4.1.0 (released in 2021) by the native |&gt; pipe. We recommend |&gt; because it’s a simpler, built-in feature of base R, always available without needing additional package loads.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe pipe operator lets you chain multiple operations together, which is especially handy for performing several calculations on a data frame without saving the result of each intermediate step.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nThe pipe operator streamlines your code by feeding the output of one operation directly into the next, making your code much easier to write and read.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#extract-observations",
    "href": "chapters/chapter3_dplyr.html#extract-observations",
    "title": "3  Data Wrangling",
    "section": "3.2 Extract observations",
    "text": "3.2 Extract observations\nYou can extract observations from a dataset based on either their positions or their values.\n\n3.2.1 Based on Their Positions\nTo extract observations based on their positions, you can use the slice() function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFurthermore, for extracting specific rows like the first or last, you can use specialized functions:\n\nslice_head(): Extracts rows from the beginning of the dataset.\nslice_tail(): Extracts rows from the end of the dataset.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.2.2 Based on their values\nWhen you need to extract observations based on the values of a variable, you can use:\n\nslice_min(): Selects rows with the lowest values, allowing you to define a specific proportion.\nslice_max(): Selects rows with the highest values, also with the option to define a proportion.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#sample-observations",
    "href": "chapters/chapter3_dplyr.html#sample-observations",
    "title": "3  Data Wrangling",
    "section": "3.3 Sample Observations",
    "text": "3.3 Sample Observations\nSampling observations can be achieved in two ways:\n\nsample_n(): Takes a random sample of a specified number of rows.\nsample_frac(): Takes a random sample of a specified fraction of rows.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nIt is important to note that, similar to the base R sample() function, the size argument can exceed the total number of rows in your data frame. If this happens, some rows will be duplicated, and you will need to explicitly set the argument replace = TRUE.\n\n\nAlternatively, you can obtain a random sample (either a specific number or a fraction of rows) using slice_sample(). For this, you use:\n\nThe argument n to select a specific number of rows.\nThe argument prop to select a fraction of rows.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#sort-observations",
    "href": "chapters/chapter3_dplyr.html#sort-observations",
    "title": "3  Data Wrangling",
    "section": "3.4 Sort observations",
    "text": "3.4 Sort observations\nObservations can be sorted using the arrange() function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBy default, arrange() sorts in ascending order. To sort in descending order, simply use desc() within the arrange() function, like arrange(desc(variable_name)).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSimilar to filter(), arrange() can sort by multiple variables and works with both quantitative (numerical) and qualitative (categorical) variables. For example, arrange(sex, body_mass) would first sort by sex (alphabetical order) and then by body_mass (ascending, from lowest to highest).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to note that if a qualitative variable is defined as an ordered factor, the sorting will follow its defined level order, not alphabetical order.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#select-variables",
    "href": "chapters/chapter3_dplyr.html#select-variables",
    "title": "3  Data Wrangling",
    "section": "3.5 Select variables",
    "text": "3.5 Select variables\nYou can select variables using the select() function based on their position or name.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo remove variables, use a - sign before their position or name.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can also select a sequence of variables by name (e.g., select(df, var1:var5)).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFurthermore, select() provides a straightforward way to rearrange column order in your data frame.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n3.5.1 Select using helper functions\nThe select() function also supports helper functions for matching column names based on patterns:\nstarts_with(\"abc\") selects all columns whose names begin with the specified string.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nends_with(\"xyz\") selects all columns whose names end with the specified string.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ncontains(\"ijk\") selects all columns that contain the specified substring anywhere in their name.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe everything() helper selects all remaining columns that have not been explicitly mentioned. It is useful when you want to: move some variables to the front, or keep all others in their existing order.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nnum_range(\"prefix\", 1:3) selects variables with names like \"prefix1\", \"prefix2\", \"prefix3\", etc. This is useful for selecting numbered variables.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nmatches(\"(.)\\\\1\") selects columns whose names match a regular expression (regex).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis example selects aa and bb but not ab, since the pattern (.)\\\\1 means “a character repeated twice”.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#renaming-variables",
    "href": "chapters/chapter3_dplyr.html#renaming-variables",
    "title": "3  Data Wrangling",
    "section": "3.6 Renaming Variables",
    "text": "3.6 Renaming Variables\nTo rename variables, use the rename() function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nRemember the syntax: new_name = old_name. This means you always write the desired new name first, followed by an equals sign, and then the current old name of the variable.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#create-or-modify-variables",
    "href": "chapters/chapter3_dplyr.html#create-or-modify-variables",
    "title": "3  Data Wrangling",
    "section": "3.7 Create or Modify Variables",
    "text": "3.7 Create or Modify Variables\nThe mutate() function allows you to create new variables or modify existing ones. You can base these operations on another existing variable or a vector of your choice.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nIf you create a variable with a name that already exists, the old variable will be overwritten.\n\n\nSimilar to rename(), mutate() requires the argument to be in the format name = expression, where name is the column being created or modified, and expression is the formula for its values.\nThere is another function transmute() that is also used to create a new variable in a data frame by transforming existing ones. However, unlike mutate(), which keeps all original columns and simply adds the new ones, transmute() returns only the newly created variables. This means that when you use transmute(), the resulting data frame will include just the variables you explicitly define inside the function. It is particularly useful when you want a clean output focused only on the transformed results, without retaining the original dataset’s columns. In contrast, mutate() is ideal when you want to preserve the full structure of your data while adding new insights.\nLet us create a new column (body mass in kg).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#summarize-observations",
    "href": "chapters/chapter3_dplyr.html#summarize-observations",
    "title": "3  Data Wrangling",
    "section": "3.8 Summarize Observations",
    "text": "3.8 Summarize Observations\nTo get descriptive statistics of your data, use the summarize() (or summarise()) function in conjunction with statistical functions like mean(), median(), min(), max(), sd(), var(), etc.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nRemember to use na.rm = TRUE to exclude missing values from calculations.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#identify-distinct-values",
    "href": "chapters/chapter3_dplyr.html#identify-distinct-values",
    "title": "3  Data Wrangling",
    "section": "3.9 Identify Distinct Values",
    "text": "3.9 Identify Distinct Values\nThe distinct() function helps you find unique values within a variable.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhile typically used for qualitative or quantitative discrete variables, it works for any variable type and can identify unique combinations of values when multiple variables are specified. For instance, distinct(species, study_year) would return all unique combinations of species and study year.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#group-by",
    "href": "chapters/chapter3_dplyr.html#group-by",
    "title": "3  Data Wrangling",
    "section": "3.10 Group By",
    "text": "3.10 Group By\nThe group_by() function changes how subsequent operations are performed. Instead of applying functions to the entire data frame, operations will be applied to each defined group of rows. This is particularly useful with summarize(), as it will produce statistics for each group rather than for all observations.\nFor example, to calculate the mean and standard deviation of body_mass separately for each species, you would first group_by(species) and then summarize() the body_mass. The pipe operator smoothly passes the grouped data from group_by() to summarize().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can also group by multiple variables (e.g., group_by(var1, var2)), and the data frame’s name only needs to be specified in the very first operation of a chained sequence.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#managing-groups-ungroup",
    "href": "chapters/chapter3_dplyr.html#managing-groups-ungroup",
    "title": "3  Data Wrangling",
    "section": "3.11 Managing Groups: ungroup()",
    "text": "3.11 Managing Groups: ungroup()\nAfter performing operations on grouped data, the ungroup() function allows you to revert to a normal data frame, enabling operations on entire columns again or switching to new grouping criteria. Remember, you can also group_by() multiple columns simultaneously.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#number-of-observations",
    "href": "chapters/chapter3_dplyr.html#number-of-observations",
    "title": "3  Data Wrangling",
    "section": "3.12 Number of Observations",
    "text": "3.12 Number of Observations\nThe function n() returns the number of observations. It can only be used inside summarize().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhen combined with group_by(), you can easily get the number of observations per group. n() takes no parameters, so it’s always written as n().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNotably, the count() function is a convenient shortcut, equivalent to summarize(n = n()).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#number-of-distinct-values",
    "href": "chapters/chapter3_dplyr.html#number-of-distinct-values",
    "title": "3  Data Wrangling",
    "section": "3.13 Number of Distinct Values",
    "text": "3.13 Number of Distinct Values\nTo count the number of unique values or levels in a variable (or combination of variables), use n_distinct(). Like n(), it’s exclusively used within summarize().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou do not have to explicitly name the output; the operation’s name will be used by default (e.g., summarize(n_distinct(variable))).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#first-last-or-nth-value",
    "href": "chapters/chapter3_dplyr.html#first-last-or-nth-value",
    "title": "3  Data Wrangling",
    "section": "3.14 First, Last, or \\(n\\)th Value",
    "text": "3.14 First, Last, or \\(n\\)th Value\nAlso available only within summarize(), you can retrieve the first, last, or nth value of a variable. Functions like first(), last(), and nth() enable this.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThese functions offer arguments to handle missing values; for more details, consult their documentation (e.g., ?nth()).",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#conditional-transformations",
    "href": "chapters/chapter3_dplyr.html#conditional-transformations",
    "title": "3  Data Wrangling",
    "section": "3.15 Conditional Transformations",
    "text": "3.15 Conditional Transformations\n\n3.15.1 If Else\nThe if_else() function (used with mutate()) is ideal for creating a new variable with two levels based on a condition. It takes three arguments:\n\nThe condition (e.g., body_mass_g &gt;= 4000).\nThe output value if the condition is TRUE (e.g., “High”).\nThe output value if the condition is FALSE (e.g., “Low”).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nTip\n\n\n\nA key advantage is that if_else() propagates missing values (NA) if the condition’s input is missing, preventing misclassification.\n\n\n\n\n3.15.2 Case When\nFor categorizing a variable into more than two levels, case_when() is far more appropriate and readable than nested if_else() statements.\nWhile nested if_else() functions can technically work, they are prone to errors and result in difficult-to-read code. For instance, to classify body mass into “Low” (&lt;3500), “High” (&gt;4750), and “Medium” (otherwise), nested if_else() would look like this:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis code first checks if body_mass_g is less than 3500. If true, it assigns “Low”. If false, it then checks if body_mass_g is greater than 4750. If that’s true, it assigns “High”; otherwise, it assigns “Medium”. While functional, this structure can become complex and error-prone with more conditions.\ncase_when() evaluates conditions sequentially. To improve this workflow, we now use the case when technique:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis workflow is much simpler to code and read!\nIf there are no missing values in the variable(s) used for the condition(s), it can even be simplified to:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhile a .default argument can be used to specify an output for observations not matching any condition, exercise caution with missing values. If NA values in the conditioning variable are not explicitly handled, they might be incorrectly assigned to the default category. A safer approach is to explicitly define all categories or ensure NAs remain NA.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nRegardless of whether you use if_else() or case_when(), it’s always good practice to verify the newly created variable to ensure it aligns with your intended results.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#exploring-further-dplyr-functions",
    "href": "chapters/chapter3_dplyr.html#exploring-further-dplyr-functions",
    "title": "3  Data Wrangling",
    "section": "3.16 Exploring Further dplyr Functions",
    "text": "3.16 Exploring Further dplyr Functions\nUntil now, we have focused on analyzing the penguins dataset. To effectively explain some of dplyr’s other powerful functions, we’ll now shift to creating custom datasets tailored to demonstrate their specific functionalities.\n\n3.16.1 Separate and Unite\nYou can separate a character column into two or more new columns using separate().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nConversely, to combine two or more columns into a single character column, use unite().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.16.2 Reshaping Data: gather() and spread()\ngather() transforms “wide” format data into “long” or “tall” format by collapsing columns into key-value pairs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nConversely, spread() converts “long” or “tall” format data into “wide” format by separating key-value pairs across multiple columns.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#combining-datasets-joins",
    "href": "chapters/chapter3_dplyr.html#combining-datasets-joins",
    "title": "3  Data Wrangling",
    "section": "3.17 Combining Datasets: Joins",
    "text": "3.17 Combining Datasets: Joins\nSometimes, your data is split across multiple tables. For example, one table may have demographic information (like gender, marital status, height, weight), and another table may have medical records (like visits and surgeries).\nWhen working with data spread across multiple tables, you’ll often need to combine them based on a shared column (a “key column”)—a process known as joining or merging. dplyr provides several functions for common data joins.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ninner_join() keeps only the rows where “key column” exists in both tables and drops all rows that do not have a match in both.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nlefy_join() keeps all rows from the left table (here, table1) and fills in matching information from the right table (here, table2). If no match is found, you will get NAs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nfull_join() keeps all rows from both tables. Rows without a match in the other table will have NAs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can match on more than one column. For example, match ID and also make sure the gender in table1 matches sex in table2.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis is useful when one key (ID) is not unique enough by itself.\nFilter-based joins: These do not add new columns. They just filter rows:\nsemi_join() keeps only rows in table1 that have a match in table2. It does not add any columns from table2.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nanti_join() keeps only rows in table1 that do not have a match in table2. It is good for finding “missing” mathches.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nFor more information, plesee see chapter 19 of Wickham, Cetinkaya-Rundel, and Grolemund (2023) .\n\n\n\n\nExerciseHintsSolution\n\n\nUsing the starwars dataset and dplyr functions, perform the following data manipulations.\nPart 1: Initial Exploration and Filtering\n\nFilter for Human Characters: Create a new data frame called human_characters that only includes characters of the “Human” species.\nSelect Key Attributes: From human_characters, select only the name, height, mass, and homeworld columns.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPart 2: Calculating BMI and Identifying Extremes\n\nCalculate BMI: Add a new variable called bmi (Body Mass Index) to your human_characters data frame. The formula for BMI is \\(\\text{mass}/(\\text{height}/100)^2\\). Ensure that mass is in kilograms and height in centimeters as provided in the dataset.\nSort by BMI: Arrange the human_characters data frame in descending order based on their bmi.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPart 3: Grouped Summaries\n\nSpecies Statistics: Calculate the number of characters (n) and the average height for each species in the original starwars dataset.\nFilter Significant Species: From the previous summary, filter out species that have fewer than 5 characters and an average height greater than 100.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPart 4: Conditional Categorization\n\nCategorize Height: Add a new variable called height_category to the starwars dataset using mutate() and case_when().\n\nIf height is less than or equal to 100, categorize as “Short”.\nIf height is greater than 100 but less than or equal to 180, categorize as “Medium”.\nIf height is greater than 180, categorize as “Tall”.\nHandle NA values for height appropriately so they remain NA in height_category.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPart 1: Initial Exploration and Filtering\n\nFilter for Human Characters: which dplyr function helps you select rows based on a condition? Remember the syntax for checking equality.\nThere is a dplyr function specifially for choosing columns.\n\nPart 2: Calculating BMI and Identifying Extremes\n\nCalculate BMI: Which dplyr function is used to create or modify columns? Pay attention to the order of operations in the formula.\nSort by BMI: The arrange() function is key here. How do you specify descending order?\n\nPart 3: Grouped Summaries\n\nSpecies Statistics: You will need two main functions here: one to define groups and another to calculate summary statistics within those groups. Do not forget to handle NA values for the mean.\nFilter Significant Species: You will apply another filter() operation, but this time on the summarized data. Remember how to combine two conditions.\n\nPart 4: Conditional Categorization\n\nCategorize Height: case_when() is perfect for multiple conditions. How do you specify the conditions and their corresponding outputs? Remember to check for NAs first to ensure they do not get misclassified by other conditions.\n\n\n\n\n\nSolution. \nlibrary(dplyr)\n\n# Part 1: Initial Exploration and Filtering\nhuman_characters &lt;- starwars |&gt; \n  filter(species == \"Human\") |&gt; \n  select(name, height, mass, homeworld)\n\n# Part 2: Calculating BMI and Identifying Extremes\nhuman_characters_bmi &lt;- human_characters  |&gt; \n  mutate(bmi = mass / ((height / 100)^2))  |&gt; \n  arrange(desc(bmi))\n\n# Part 3: Grouped Summaries\nspecies_stats &lt;- starwars  |&gt; \n  group_by(species) %&gt;%\n  summarise(\n    n = n(),\n    avg_height = mean(height, na.rm = TRUE)\n  ) %&gt;%\n  filter(\n    n &gt;= 5, \n    avg_height &gt; 100\n  )\n\n# Part 4: Conditional Categorization\nstarwars_with_height_category &lt;- starwars %&gt;%\n  mutate(\n    height_category = case_when(\n      is.na(height) ~ NA_character_, # Handle NA values first\n      height &lt;= 100 ~ \"Short\",\n      height &gt; 100 & height &lt;= 180 ~ \"Medium\",\n      height &gt; 180 ~ \"Tall\"\n    )\n  )\n\n\n\n\nTo learn more about the dplyr package, here are some recommended resources:\n\ndplyr.tidyverse.org\nChapter “Data transformation” in the book “R for Data Science”\nCheatsheet\nBlog Stats and R\nKaggele\n\n\n\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed. https://r4ds.hadley.nz/.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html",
    "href": "chapters/chapter2_data_visualization.html",
    "title": "4  Data Visualization",
    "section": "",
    "text": "4.1 Data\nThe most famous package in R that is used for visualizing of data is ggplot2 that is based on the grammar of graphics. It allows you to `speak’ a graph from composable elements, instead of being limited to a predefined set of charts.\nBefore using ggplot2, a user must first install the package and then load it into their R session. Installation is done only using the command install.packages('ggplot2'). Alternatively, the following code can be used to install if only if the package is not already installed on the computer.\nwhich downloads the package from CRAN (the Comprehensive R Archive Network). After installation, the package needs to be loaded each time R is restarted. This is done with the following command:\nA brief overview about using this package is available at this website and more complete information about how to use ggplot2 can be found in Wickham, Navarro, and Pederson (2019).\nThe structure of the package includes 7 composable parts that come together as a set of instructions on how to draw a chart.\nThe package ggplot2 requires a minimum of three main components to create a chart: data, mapping, and a layer. Other components – like scales, facets, coordinates, and themes – are optional because ggplot2 gives them automatic settings that usually work well, so you do not need to adjust them too much.\nIn the following, we briefly describe these components:\nEvery plot made with ggplot2 starts with data. This data should be in a tidy format that means the data is in a table (called a rectangular data frame) where:\nThe first step to create a plot with ggplot2 is to pass this data to the ggplot() function, which stores the data to be used later by other parts of the plot.\nFor example, if we want to make a plot using the mpg dataset, we begin like this:",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#data",
    "href": "chapters/chapter2_data_visualization.html#data",
    "title": "4  Data Visualization",
    "section": "",
    "text": "Each row is one observation.\nEach column is one variable.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNoteYou should see an empty plot. It is correct. Can you explain why the plot is empty?",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#mapping",
    "href": "chapters/chapter2_data_visualization.html#mapping",
    "title": "4  Data Visualization",
    "section": "4.2 Mapping",
    "text": "4.2 Mapping\nIn ggplot2 package, mapping means telling the system how to connect parts of the data to aesthetic properties of the plot.\nAesthetic (say: es-THE-ik) is a word that describes how something looks – its style, color, shape, and beauty. In ggplot2, an aesthetic is a visual feature of a plot like:\n\n Position (x and y) – controls where data appears on the plot\n\n Color – changes color based on data values\n\n Size – adjusts the size of points\n\n Shape – uses different shapes for different categories\n\nThese help us turn numbers into pictures. It is how we ’’dress up” the data so it speaks to our eyes.\n\n\n\n\n\n\nTipMemory tip\n\n\n\nJust like fashion has aesthetic styles (like modern, classic, or colorful), plots also have aesthetics – they decide how the data looks!\n\n\nA mapping can be made by using aes() function to make pairs of graphical attributes and parts of the data. Inside aes(), we match parts of the data (like column names) with visual elements (like \\(x\\) and \\(y\\) position).\nFor the dataset mpg, if we want to show cty (city miles per gallon) on the \\(x\\)-axis and hwy (highway miles per gallon) on the \\(y\\)-axis, we write\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#layers",
    "href": "chapters/chapter2_data_visualization.html#layers",
    "title": "4  Data Visualization",
    "section": "4.3 Layers",
    "text": "4.3 Layers\nThe layer is the heart of any plot in ggplot2. A layer takes the mapped data and turns it into something that a human can see and understand – like points, lines, or bars.\nEach layer has three main parts: 1. Geometry – decides how the data is shown (for example: points, lines, bars) 2. Statistical transformation – can create new values from the data (like averages or smooth curves). 3. Position adjustment – controls where each part of the plot appears, especially when things overlap.\nA layer can be constructed using functions that start with geom_ (for geometry) and stat_ (for statistics). These function help us choose how the data looks and what to display.\n\n\n\n\n\n\nNote\n\n\n\nThe geom_*() and stat_*() functions usually control one part of the layer – like the geometry or the statistics – but you can still manullay choose the other two parts if you want.\n\n\nThe code below shows how to make a scatter plot with a trend line using two layers:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#scales",
    "href": "chapters/chapter2_data_visualization.html#scales",
    "title": "4  Data Visualization",
    "section": "4.4 Scales",
    "text": "4.4 Scales\nA scale translates what we see on the plot into something we can understand from the data – like showing how far, how big, or what category a point represents.\nEach scale is connected to an aesthetic – for example, the \\(x\\)-axis, \\(y\\)-axis, color, or size – and it controls things like - The limits of the plot (minimum and maximum values) - The breaks (where ticks or labels appear) - The format of the labels (like numbers or percentages) - Any transformation (like logarithmic scale)\nScales also create guides for the reader – like axes or legend – so we can better understand the meaning of the plot.\nScale functions in ggplot2 usually follow the format scale_[aesthetic]_[type](), where {aesthetic} is one of the pairing made in the mapping part of a plot. For example, scale_x_continuous() – for a numeric \\(x\\)-axis; scale_colour_viridis_d() – for discrete color with the Viridis palette.\nHere is how to use a custom color scale for the class variable in the mpg dataset:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis tells ggplot2 to use Viridis colors for the different car classes – making the plot easier to read and more accessible.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#facets",
    "href": "chapters/chapter2_data_visualization.html#facets",
    "title": "4  Data Visualization",
    "section": "4.5 Facets",
    "text": "4.5 Facets\nFacets are used to split a plot into smaller subplots, each showing a subet of the data. This is helpful when you want to compare groups – for example, different years, categories, or types – in separate panels.\nFacets are a powerful way to quickly see patterns, trends, or even no patterns in different parts of the data.\nFacets have their own mapping, written as a formula.\nThe following code creates one scatter plot of two variables cty and hwy of the dataset mpg for each combination of year and drv (driver type) in different panels; so, you can compare them side by side.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn facet_grid(year ~ drv), the rows are based on year and the columns are based on drv. As you see, this creates a grid of plots that makes it easy to explore how variables interact.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#coordinates",
    "href": "chapters/chapter2_data_visualization.html#coordinates",
    "title": "4  Data Visualization",
    "section": "4.6 Coordinates",
    "text": "4.6 Coordinates\nThe coordinates is the part if the plot that controls how position aesthetics (like \\(x\\) and \\(y\\)) are shown. You can think of it as the interpreter that tells the plot where to place things.\nMost ggplot2 plots use Cartesian coordinates, where data is shown on regular \\(x\\)-\\(y\\) grid. But you can also use other systems, like: Polar cooridinate (for circular plots) and Map projections (for geographic data).\nYou can also use coordinates to make sure that one unit on the \\(x\\)-axis is the same size as one unit on the \\(y\\)-axis. This is called a fixed aspect ratio. The function coord_fixed() does this ratio automatically.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#theme",
    "href": "chapters/chapter2_data_visualization.html#theme",
    "title": "4  Data Visualization",
    "section": "4.7 Theme",
    "text": "4.7 Theme\nThe theme controls parts of the plot that are not related to the data, such as background, text, grid, and legend position. It helps define the overall look and feel of the plot.\nFor instance, you can use the theme to: - Move or hide the legend - Change font size or colors - Set a new background style - Adjust the axes or grid lines\nSome them settings are hierarchical, meaning that if you change the general axis style, it also changes both the \\(x\\)-axis and \\(y\\)-axis unless you set them separately.\nTo change the appearance of a plot, you can use the built-in theme_*() functions (like theme_minimal(), theme_classic(), or theme_light()), or you can customize specific details using the theme() function. With element_*() functions (e.g. element_line(), element_text()) let you adjust the visual style of theme parts, such as lines, texts, or backgrounds.\nThe following code creates a scatter plot and applies a minimal theme. It also moves the legend to the top, makes axis lines thicker, and colors the bottom \\(x\\)-axis blue.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#mastering-plot-layers-guided-examples-and-student-activities",
    "href": "chapters/chapter2_data_visualization.html#mastering-plot-layers-guided-examples-and-student-activities",
    "title": "4  Data Visualization",
    "section": "4.8 Mastering Plot Layers: Guided Examples and Student Activities",
    "text": "4.8 Mastering Plot Layers: Guided Examples and Student Activities\nAs we mentioned before, a ggplot2 is made by layering different parts. You can combine data, mapping, geoms, scales, facets, coordinates, and themes to build a fully customized plot.\nBelow is an example that brings everything together:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nThere is no need to type argumants.\n\n\n\n4.8.1 Scatter Plot – geom_point()\nA scatter plot shows the relationship between two numeric variables. each point on the plot represents one observation.\n\n\n\n\n\n\nTip\n\n\n\nScatter plots are useful for seeing patterns, trends, or outliers in data.\n\n\nAn example for seeing the basic scatter plot is showing how height (in inches) changes with age (in years) for each person in the dataset heightweight.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nwe can customize the size of points by adding size = 1.5 to the function geom_point().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, we can add two aesthetics: shape and color, based on the sex variable.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, we are mapping colour or size to a numeric variable (e.g. weightLb)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThese plots show how weight varies with age and height using color or size.\nNow, we are mapping colour to the variable sex and size to a numeric variable weightLb\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExerciseHintsSolution\n\n\nUsing the heightweight dataset, make a scatter plot to explore the relationship between heightIn and weightLb variables. Your tasks are:\n\nUse points to represent the data.\nShow the variable sex using color so that we can see the difference between males and females.\nMake the points larger than usual, with a size of \\(2.5\\).\nFinally, give your plot a title that describes what it shows, The title should be: “Height vs, Weight by Sex”\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nYou will need\n\naes(x = ______, y = ______, colour = ______) -\ngeom_point(size = ______)\nggtitle(______)\n\n\n\n\n\nSolution. \nggplot(heightweight, aes(x = weightLb, y = heightIn, colour = sex)) + \n  geom_point(size = 2.5) +\n  ggtitle(\"Height vs. Weight by Sex\")\n\n\n\n\n\n\n4.8.2 Line Plot – geom_line()\nUse when: You want to show how a variable changes over time or another ordered variable (like day, year, index)\nWhat is shows: Trends, increases or decreases, cycles over time\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis plot shows how unemployment has changed over time.\n\n\n4.8.3 Barplot – geom_bar() and geom_col()\nUse when: You want to compare counts or values between categories.\n\ngeom_bar() counts how many times each category appears.\ngeom_col() shows values you provided directly.\n\nWhat it shows: Frequencies or values across different categories.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.4 Histogram – geom_histogram()\nUse when : You want to see the distribution of a numeric variable.\nWhat is shows : How values are spread across intervals (called “bins”).\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis plot shows many cars fall into different highway mpg ranges.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.5 Boxplot – geom_boxplot()\nUse when: You want to compare summary statistics (like median, quartiles, ouliers) across different groups.\nWhat it shows : Median, spread, and outliers for each group.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis plot compare highway mpg for different type of vehicles.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.6 Violin Plot – geom_violin()\nUse when: You want to see the distribution shape plus summary info for groups.\nWhat it shows: Like a boxplot, but with a mirrored density curve – great for showing the full distribution and comparison across groups.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can also add add points or boxplots inside the violins:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.7 Dot plot – geom_dotplot()\nUse when: You want to show individual values, especially for small datasets.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.8 Density Plot – geom_density()\nUse when: You want a smoothed version of a histogram.\nWhat is shows: Estimate of the data’s distribution, good for seeing shapes or peaks.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can combine the histogram and density plot to show exact counts (histogram) and smooth distribution (density) in one plot.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNoteWhat is after_stat(density)?\n\n\n\nIn ggplot2, when you use geom_histogram(), the default \\(y\\)-axis is count (how many observations fall in each bin). But if you write aes(y = after_stat(density)), you are asking ggplot2 to scale the height of the bars so they represent probability density instead of raw counts. This allows you to compare this histogram with a density curve (like the one from geom_density()), which also shows density.\n\n\nThis shows both frequency and smoothed distribution of highway mpg.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.9 Pie Chart (using Polar Coordinates)\nUse when: You want to show parts of a whole, but use with caution – bar plots are often easier to read.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.10 2D Density Plot\nA 2D density plot is used to show the distribution of two numeric variables. Instead of plotting each point (like in a scatter plot), it shows where points are concentrated using contour lines (geom_density_2d()) and filled contour areas (geom_density_2d_filled()).\nUse when: you have many overlapping points (overlapping) in a scatter plot and you want to highlight patterns or clusters in two numeric variables and see a smoother version of the joint distribution.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAdding points makes it easier to see what the contours represent.\n\nggplot(faithful, aes(x = waiting, y = eruptions)) +\n  # Add Data Points for Context\n  geom_point(alpha = 0.3) +\n  geom_density_2d()\n\n\n\n\n\n\n\n\nThe following code results in a plot that shows filled bands of density that it is great for visual impact.\n\nggplot(faithful, aes(x = eruptions, y = waiting)) + \n  geom_density_2d_filled(alpha = 0.6) \n\n\n\n\n\n\n\n\nCombining points and filled contours gives both precision and overview.\n\nggplot(faithful, aes(x = eruptions, y = waiting)) +\n  geom_point(alpha = 0.3) +\n  geom_density_2d_filled(alpha = 0.5) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nYou can map density levels to color using after_stat(level)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can also different code to produce the plot.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.11 2D Binned Plot for Big Data – stat_bin2d()\nUse when: You have a lot of points and scatter plots are too dense.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.12 Frequency Polygon – geom_freqpoly()\nA frequency plot is similar to a histogram, but it uses lines instead of bars to show how a variable is distributed. It is especially useful when you want to compare distributions across groups.\nUse when: You want to show the shape of a distribution and also compare multiple groups using lines (which may be easier to read than overlapping histograms).\nExamples\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.13 Correlation Plot (Cor plot)\nA correlation plot (or cor plot) is a visual way to show how strongly variables are related to each other. Correlation values range from \\(-1\\) to \\(1\\), and the plot helps you quickly see positive, negative, or no relationships.\nThis is not built into ggplot2, but we can use it together with\n\nThe cor() function to compute correlation\nThe corrplot or ggcorrplot package to visualize it.\n\nExample using ggcorrplot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nExample using corrplot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExercisesHints\n\n\nFor the following datasets (data description: DataDescriptions.pdf):\n\nCeoCompensation\nMassBodilyInjury\nRefrigerator\n\nPerform an exploratory data analysis:\n\nUnivariate analysis\nBivariate analysis\nOutlier analysis\n\nReport any conclusions using appropriate descriptive statistics and graphical visualizations.\n\n\n\nConsider the distribution of each variable in the univariate analysis.\nIn the bivariate analysis, look for relationships between pairs of variables.\nFor outlier analysis, identify any unusual observations that may affect your results. scatter plots to visualize potential outliers.\n\n\n\n\n\n\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pederson. 2019. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/project.html",
    "href": "chapters/project.html",
    "title": "5  Project",
    "section": "",
    "text": "5.1 Exercise 1\nPlease copy this R code into your computer and try to solve it.\ndownload.file(\n  url = \"https://raw.githubusercontent.com/mnrzrad/SIE/main/exercises/Exercise_EDA.Rmd\",\n  destfile = \"Exercise_EDA.Rmd\",\n  method = \"curl\"\n)\nThis code saves the file into your current working directory.\nAfter, chech where the file was saved using the code\ngetwd()\nThis code tells you the working directory (the folder where R is looking and saving files).\nThen, you can list files in that folder to confirm.\nlist.files()\nNow, open it in R studio.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project</span>"
    ]
  },
  {
    "objectID": "chapters/project.html#exercise-1",
    "href": "chapters/project.html#exercise-1",
    "title": "5  Project",
    "section": "",
    "text": "In RStudio, go to File → Open File\nNavigate to the folder shown by getwd()\nSelect Exercise_EDA.Rmd → Open",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5_logistic_regression.html",
    "href": "chapters/chapter5_logistic_regression.html",
    "title": "6  Logistic Regression",
    "section": "",
    "text": "6.1 Modelling Probabilities\nA binary classification problem is one where the outcome variable can take on only two possible values. For example:\nFor this kind of problem, the dataset used to fit the model is composed of observations \\((\\mathbf{x}_1, y_1)\\), \\((\\mathbf{x}_2, y_2)\\), …, \\((\\mathbf{x}_n, y_n)\\) where each \\(y_i\\) is usually labelled 0 (failure, no event, …) or 1 (success, event, …) and \\(\\mathbf{x}_i\\) is a vector of input variables.\nThe task at hand is to build a model that, given an arbitrary \\(\\mathbf{x}\\), will predict the probability that \\(y = 1\\).\nWhy not use a linear regression model to predict the probability that \\(y = 1\\)? After all, linear regression is a well-known and widely used method.\nThere are some problems with this approach:",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5_logistic_regression.html#modelling-probabilities",
    "href": "chapters/chapter5_logistic_regression.html#modelling-probabilities",
    "title": "6  Logistic Regression",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\nthe output values are never (with probability 1) exactly 0 or 1, the values that are taken by \\(y\\);\neven considering a threshold (e.g. 0.5) to classify the predicted values into 0 or 1, this would be mostly an arbitrary chosen value.",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5_logistic_regression.html#sigmoid-functions",
    "href": "chapters/chapter5_logistic_regression.html#sigmoid-functions",
    "title": "6  Logistic Regression",
    "section": "6.2 Sigmoid Functions",
    "text": "6.2 Sigmoid Functions\nInstead of a linear function, we can opt to model the probability of success of the event described by \\(y\\), i.e \\(\\operatorname{P}(Y=1)\\). This is a more sound and natural way to model the prediction of a random binary event. In order to do this, we can use sigmoid functions. A sigmoid function \\(f(x)\\) is a function having an “S” shaped curve (sigmoid curve) with the following properties:\n\n\\(0&lt;f(x)&lt;1\\);\n\\(\\displaystyle \\lim_{x \\to -\\infty} f(x) = 0\\), \\(\\displaystyle \\lim_{x \\to +\\infty} f(x) = 1\\);\n\\(f'(x)&gt;0\\).\n\nA common sigmoid function is the logistic function, defined as:\n\\[\nf(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}}.\n\\]\nA logistic function can be used to model the probability that \\(y = 1\\) given \\(\\mathbf{x}\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis function will play a central row in the logistic regression model, as we will see briefly.",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5_logistic_regression.html#logistic-model",
    "href": "chapters/chapter5_logistic_regression.html#logistic-model",
    "title": "6  Logistic Regression",
    "section": "6.3 Logistic Model",
    "text": "6.3 Logistic Model\nThe first random variable that comes to mind in order to model a random binary event is the Bernoulli random variable, which is a discrete random variable that takes value 1 with probability \\(p\\) and value 0 with probability \\(1-p\\). The parameter \\(p\\) is the probability of success of the event described by the random variable, and it’s probability function is\n\\[\np(y) = p^y (1-p)^{1-y}, \\quad y \\in \\{0, 1\\}.\n\\] The expected value of a Bernoulli random variable is \\(\\operatorname{E}[Y] = p\\) and it’s variance is \\(\\operatorname{V}(Y) = p(1-p)\\).\n\n\n\n\n\n\nNote\n\n\n\nA natural extension is the Binomial random variable, which is the sum of \\(n\\) independent Bernoulli random variables with the same parameter \\(p\\). The parameter \\(p\\) is again the probability of success of the event described by the random variable, and it’s probability function is\n\\[\np(y) = \\binom{n}{y} p^y (1-p)^{n-y}, \\quad y \\in \\{0, 1, ..., n\\}.\n\\]\n\n\nAnother way of expressing the probability of an event is through the odds of the event, defined as the ratio between the probability of the event and the probability of it’s complementary event:\n\\[\n\\operatorname{odds} = \\frac{p}{1-p} = \\frac{1}{1-p^{-1}}, \\quad p \\in ]0, 1[.\n\\]\nThe odds are a number between 0 and \\(+\\infty\\). If the odds are equal to 1, then the event and it’s complementary are equally likely. If the odds are greater than 1, then the event is more likely than it’s complementary event, and vice versa. It’s easy to see that odds are monotonically increasing with respect to \\(p\\):\n\\[\n{(\\operatorname{odds})}' = \\frac{1}{(1-p)^2} &gt; 0.\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOdds are an alternative way of expressing probabilities, still being relatively intuitive concerning interpretation. Nevertheless, they are still confined to the interval \\(]0, +\\infty[\\), because\n\\[\n\\lim_{p \\to 0^+} \\frac{1}{1-p^{-1}} = 0, \\quad \\lim_{p \\to 1^-} \\frac{1}{1-p^{-1}} = +\\infty.\n\\]\nIf we want to have a represantation of probabilities that spans the whole real line, we can use the log-odds, also called logit function, defined as\n\\[\n\\operatorname{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right), \\quad p \\in ]0, 1[.\n\\]\nThis creates a one-to-one mapping between the interval \\(]0, 1[\\) and the whole real line \\(\\mathbb{R}\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe logit function is also monotonically increasing with respect to \\(p\\):\n\\[\n{\\big(\\operatorname{logit}(p)\\big)}' = \\frac{1}{p(1-p)} &gt; 0.\n\\]\nThis representation of probabilities is well suited to be used in a regression model, because it spans the whole real line. In fact, in a logistic regression model, we assume that the log-odds of the probability that \\(y = 1\\) is a linear combination of the input variables: \\[\n\\operatorname{logit}\\big(\\operatorname{P}(Y=1|\\mathbf{x})\\big) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p = \\mathbf{x}^T \\boldsymbol{\\beta} = \\eta (\\mathbf{x}),\n\\] where \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, ..., \\beta_p)^T\\) is the vector of parameters to be estimated.\nNote that the logit function is the inverse of the logistic function! Hence, we can rewrite \\(\\operatorname{P}(Y=1|\\mathbf{X}=\\mathbf{x})\\) as\n\\[\n\\operatorname{P}(Y=1|\\mathbf{x}) = \\frac{e^{\\eta(\\mathbf{x})}}{1 + e^{\\eta(\\mathbf{x})}} = \\frac{1}{1 + e^{-\\eta(\\mathbf{x})}}.\n\\tag{6.1}\\]\n\n\n\n\n\n\nWarning\n\n\n\nEquation 6.1 describes a model for the expected value of the random variable \\(Y\\) given \\(\\mathbf{x}\\), that follows a Bernoulli distribution with parameter \\(p = \\operatorname{P}(Y=1|\\mathbf{x})\\). It does not describe a model for \\(Y\\) itself, which is a discrete random variable taking values 0 or 1.",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1_data_ethics.html",
    "href": "chapters/chapter1_data_ethics.html",
    "title": "7  Data Ethics",
    "section": "",
    "text": "7.1 What is the Ethics of Data Science?\nThe ethics of data science are very important, and all data analyst, data scientists, and everyone who works with data should know the principles of ethics.\nAll individuals handling data are obligated to report any occurrences such as data theft, improper storage, unethical data collection, or data misuse.\nFor example, a business might track and store information about a customer’s path, starting from the moment they enter their email address on the website until they purchase products. In such cases, it is essential that the individual’s report remains confidential and is safeguarded it from unauthorized access.\nThe study and evaluation of ethical issues related to data have led to the emergence of a new domain in ethics, known as “data science ethics.” Data can be collected, recorded, generated, processed, shared, and used. This field also covers various data and technologies, including programming, professional code, and algorithms.\nIn the past, ethical principles in computer and information science primarily focused on the content and information present within computer systems. However, with the advancement of technology and the increasing volume of data, the scope of data science ethics has expanded to include concepts and principles related to data collection, use, processing, and sharing.\nWhen companies collect data from individuals, ethical issues related to privacy, personal information, and data misuse arise. However, when companies begin to use these data for purposes not initially specified, even more ethical concerns emerge. In other words, companies try to monetize the collected data, using it in ways that were not previously disclosed. This practice further challenges privacy, trust, and ethics in data collection and use.\nAlthough “ethics” seems complicated, it is really about understanding what is right or wrong. There are different ways people think about ethics, especially when it comes to data world.",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Ethics</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1_data_ethics.html#what-is-the-ethics-of-data-science",
    "href": "chapters/chapter1_data_ethics.html#what-is-the-ethics-of-data-science",
    "title": "7  Data Ethics",
    "section": "",
    "text": "Important\n\n\n\nData Ethics covers the moral and ethical obligations related to the collection, sharing, and use of data, focusing on ensuring its fair and beneficial applications. It is mainly focused on any negative impacts that data might have on individuals, groups or wider society.\n\n\n\n7.1.1 Principles of Data Ethics for Business Professionals\nIn an era where data is considered the new oil, understanding the ethical issues related to its collection, analysis, and use is of most important. Below, we will introduce the five key ethical principles that every data professional should be aware of.\n\nOwnership: The first principle of data ethics is that every individual has the right to ownership over their personal information. Just as it is considered theft to take an object without its owner’s permission, collecting someone’s personal information without their consent is both illegal and unethical. Some common methods for obtaining consent include signed written agreements, digital privacy policies that require users to agree to a company’s terms and conditions, and pop-up windows with check-boxes that allow websites to track users’ online behavior with cookies. *Never assume customers agree to their information being collected; always seek their permission to avoid ethical and legal problems.\nTransparency: In addition to ownership, individuals whose data you collect have the right to know how you plan to gather, store, and use their personal information. When collecting data, prioritize transparency and provide them with all necessary information.\nFor example, imagine your company decides to implement an algorithm to personalize the website experience based on user shopping habits and behavior. You should develop a policy explaining that cookies will track user behavior, the collected data will be stored in a secure database, and an algorithm will be trained to personalize the website experience.\nUsers have the right to access this information so they can decide whether to accept or reject your website’s cookies. Hiding or lying about your company’s methods and goals is deceptive, illegal, and unfair to the individuals whose data you handle.\nPrivacy: One of the ethical responsibilities associated with handling data is preserving the privacy of individuals whose data is involved. Even if a customer has given your company permission to collect, store, and analyze their Personally Identifiable Information (PII), it does not mean they want this information to be publicly accessible.\nPII is any data that relates to an individual’s identity. This typically includes items such as full name, address, phone number, date of birth, credit card number, bank account number, social security number, passport number, and other information that can be used to identify a person.\nTo protect individuals’ privacy, ensure you store data in a secure database to prevent it from falling into the wrong hands. Data security methods that help maintain privacy include file encryption (transforming information into a readable format only with an encryption key) and dual-authentication (password protection with two-factor authentication).\nOne way to prevent errors is to de-identify the dataset. When all PII is removed, only anonymous data remains, and the dataset can no longer be traced back to individuals. This allows analysis to identify relationships between variables of interest without linking data points to personal identities.\nIntention : Before you even begin collecting data, it is crucial to consider why you need this data and what purpose you aim to achieve with it. If your intention is to engage in unethical activities, such as harassing others or improperly exploiting individuals’ vulnerabilities, then collecting their data is not ethical. Ethical data collection requires a legitimate purpose and operating in accordance with regulations and ethical principles.\nEven when your intention is good - for instance, collecting data to understand health experience in order to create an app that addresses urgent needs – you still need to evaluate your purpose for collecting every piece of data.\nOutcomes : Even if your intention and purpose in collecting and analyzing data are good, the outcome of this process might unintentionally cause harm to individuals or groups. This type of harm is known as disparate impact and is considered invalid under civil rights laws, meaning it is illegal.\nUnfortunately, you cannot definitively predict the exact impact of data analysis until it is complete. However, by considering this question and its importance before conducting the analysis, you can identify any potential disparate impacts that might occur. By being aware if the possibility of disparate impact, you can then take the necessary steps to prevent and mitigate it.",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Ethics</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1_data_ethics.html#why-is-ethics-important-in-the-age-of-data-abundance",
    "href": "chapters/chapter1_data_ethics.html#why-is-ethics-important-in-the-age-of-data-abundance",
    "title": "7  Data Ethics",
    "section": "7.2 Why is Ethics Important in the Age of Data Abundance?",
    "text": "7.2 Why is Ethics Important in the Age of Data Abundance?\nUsing information ethically within decision-making has always been important. However, two factors have made data ethics business-critical:\n\nData Volumes : There has been an explosion in the amount of data available to organizations, both collected themselves, ans sourced from third-parties. It is not always clear where this information has come from, particularly in the case of personal information, and what permissions have been provided for its reuse.\nArtificial Intelligence : Organizations are increasing using machine learning and artificial intelligence algorithms to make sense of data and take automated decisions based on data analysis without involving human oversights. This can lead to issues around fairness and discrimination, even if these are unintended consequences of how data is used.\n\nThe importance of ethics in data science stems from the necessity of having a clear set of rules and guidelines that determine what businesses can and cannot do with the personal information they collect from their customers. This is crucial for safeguarding customer privacy and rights, and appropriate laws and standards must be established to protect personal information.\nAll experts agree that certain fundamental principles must be implemented, even if this field still contains gray areas and nothing is simple or uniform. These are just a few important topics and strategic ideas that have currently garnered the most attention. However, there is still much ground to cover in data ethics, and further progress and development are needed in this area.\n\n\n\n\n\n\nNote Exercise 1\n\n\n\nIdentify the data ethics principle related to each question:\n\nIs it clear what data is being used for, how and where it is being stored, and is this information freely available to all?\nAre you collecting only data that is necessary and relevant to your clearly defined goals?\nHave you anticipated any potential harmful, negative, or biased impacts that could result from your use of the data?\nIs any personal or identifiable data being securely stored and anonymized to prevent unauthorized access?\nHave you obtained informed consent from individuals for how their data will be used in your project?\n\n\n\n\n\n\n\nTip Answers\n\n\n\n\n\n\nTransparency\nIntention\nOutcomes\nPrivacy\nOwnership\n\n\n\n\n\n\n\n\n\n\n\n\nNote Exercise 2\n\n\n\nIdentify the data ethics principle related to each question:\n\nA company collects location data from users’ mobile phones but does not disclose that this information will be shared with advertisers.\nAn analyst stores survey responses on an unsecured shared drive, and the file includes names and phone numbers of participants.\nA data science team begins using a health dataset for a study unrelated to the one participants originally agreed to take part in.\nBefore collecting any data, a team carefully evaluates whether the data they plan to gather is essential to their project goals and excludes anything not directly needed.\nBefore launching a new app feature based on user behavior data, a team conducts a risk assessment to explore how it might negatively affect vulnerable users.\n\n\n\n\n\n\n\nTip Answers\n\n\n\n\n\n\nTransparency\nPrivacy\nOwnership\nIntention\nOutcomes",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Ethics</span>"
    ]
  }
]