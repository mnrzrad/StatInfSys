[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics and Information Systems",
    "section": "",
    "text": "Prerequisites\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "chapters/chapter1_data_ethics.html",
    "href": "chapters/chapter1_data_ethics.html",
    "title": "1  Data Ethics",
    "section": "",
    "text": "1.1 What is the Ethics of Data Science?\nThe ethics of data science are very important, and all data analyst, data scientists, and everyone who works with data should know the principles of ethics.\nAll individuals handling data are obligated to report any occurrences such as data theft, improper storage, unethical data collection, or data misuse.\nFor example, a business might track and store information about a customer’s path, starting from the moment they enter their email address on the website until they purchase products. In such cases, it is essential that the individual’s report remains confidential and is safeguarded it from unauthorized access.\nThe study and evaluation of ethical issues related to data have led to the emergence of a new domain in ethics, known as “data science ethics.” Data can be collected, recorded, generated, processed, shared, and used. This field also covers various data and technologies, including programming, professional code, and algorithms.\nIn the past, ethical principles in computer and information science primarily focused on the content and information present within computer systems. However, with the advancement of technology and the increasing volume of data, the scope of data science ethics has expanded to include concepts and principles related to data collection, use, processing, and sharing.\nWhen companies collect data from individuals, ethical issues related to privacy, personal information, and data misuse arise. However, when companies begin to use these data for purposes not initially specified, even more ethical concerns emerge. In other words, companies try to monetize the collected data, using it in ways that were not previously disclosed. This practice further challenges privacy, trust, and ethics in data collection and use.\nAlthough “ethics” seems complicated, it is really about understanding what is right or wrong. There are different ways people think about ethics, especially when it comes to data world.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Ethics</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1_data_ethics.html#what-is-the-ethics-of-data-science",
    "href": "chapters/chapter1_data_ethics.html#what-is-the-ethics-of-data-science",
    "title": "1  Data Ethics",
    "section": "",
    "text": "Important\n\n\n\nData Ethics covers the moral and ethical obligations related to the collection, sharing, and use of data, focusing on ensuring its fair and beneficial applications. It is mainly focused on any negative impacts that data might have on individuals, groups or wider society.\n\n\n\n1.1.1 Principles of Data Ethics for Business Professionals\nIn an era where data is considered the new oil, understanding the ethical issues related to its collection, analysis, and use is of most important. Below, we will introduce the five key ethical principles that every data professional should be aware of.\n\nOwnership: The first principle of data ethics is that every individual has the right to ownership over their personal information. Just as it is considered theft to take an object without its owner’s permission, collecting someone’s personal information without their consent is both illegal and unethical. Some common methods for obtaining consent include signed written agreements, digital privacy policies that require users to agree to a company’s terms and conditions, and pop-up windows with check-boxes that allow websites to track users’ online behavior with cookies. *Never assume customers agree to their information being collected; always seek their permission to avoid ethical and legal problems.\nTransparency: In addition to ownership, individuals whose data you collect have the right to know how you plan to gather, store, and use their personal information. When collecting data, prioritize transparency and provide them with all necessary information.\nFor example, imagine your company decides to implement an algorithm to personalize the website experience based on user shopping habits and behavior. You should develop a policy explaining that cookies will track user behavior, the collected data will be stored in a secure database, and an algorithm will be trained to personalize the website experience.\nUsers have the right to access this information so they can decide whether to accept or reject your website’s cookies. Hiding or lying about your company’s methods and goals is deceptive, illegal, and unfair to the individuals whose data you handle.\nPrivacy: One of the ethical responsibilities associated with handling data is preserving the privacy of individuals whose data is involved. Even if a customer has given your company permission to collect, store, and analyze their Personally Identifiable Information (PII), it does not mean they want this information to be publicly accessible.\nPII is any data that relates to an individual’s identity. This typically includes items such as full name, address, phone number, date of birth, credit card number, bank account number, social security number, passport number, and other information that can be used to identify a person.\nTo protect individuals’ privacy, ensure you store data in a secure database to prevent it from falling into the wrong hands. Data security methods that help maintain privacy include file encryption (transforming information into a readable format only with an encryption key) and dual-authentication (password protection with two-factor authentication).\nOne way to prevent errors is to de-identify the dataset. When all PII is removed, only anonymous data remains, and the dataset can no longer be traced back to individuals. This allows analysis to identify relationships between variables of interest without linking data points to personal identities.\nIntention : Before you even begin collecting data, it is crucial to consider why you need this data and what purpose you aim to achieve with it. If your intention is to engage in unethical activities, such as harassing others or improperly exploiting individuals’ vulnerabilities, then collecting their data is not ethical. Ethical data collection requires a legitimate purpose and operating in accordance with regulations and ethical principles.\nEven when your intention is good - for instance, collecting data to understand health experience in order to create an app that addresses urgent needs – you still need to evaluate your purpose for collecting every piece of data.\nOutcomes : Even if your intention and purpose in collecting and analyzing data are good, the outcome of this process might unintentionally cause harm to individuals or groups. This type of harm is known as disparate impact and is considered invalid under civil rights laws, meaning it is illegal.\nUnfortunately, you cannot definitively predict the exact impact of data analysis until it is complete. However, by considering this question and its importance before conducting the analysis, you can identify any potential disparate impacts that might occur. By being aware if the possibility of disparate impact, you can then take the necessary steps to prevent and mitigate it.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Ethics</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1_data_ethics.html#why-is-ethics-important-in-the-age-of-data-abundance",
    "href": "chapters/chapter1_data_ethics.html#why-is-ethics-important-in-the-age-of-data-abundance",
    "title": "1  Data Ethics",
    "section": "1.2 Why is Ethics Important in the Age of Data Abundance?",
    "text": "1.2 Why is Ethics Important in the Age of Data Abundance?\nUsing information ethically within decision-making has always been important. However, two factors have made data ethics business-critical:\n\nData Volumes : There has been an explosion in the amount of data available to organizations, both collected themselves, ans sourced from third-parties. It is not always clear where this information has come from, particularly in the case of personal information, and what permissions have been provided for its reuse.\nArtificial Intelligence : Organizations are increasing using machine learning and artificial intelligence algorithms to make sense of data and take automated decisions based on data analysis without involving human oversights. This can lead to issues around fairness and discrimination, even if these are unintended consequences of how data is used.\n\nThe importance of ethics in data science stems from the necessity of having a clear set of rules and guidelines that determine what businesses can and cannot do with the personal information they collect from their customers. This is crucial for safeguarding customer privacy and rights, and appropriate laws and standards must be established to protect personal information.\nAll experts agree that certain fundamental principles must be implemented, even if this field still contains gray areas and nothing is simple or uniform. These are just a few important topics and strategic ideas that have currently garnered the most attention. However, there is still much ground to cover in data ethics, and further progress and development are needed in this area.\n\n\n\n\n\n\n Exercise 1\n\n\n\nIdentify the data ethics principle related to each question:\n\nIs it clear what data is being used for, how and where it is being stored, and is this information freely available to all?\nAre you collecting only data that is necessary and relevant to your clearly defined goals?\nHave you anticipated any potential harmful, negative, or biased impacts that could result from your use of the data?\nIs any personal or identifiable data being securely stored and anonymized to prevent unauthorized access?\nHave you obtained informed consent from individuals for how their data will be used in your project?\n\n\n\n\n\n\n\n Answers\n\n\n\n\n\n\nTransparency\nIntention\nOutcomes\nPrivacy\nOwnership\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise 2\n\n\n\nIdentify the data ethics principle related to each question:\n\nA company collects location data from users’ mobile phones but does not disclose that this information will be shared with advertisers.\nAn analyst stores survey responses on an unsecured shared drive, and the file includes names and phone numbers of participants.\nA data science team begins using a health dataset for a study unrelated to the one participants originally agreed to take part in.\nBefore collecting any data, a team carefully evaluates whether the data they plan to gather is essential to their project goals and excludes anything not directly needed.\nBefore launching a new app feature based on user behavior data, a team conducts a risk assessment to explore how it might negatively affect vulnerable users.\n\n\n\n\n\n\n\n Answers\n\n\n\n\n\n\nTransparency\nPrivacy\nOwnership\nIntention\nOutcomes",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Data Ethics</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html",
    "href": "chapters/chapter3_dplyr.html",
    "title": "2  Getting Started with dplyr",
    "section": "",
    "text": "2.1 Filter observations - filter()\nThe dplyr package is a part of the R tidyverse : an ecosystem of several libraries designed to work together by representing data in common formats.\nThe data frame is a key data structure in statistics and in R. The basic structure of a data frame is that there is one observation per ro and each column represents a variable, a measure, feature, or characteristic of that observation. Given the importance of managing data frames, it is important that we have good tools for dealing with them.\nThe dplyr package is a relatively new R package that allows you to do all kinds of analyses quickly and easily.\nOne important contribution of the dplyr package is that it provides a “grammar” (in particular, verbs) for data manipulating and for operating on data frames. With this grammar, you can sensibly communicate what it is that you are doing to a data frame that other people can understand (assuming they also know the grammar). This is useful because it provides an abstraction for data manipulation that previously did not exist.\nSome of the key “verbs” provided by the dplyr package are\nTo install the dplyr package from CRAN, just run\nAfter installing the package, it is important to load it into the R session.\nTo better understanding, let us continue within analyzing a dataset, penguins, available within the palmerpenguins package. we need to load the dataset and if it is necessary, we need to install the package.\n‌The data frame contain data for \\(344\\) penguins and \\(8\\) variables describing the species (species), the island (island), some measurements of the size of the bill (bill_length_mm and bill_depth_mm), flipper (flipper_length_mm) and body mass (body_mass_g), the sex (sex) and the study year (year). More information about the data frame can be found by running ?penguins .\nYou can see some basic characteristics of the dataset with the dim() and str() functions.\nThe first and last 6 rows can be displayed by head() and tail(), respectively.\nThe summary information of data frame can be found by summary() .\nThis function works on both quantitative and qualitative variables.\nYou can combine multiple conditions using & if all conditions must be true (cumulative), or | if at least one condition must be true (alternative). For example,\nAs you can see, the filter() functions require the name of the data frames as the first argument, then the condition (with the usual logical operators &gt;, &lt;, &gt;=, &lt;=, ==, !=, %in%, etc.) as second argument.\nSo with the pipe operator, the code above becomes:\nInstead of listing the data frame’s name as the initial argument within functions like filter() (or other {dplyr} functions), you simply specify the data frame once, then use the pipe operator to connect it to the desired function.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#filter-observations---filter",
    "href": "chapters/chapter3_dplyr.html#filter-observations---filter",
    "title": "2  Getting Started with dplyr",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nVariable names should be used directly, without enclosing them in single or double quotation marks (' or \").\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nTo use any functions in the dplyr package, you must specify the data frame’s name as the first argument. Alternatively, you can use the pipe operator (|&gt; or %&gt;% ) to avoid explicitly naming the data frame within each function.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe keyboard shortcut for the pipe operator is ctrl+shift+M on Windows or cmd + shift + M on Mac. By default, this will produce %&gt;%, but if you have configured RStudio to use the native pipe operator, it will print |&gt; .\n\n\n\n\n\n\nthe %&gt;% pipe, originating from the magrittr package (included in the tidyverse), was superseded in R 4.1.0 (released in 2021) by the native |&gt; pipe. We recommend |&gt; because it’s a simpler, built-in feature of base R, always available without needing additional package loads.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe pipe operator lets you chain multiple operations together, which is especially handy for performing several calculations on a data frame without saving the result of each intermediate step.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nThe pipe operator streamlines your code by feeding the output of one operation directly into the next, making your code much easier to write and read.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#extract-observations",
    "href": "chapters/chapter3_dplyr.html#extract-observations",
    "title": "2  Getting Started with dplyr",
    "section": "2.2 Extract observations",
    "text": "2.2 Extract observations\nYou can extract observations from a dataset based on either their positions or their values.\n\n2.2.1 Based on Their Positions\nTo extract observations based on their positions, you can use the slice() function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFurthermore, for extracting specific rows like the first or last, you can use specialized functions:\n\nslice_head(): Extracts rows from the beginning of the dataset.\nslice_tail(): Extracts rows from the end of the dataset.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n2.2.2 Based on their values\nWhen you need to extract observations based on the values of a variable, you can use:\n\nslice_min(): Selects rows with the lowest values, allowing you to define a specific proportion.\nslice_max(): Selects rows with the highest values, also with the option to define a proportion.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#sample-observations",
    "href": "chapters/chapter3_dplyr.html#sample-observations",
    "title": "2  Getting Started with dplyr",
    "section": "2.3 Sample Observations",
    "text": "2.3 Sample Observations\nSampling observations can be achieved in two ways:\n\nsample_n(): Takes a random sample of a specified number of rows.\nsample_frac(): Takes a random sample of a specified fraction of rows.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nIt is important to note that, similar to the base R sample() function, the size argument can exceed the total number of rows in your data frame. If this happens, some rows will be duplicated, and you will need to explicitly set the argument replace = TRUE.\n\n\nAlternatively, you can obtain a random sample (either a specific number or a fraction of rows) using slice_sample(). For this, you use:\n\nThe argument n to select a specific number of rows.\nThe argument prop to select a fraction of rows.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#sort-observations",
    "href": "chapters/chapter3_dplyr.html#sort-observations",
    "title": "2  Getting Started with dplyr",
    "section": "2.4 Sort observations",
    "text": "2.4 Sort observations\nObservations can be sorted using the arrange() function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBy default, arrange() sorts in ascending order. To sort in descending order, simply use desc() within the arrange() function, like arrange(desc(variable_name)).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSimilar to filter(), arrange() can sort by multiple variables and works with both quantitative (numerical) and qualitative (categorical) variables. For example, arrange(sex, body_mass) would first sort by sex (alphabetical order) and then by body_mass (ascending, from lowest to highest).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to note that if a qualitative variable is defined as an ordered factor, the sorting will follow its defined level order, not alphabetical order.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#select-variables",
    "href": "chapters/chapter3_dplyr.html#select-variables",
    "title": "2  Getting Started with dplyr",
    "section": "2.5 Select variables",
    "text": "2.5 Select variables\nYou can select variables using the select() function based on their position or name.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo remove variables, use a - sign before their position or name.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can also select a sequence of variables by name (e.g., select(df, var1:var5)).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFurthermore, select() provides a straightforward way to rearrange column order in your data frame.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n2.5.1 Select using helper functions\nThe select() function also supports helper functions for matching column names based on patterns:\nstarts_with(\"abc\") selects all columns whose names begin with the specified string.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nends_with(\"xyz\") selects all columns whose names end with the specified string.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ncontains(\"ijk\") selects all columns that contain the specified substring anywhere in their name.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe everything() helper selects all remaining columns that have not been explicitly mentioned. It is useful when you want to: move some variables to the front, or keep all others in their existing order.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nnum_range(\"prefix\", 1:3) selects variables with names like \"prefix1\", \"prefix2\", \"prefix3\", etc. This is useful for selecting numbered variables.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nmatches(\"(.)\\\\1\") selects columns whose names match a regular expression (regex).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis example selects aa and bb but not ab, since the pattern (.)\\\\1 means “a character repeated twice”.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#renaming-variables",
    "href": "chapters/chapter3_dplyr.html#renaming-variables",
    "title": "2  Getting Started with dplyr",
    "section": "2.6 Renaming Variables",
    "text": "2.6 Renaming Variables\nTo rename variables, use the rename() function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nRemember the syntax: new_name = old_name. This means you always write the desired new name first, followed by an equals sign, and then the current old name of the variable.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#create-or-modify-variables",
    "href": "chapters/chapter3_dplyr.html#create-or-modify-variables",
    "title": "2  Getting Started with dplyr",
    "section": "2.7 Create or Modify Variables",
    "text": "2.7 Create or Modify Variables\nThe mutate() function allows you to create new variables or modify existing ones. You can base these operations on another existing variable or a vector of your choice.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nIf you create a variable with a name that already exists, the old variable will be overwritten.\n\n\nSimilar to rename(), mutate() requires the argument to be in the format name = expression, where name is the column being created or modified, and expression is the formula for its values.\nThere is another function transmute() that is also used to create a new variable in a data frame by transforming existing ones. However, unlike mutate(), which keeps all original columns and simply adds the new ones, transmute() returns only the newly created variables. This means that when you use transmute(), the resulting data frame will include just the variables you explicitly define inside the function. It is particularly useful when you want a clean output focused only on the transformed results, without retaining the original dataset’s columns. In contrast, mutate() is ideal when you want to preserve the full structure of your data while adding new insights.\nLet us create a new column (body mass in kg).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#summarize-observations",
    "href": "chapters/chapter3_dplyr.html#summarize-observations",
    "title": "2  Getting Started with dplyr",
    "section": "2.8 Summarize Observations",
    "text": "2.8 Summarize Observations\nTo get descriptive statistics of your data, use the summarize() (or summarise()) function in conjunction with statistical functions like mean(), median(), min(), max(), sd(), var(), etc.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nRemember to use na.rm = TRUE to exclude missing values from calculations.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#identify-distinct-values",
    "href": "chapters/chapter3_dplyr.html#identify-distinct-values",
    "title": "2  Getting Started with dplyr",
    "section": "2.9 Identify Distinct Values",
    "text": "2.9 Identify Distinct Values\nThe distinct() function helps you find unique values within a variable.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhile typically used for qualitative or quantitative discrete variables, it works for any variable type and can identify unique combinations of values when multiple variables are specified. For instance, distinct(species, study_year) would return all unique combinations of species and study year.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#group-by",
    "href": "chapters/chapter3_dplyr.html#group-by",
    "title": "2  Getting Started with dplyr",
    "section": "2.10 Group By",
    "text": "2.10 Group By\nThe group_by() function changes how subsequent operations are performed. Instead of applying functions to the entire data frame, operations will be applied to each defined group of rows. This is particularly useful with summarize(), as it will produce statistics for each group rather than for all observations.\nFor example, to calculate the mean and standard deviation of body_mass separately for each species, you would first group_by(species) and then summarize() the body_mass. The pipe operator smoothly passes the grouped data from group_by() to summarize().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can also group by multiple variables (e.g., group_by(var1, var2)), and the data frame’s name only needs to be specified in the very first operation of a chained sequence.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#managing-groups-ungroup",
    "href": "chapters/chapter3_dplyr.html#managing-groups-ungroup",
    "title": "2  Getting Started with dplyr",
    "section": "2.11 Managing Groups: ungroup()",
    "text": "2.11 Managing Groups: ungroup()\nAfter performing operations on grouped data, the ungroup() function allows you to revert to a normal data frame, enabling operations on entire columns again or switching to new grouping criteria. Remember, you can also group_by() multiple columns simultaneously.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#number-of-observations",
    "href": "chapters/chapter3_dplyr.html#number-of-observations",
    "title": "2  Getting Started with dplyr",
    "section": "2.12 Number of Observations",
    "text": "2.12 Number of Observations\nThe function n() returns the number of observations. It can only be used inside summarize().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhen combined with group_by(), you can easily get the number of observations per group. n() takes no parameters, so it’s always written as n().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNotably, the count() function is a convenient shortcut, equivalent to summarize(n = n()).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#number-of-distinct-values",
    "href": "chapters/chapter3_dplyr.html#number-of-distinct-values",
    "title": "2  Getting Started with dplyr",
    "section": "2.13 Number of Distinct Values",
    "text": "2.13 Number of Distinct Values\nTo count the number of unique values or levels in a variable (or combination of variables), use n_distinct(). Like n(), it’s exclusively used within summarize().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou do not have to explicitly name the output; the operation’s name will be used by default (e.g., summarize(n_distinct(variable))).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#first-last-or-nth-value",
    "href": "chapters/chapter3_dplyr.html#first-last-or-nth-value",
    "title": "2  Getting Started with dplyr",
    "section": "2.14 First, Last, or \\(n\\)th Value",
    "text": "2.14 First, Last, or \\(n\\)th Value\nAlso available only within summarize(), you can retrieve the first, last, or nth value of a variable. Functions like first(), last(), and nth() enable this.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThese functions offer arguments to handle missing values; for more details, consult their documentation (e.g., ?nth()).",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#conditional-transformations",
    "href": "chapters/chapter3_dplyr.html#conditional-transformations",
    "title": "2  Getting Started with dplyr",
    "section": "2.15 Conditional Transformations",
    "text": "2.15 Conditional Transformations\n\n2.15.1 If Else\nThe if_else() function (used with mutate()) is ideal for creating a new variable with two levels based on a condition. It takes three arguments:\n\nThe condition (e.g., body_mass_g &gt;= 4000).\nThe output value if the condition is TRUE (e.g., “High”).\nThe output value if the condition is FALSE (e.g., “Low”).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nTip\n\n\n\nA key advantage is that if_else() propagates missing values (NA) if the condition’s input is missing, preventing misclassification.\n\n\n\n\n2.15.2 Case When\nFor categorizing a variable into more than two levels, case_when() is far more appropriate and readable than nested if_else() statements.\nWhile nested if_else() functions can technically work, they are prone to errors and result in difficult-to-read code. For instance, to classify body mass into “Low” (&lt;3500), “High” (&gt;4750), and “Medium” (otherwise), nested if_else() would look like this:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis code first checks if body_mass_g is less than 3500. If true, it assigns “Low”. If false, it then checks if body_mass_g is greater than 4750. If that’s true, it assigns “High”; otherwise, it assigns “Medium”. While functional, this structure can become complex and error-prone with more conditions.\ncase_when() evaluates conditions sequentially. To improve this workflow, we now use the case when technique:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis workflow is much simpler to code and read!\nIf there are no missing values in the variable(s) used for the condition(s), it can even be simplified to:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhile a .default argument can be used to specify an output for observations not matching any condition, exercise caution with missing values. If NA values in the conditioning variable are not explicitly handled, they might be incorrectly assigned to the default category. A safer approach is to explicitly define all categories or ensure NAs remain NA.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nRegardless of whether you use if_else() or case_when(), it’s always good practice to verify the newly created variable to ensure it aligns with your intended results.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#exploring-further-dplyr-functions",
    "href": "chapters/chapter3_dplyr.html#exploring-further-dplyr-functions",
    "title": "2  Getting Started with dplyr",
    "section": "2.16 Exploring Further dplyr Functions",
    "text": "2.16 Exploring Further dplyr Functions\nUntil now, we have focused on analyzing the penguins dataset. To effectively explain some of dplyr’s other powerful functions, we’ll now shift to creating custom datasets tailored to demonstrate their specific functionalities.\n\n2.16.1 Separate and Unite\nYou can separate a character column into two or more new columns using separate().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nConversely, to combine two or more columns into a single character column, use unite().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n2.16.2 Reshaping Data: gather() and spread()\ngather() transforms “wide” format data into “long” or “tall” format by collapsing columns into key-value pairs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nConversely, spread() converts “long” or “tall” format data into “wide” format by separating key-value pairs across multiple columns.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#combining-datasets-joins",
    "href": "chapters/chapter3_dplyr.html#combining-datasets-joins",
    "title": "2  Getting Started with dplyr",
    "section": "2.17 Combining Datasets: Joins",
    "text": "2.17 Combining Datasets: Joins\nSometimes, your data is split across multiple tables. For example, one table may have demographic information (like gender, marital status, height, weight), and another table may have medical records (like visits and surgeries).\nWhen working with data spread across multiple tables, you’ll often need to combine them based on a shared column (a “key column”)—a process known as joining or merging. dplyr provides several functions for common data joins.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ninner_join() keeps only the rows where “key column” exists in both tables and drops all rows that do not have a match in both.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nlefy_join() keeps all rows from the left table (here, table1) and fills in matching information from the right table (here, table2). If no match is found, you will get NAs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nfull_join() keeps all rows from both tables. Rows without a match in the other table will have NAs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can match on more than one column. For example, match ID and also make sure the gender in table1 matches sex in table2.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis is useful when one key (ID) is not unique enough by itself.\nFilter-based joins: These do not add new columns. They just filter rows:\nsemi_join() keeps only rows in table1 that have a match in table2. It does not add any columns from table2.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nanti_join() keeps only rows in table1 that do not have a match in table2. It is good for finding “missing” mathches.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nFor more information, plesee see chapter 19 of Wickham, Cetinkaya-Rundel, and Grolemund (2023) .\n\n\n\n\nExerciseHintsSolution\n\n\nUsing the starwars dataset and dplyr functions, perform the following data manipulations.\nPart 1: Initial Exploration and Filtering\n\nFilter for Human Characters: Create a new data frame called human_characters that only includes characters of the “Human” species.\nSelect Key Attributes: From human_characters, select only the name, height, mass, and homeworld columns.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPart 2: Calculating BMI and Identifying Extremes\n\nCalculate BMI: Add a new variable called bmi (Body Mass Index) to your human_characters data frame. The formula for BMI is \\(\\text{mass}/(\\text{height}/100)^2\\). Ensure that mass is in kilograms and height in centimeters as provided in the dataset.\nSort by BMI: Arrange the human_characters data frame in descending order based on their bmi.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPart 3: Grouped Summaries\n\nSpecies Statistics: Calculate the number of characters (n) and the average height for each species in the original starwars dataset.\nFilter Significant Species: From the previous summary, filter out species that have fewer than 5 characters and an average height greater than 100.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPart 4: Conditional Categorization\n\nCategorize Height: Add a new variable called height_category to the starwars dataset using mutate() and case_when().\n\nIf height is less than or equal to 100, categorize as “Short”.\nIf height is greater than 100 but less than or equal to 180, categorize as “Medium”.\nIf height is greater than 180, categorize as “Tall”.\nHandle NA values for height appropriately so they remain NA in height_category.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPart 1: Initial Exploration and Filtering\n\nFilter for Human Characters: which dplyr function helps you select rows based on a condition? Remember the syntax for checking equality.\nThere is a dplyr function specifially for choosing columns.\n\nPart 2: Calculating BMI and Identifying Extremes\n\nCalculate BMI: Which dplyr function is used to create or modify columns? Pay attention to the order of operations in the formula.\nSort by BMI: The arrange() function is key here. How do you specify descending order?\n\nPart 3: Grouped Summaries\n\nSpecies Statistics: You will need two main functions here: one to define groups and another to calculate summary statistics within those groups. Do not forget to handle NA values for the mean.\nFilter Significant Species: You will apply another filter() operation, but this time on the summarized data. Remember how to combine two conditions.\n\nPart 4: Conditional Categorization\n\nCategorize Height: case_when() is perfect for multiple conditions. How do you specify the conditions and their corresponding outputs? Remember to check for NAs first to ensure they do not get misclassified by other conditions.\n\n\n\n\n\nSolution. \nlibrary(dplyr)\n\n# Part 1: Initial Exploration and Filtering\nhuman_characters &lt;- starwars |&gt; \n  filter(species == \"Human\") |&gt; \n  select(name, height, mass, homeworld)\n\n# Part 2: Calculating BMI and Identifying Extremes\nhuman_characters_bmi &lt;- human_characters  |&gt; \n  mutate(bmi = mass / ((height / 100)^2))  |&gt; \n  arrange(desc(bmi))\n\n# Part 3: Grouped Summaries\nspecies_stats &lt;- starwars  |&gt; \n  group_by(species) %&gt;%\n  summarise(\n    n = n(),\n    avg_height = mean(height, na.rm = TRUE)\n  ) %&gt;%\n  filter(\n    n &gt;= 5, \n    avg_height &gt; 100\n  )\n\n# Part 4: Conditional Categorization\nstarwars_with_height_category &lt;- starwars %&gt;%\n  mutate(\n    height_category = case_when(\n      is.na(height) ~ NA_character_, # Handle NA values first\n      height &lt;= 100 ~ \"Short\",\n      height &gt; 100 & height &lt;= 180 ~ \"Medium\",\n      height &gt; 180 ~ \"Tall\"\n    )\n  )\n\n\n\n\nTo learn more about the dplyr package, here are some recommended resources:\n\ndplyr.tidyverse.org\nChapter “Data transformation” in the book “R for Data Science”\nCheatsheet\nBlog Stats and R\nKaggele\n\n\n\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed. https://r4ds.hadley.nz/.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with `dplyr`</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html",
    "href": "chapters/chapter2_data_visualization.html",
    "title": "3  Data Visualization",
    "section": "",
    "text": "3.1 Data\nThe most famous package in R that is used for visualizing of data is ggplot2 that is based on the grammar of graphics. It allows you to `speak’ a graph from composable elements, instead of being limited to a predefined set of charts.\nBefore using ggplot2, a user must first install the package and then load it into their R session. Installation is done only using the command install.packages('ggplot2'). Alternatively, the following code can be used to install if only if the package is not already installed on the computer.\nwhich downloads the package from CRAN (the Comprehensive R Archive Network). After installation, the package needs to be loaded each time R is restarted. This is done with the following command:\nA brief overview about using this package is available at this website and more complete information about how to use ggplot2 can be found in Wickham, Navarro, and Pederson (2019).\nThe structure of the package includes 7 composable parts that come together as a set of instructions on how to draw a chart.\nThe package ggplot2 requires a minimum of three main components to create a chart: data, mapping, and a layer. Other components – like scales, facets, coordinates, and themes – are optional because ggplot2 gives them automatic settings that usually work well, so you do not need to adjust them too much.\nIn the following, we briefly describe these components:\nEvery plot made with ggplot2 starts with data. This data should be in a tidy format that means the data is in a table (called a rectangular data frame) where:\nThe first step to create a plot with ggplot2 is to pass this data to the ggplot() function, which stores the data to be used later by other parts of the plot.\nFor example, if we want to make a plot using the mpg dataset, we begin like this:",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#data",
    "href": "chapters/chapter2_data_visualization.html#data",
    "title": "3  Data Visualization",
    "section": "",
    "text": "Each row is one observation.\nEach column is one variable.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nYou should see an empty plot. It is correct. Can you explain why the plot is empty?",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#mapping",
    "href": "chapters/chapter2_data_visualization.html#mapping",
    "title": "3  Data Visualization",
    "section": "3.2 Mapping",
    "text": "3.2 Mapping\nIn ggplot2 package, mapping means telling the system how to connect parts of the data to aesthetic properties of the plot.\nAesthetic (say: es-THE-ik) is a word that describes how something looks – its style, color, shape, and beauty. In ggplot2, an aesthetic is a visual feature of a plot like:\n\n Position (x and y) – controls where data appears on the plot\n\n Color – changes color based on data values\n\n Size – adjusts the size of points\n\n Shape – uses different shapes for different categories\n\nThese help us turn numbers into pictures. It is how we ’’dress up” the data so it speaks to our eyes.\n\n\n\n\n\n\nMemory tip\n\n\n\nJust like fashion has aesthetic styles (like modern, classic, or colorful), plots also have aesthetics – they decide how the data looks!\n\n\nA mapping can be made by using aes() function to make pairs of graphical attributes and parts of the data. Inside aes(), we match parts of the data (like column names) with visual elements (like \\(x\\) and \\(y\\) position).\nFor the dataset mpg, if we want to show cty (city miles per gallon) on the \\(x\\)-axis and hwy (highway miles per gallon) on the \\(y\\)-axis, we write\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#layers",
    "href": "chapters/chapter2_data_visualization.html#layers",
    "title": "3  Data Visualization",
    "section": "3.3 Layers",
    "text": "3.3 Layers\nThe layer is the heart of any plot in ggplot2. A layer takes the mapped data and turns it into something that a human can see and understand – like points, lines, or bars.\nEach layer has three main parts: 1. Geometry – decides how the data is shown (for example: points, lines, bars) 2. Statistical transformation – can create new values from the data (like averages or smooth curves). 3. Position adjustment – controls where each part of the plot appears, especially when things overlap.\nA layer can be constructed using functions that start with geom_ (for geometry) and stat_ (for statistics). These function help us choose how the data looks and what to display.\n\n\n\n\n\n\nNote\n\n\n\nThe geom_*() and stat_*() functions usually control one part of the layer – like the geometry or the statistics – but you can still manullay choose the other two parts if you want.\n\n\nThe code below shows how to make a scatter plot with a trend line using two layers:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#scales",
    "href": "chapters/chapter2_data_visualization.html#scales",
    "title": "3  Data Visualization",
    "section": "3.4 Scales",
    "text": "3.4 Scales\nA scale translates what we see on the plot into something we can understand from the data – like showing how far, how big, or what category a point represents.\nEach scale is connected to an aesthetic – for example, the \\(x\\)-axis, \\(y\\)-axis, color, or size – and it controls things like - The limits of the plot (minimum and maximum values) - The breaks (where ticks or labels appear) - The format of the labels (like numbers or percentages) - Any transformation (like logarithmic scale)\nScales also create guides for the reader – like axes or legend – so we can better understand the meaning of the plot.\nScale functions in ggplot2 usually follow the format scale_[aesthetic]_[type](), where {aesthetic} is one of the pairing made in the mapping part of a plot. For example, scale_x_continuous() – for a numeric \\(x\\)-axis; scale_colour_viridis_d() – for discrete color with the Viridis palette.\nHere is how to use a custom color scale for the class variable in the mpg dataset:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis tells ggplot2 to use Viridis colors for the different car classes – making the plot easier to read and more accessible.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#facets",
    "href": "chapters/chapter2_data_visualization.html#facets",
    "title": "3  Data Visualization",
    "section": "3.5 Facets",
    "text": "3.5 Facets\nFacets are used to split a plot into smaller subplots, each showing a subet of the data. This is helpful when you want to compare groups – for example, different years, categories, or types – in separate panels.\nFacets are a powerful way to quickly see patterns, trends, or even no patterns in different parts of the data.\nFacets have their own mapping, written as a formula.\nThe following code creates one scatter plot of two variables cty and hwy of the dataset mpg for each combination of year and drv (driver type) in different panels; so, you can compare them side by side.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn facet_grid(year ~ drv), the rows are based on year and the columns are based on drv. As you see, this creates a grid of plots that makes it easy to explore how variables interact.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#coordinates",
    "href": "chapters/chapter2_data_visualization.html#coordinates",
    "title": "3  Data Visualization",
    "section": "3.6 Coordinates",
    "text": "3.6 Coordinates\nThe coordinates is the part if the plot that controls how position aesthetics (like \\(x\\) and \\(y\\)) are shown. You can think of it as the interpreter that tells the plot where to place things.\nMost ggplot2 plots use Cartesian coordinates, where data is shown on regular \\(x\\)-\\(y\\) grid. But you can also use other systems, like: Polar cooridinate (for circular plots) and Map projections (for geographic data).\nYou can also use coordinates to make sure that one unit on the \\(x\\)-axis is the same size as one unit on the \\(y\\)-axis. This is called a fixed aspect ratio. The function coord_fixed() does this ratio automatically.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#theme",
    "href": "chapters/chapter2_data_visualization.html#theme",
    "title": "3  Data Visualization",
    "section": "3.7 Theme",
    "text": "3.7 Theme\nThe theme controls parts of the plot that are not related to the data, such as background, text, grid, and legend position. It helps define the overall look and feel of the plot.\nFor instance, you can use the theme to: - Move or hide the legend - Change font size or colors - Set a new background style - Adjust the axes or grid lines\nSome them settings are hierarchical, meaning that if you change the general axis style, it also changes both the \\(x\\)-axis and \\(y\\)-axis unless you set them separately.\nTo change the appearance of a plot, you can use the built-in theme_*() functions (like theme_minimal(), theme_classic(), or theme_light()), or you can customize specific details using the theme() function. With element_*() functions (e.g. element_line(), element_text()) let you adjust the visual style of theme parts, such as lines, texts, or backgrounds.\nThe following code creates a scatter plot and applies a minimal theme. It also moves the legend to the top, makes axis lines thicker, and colors the bottom \\(x\\)-axis blue.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#mastering-plot-layers-guided-examples-and-student-activities",
    "href": "chapters/chapter2_data_visualization.html#mastering-plot-layers-guided-examples-and-student-activities",
    "title": "3  Data Visualization",
    "section": "3.8 Mastering Plot Layers: Guided Examples and Student Activities",
    "text": "3.8 Mastering Plot Layers: Guided Examples and Student Activities\nAs we mentioned before, a ggplot2 is made by layering different parts. You can combine data, mapping, geoms, scales, facets, coordinates, and themes to build a fully customized plot.\nBelow is an example that brings everything together:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nThere is no need to type argumants.\n\n\n\n3.8.1 Scatter Plot – geom_point()\nA scatter plot shows the relationship between two numeric variables. each point on the plot represents one observation.\n\n\n\n\n\n\nTip\n\n\n\nScatter plots are useful for seeing patterns, trends, or outliers in data.\n\n\nAn example for seeing the basic scatter plot is showing how height (in inches) changes with age (in years) for each person in the dataset heightweight.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nwe can customize the size of points by adding size = 1.5 to the function geom_point().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, we can add two aesthetics: shape and color, based on the sex variable.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, we are mapping colour or size to a numeric variable (e.g. weightLb)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThese plots show how weight varies with age and height using color or size.\nNow, we are mapping colour to the variable sex and size to a numeric variable weightLb\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExerciseHintsSolution\n\n\nUsing the heightweight dataset, make a scatter plot to explore the relationship between heightIn and weightLb variables. Your tasks are:\n\nUse points to represent the data.\nShow the variable sex using color so that we can see the difference between males and females.\nMake the points larger than usual, with a size of \\(2.5\\).\nFinally, give your plot a title that describes what it shows, The title should be: “Height vs, Weight by Sex”\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nYou will need\n\naes(x = ______, y = ______, colour = ______) -\ngeom_point(size = ______)\nggtitle(______)\n\n\n\n\n\nSolution. \nggplot(heightweight, aes(x = weightLb, y = heightIn, colour = sex)) + \n  geom_point(size = 2.5) +\n  ggtitle(\"Height vs. Weight by Sex\")\n\n\n\n\n\n\n3.8.2 Line Plot – geom_line()\nUse when: You want to show how a variable changes over time or another ordered variable (like day, year, index)\nWhat is shows: Trends, increases or decreases, cycles over time\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis plot shows how unemployment has changed over time.\n\n\n3.8.3 Barplot – geom_bar() and geom_col()\nUse when: You want to compare counts or values between categories.\n\ngeom_bar() counts how many times each category appears.\ngeom_col() shows values you provided directly.\n\nWhat it shows: Frequencies or values across different categories.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.8.4 Histogram – geom_histogram()\nUse when : You want to see the distribution of a numeric variable.\nWhat is shows : How values are spread across intervals (called “bins”).\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis plot shows many cars fall into different highway mpg ranges.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.8.5 Boxplot – geom_boxplot()\nUse when: You want to compare summary statistics (like median, quartiles, ouliers) across different groups.\nWhat it shows : Median, spread, and outliers for each group.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis plot compare highway mpg for different type of vehicles.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.8.6 Violin Plot – geom_violin()\nUse when: You want to see the distribution shape plus summary info for groups.\nWhat it shows: Like a boxplot, but with a mirrored density curve – great for showing the full distribution and comparison across groups.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can also add add points or boxplots inside the violins:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.8.7 Dot plot – geom_dotplot()\nUse when: You want to show individual values, especially for small datasets.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.8.8 Density Plot – geom_density()\nUse when: You want a smoothed version of a histogram.\nWhat is shows: Estimate of the data’s distribution, good for seeing shapes or peaks.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can combine the histogram and density plot to show exact counts (histogram) and smooth distribution (density) in one plot.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nWhat is after_stat(density)?\n\n\n\nIn ggplot2, when you use geom_histogram(), the default \\(y\\)-axis is count (how many observations fall in each bin). But if you write aes(y = after_stat(density)), you are asking ggplot2 to scale the height of the bars so they represent probability density instead of raw counts. This allows you to compare this histogram with a density curve (like the one from geom_density()), which also shows density.\n\n\nThis shows both frequency and smoothed distribution of highway mpg.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.8.9 Pie Chart (using Polar Coordinates)\nUse when: You want to show parts of a whole, but use with caution – bar plots are often easier to read.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.8.10 2D Density Plot\nA 2D density plot is used to show the distribution of two numeric variables. Instead of plotting each point (like in a scatter plot), it shows where points are concentrated using contour lines (geom_density_2d()) and filled contour areas (geom_density_2d_filled()).\nUse when: you have many overlapping points (overlapping) in a scatter plot and you want to highlight patterns or clusters in two numeric variables and see a smoother version of the joint distribution.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAdding points makes it easier to see what the contours represent.\n\nggplot(faithful, aes(x = waiting, y = eruptions)) +\n  # Add Data Points for Context\n  geom_point(alpha = 0.3) +\n  geom_density_2d()\n\n\n\n\n\n\n\n\nThe following code results in a plot that shows filled bands of density that it is great for visual impact.\n\nggplot(faithful, aes(x = eruptions, y = waiting)) + \n  geom_density_2d_filled(alpha = 0.6) \n\n\n\n\n\n\n\n\nCombining points and filled contours gives both precision and overview.\n\nggplot(faithful, aes(x = eruptions, y = waiting)) +\n  geom_point(alpha = 0.3) +\n  geom_density_2d_filled(alpha = 0.5) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nYou can map density levels to color using after_stat(level)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can also different code to produce the plot.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.8.11 2D Binned Plot for Big Data – stat_bin2d()\nUse when: You have a lot of points and scatter plots are too dense.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.8.12 Frequency Polygon – geom_freqpoly()\nA frequency plot is similar to a histogram, but it uses lines instead of bars to show how a variable is distributed. It is especially useful when you want to compare distributions across groups.\nUse when: You want to show the shape of a distribution and also compare multiple groups using lines (which may be easier to read than overlapping histograms).\nExamples\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.8.13 Correlation Plot (Cor plot)\nA correlation plot (or cor plot) is a visual way to show how strongly variables are related to each other. Correlation values range from \\(-1\\) to \\(1\\), and the plot helps you quickly see positive, negative, or no relationships.\nThis is not built into ggplot2, but we can use it together with\n\nThe cor() function to compute correlation\nThe corrplot or ggcorrplot package to visualize it.\n\nExample using ggcorrplot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nExample using corrplot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pederson. 2019. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html",
    "href": "chapters/chapter4_EDA.html",
    "title": "4  Exploratory data analysis (EDA)",
    "section": "",
    "text": "4.1 Plan the Study\nStatProcess\n\ncluster_problem\n\n\ncluster_plan\n\n\ncluster_data\n\n\ncluster_analysis\n\n\ncluster_conclusion\n\n\n\nP1\n\nP\n\n\n\nBOX1\n\n\n\n\n\n\n\nProblem\n• Write down study objectives\n• Identify target/sample population\n• Define the variates\n\n\n\nP1-&gt;BOX1\n\n\n\n\nP2\n\nP\n\n\n\nP1-&gt;P2\n\n\n\n\n\nBOX2\n\n\n\n\n\n\n\nPlan\n• Plan data collection methods\n• Calculate sample size\n• Consider analysis options\n\n\n\nP2-&gt;BOX2\n\n\n\n\nD\n\nD\n\n\n\nP2-&gt;D\n\n\n\n\n\nBOX3\n\n\n\n\n\n\n\nData\n• Collect data per plan\n• Note any deviations\n\n\n\nD-&gt;BOX3\n\n\n\n\nA\n\nA\n\n\n\nD-&gt;A\n\n\n\n\n\nBOX4\n\n\n\n\n\n\n\nAnalysis\n• Analyze data per plan\n\n\n\nA-&gt;BOX4\n\n\n\n\nC\n\nC\n\n\n\nA-&gt;C\n\n\n\n\n\nBOX5\n\n\n\n\n\n\n\nConclusion\n• Draw conclusion in context\n\n\n\nC-&gt;BOX5",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html#after-collecting-the-data",
    "href": "chapters/chapter4_EDA.html#after-collecting-the-data",
    "title": "4  Exploratory data analysis (EDA)",
    "section": "4.2 After collecting the data",
    "text": "4.2 After collecting the data\n\n\n\n\n\n\n\nAfterCollecting\n\n\nS1\n\nReview the hypotheses\n•\nReview the research questions\n•\nConsider sub-questions\n\n\n\nS2\n\nProcess the raw data\n•\nSelect a suitable statistics software\n•\nConvert the data into an acceptable format\n\n\n\nS1-&gt;S2\n\n\n\n\n\nS3\n\nExplore the data\n•\nDescriptive summaries\n•\nData visualization\n\n\n\nS2-&gt;S3\n\n\n\n\n\nS4\n\nAnalyze the data\n•\nInferential analysis\n•\nPrediction\n\n\n\nS3-&gt;S4\n\n\n\n\n\nS5\n\nReport the results\n•\nInterpret the results in the context of the study\n\n\n\nS4-&gt;S5",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html#what-is-eda",
    "href": "chapters/chapter4_EDA.html#what-is-eda",
    "title": "4  Exploratory data analysis (EDA)",
    "section": "4.3 What is EDA?",
    "text": "4.3 What is EDA?\nExploratory data analysis (EDA) was first formally introduced by Tukey (1977) in his influential book, Exploratory Data Analysis.\n\n\n\n\n\n\n\n\n\n\nSince then, its importance has grown significantly, especially in recent years, for several key reasons (Midway 2022):\ni) data is being produced faster and in larger volumes than ever before,\nii) modern computing and software tools make it easier to explore, clean, and visualize data in meaningful ways,\niii) contemporary statistical models are often complex and assumption-dependent, requiring us to thoroughly understand the data before applying formal techniques.\nThe EDA may not be fully described in concrete terms, but most data analysts and statisticians know it when they see it.\n\n\n\n\n\n\nImportant\n\n\n\nThe EDA is important because it helps researchers make thoughtful decisions about which ideas are worth exploring. Sometimes, the data clearly show that a particular question does not have enough support to be studied further—at least not with the current evidence.\n\n\nThe main goals of EDA are:\n\nTo suggest hypotheses about what might be causing the patterns or relationships observed in the data,\nTo guide the choice of appropriate statistical tools or models by helping you understand the structure of the data,\nTo assess key assumptions that must be checked before applying formal statistical analysis (e.g., linearity, normality, independence),\nTo provide a basis for further data collection, by highlighting gaps, inconsistencies, or areas where more information is needed.\n\n\n\n\n\n\n\nNote\n\n\n\nEDA is not typically the final stage of analysis. Rather, it serves as a transitional step between raw data and formal modeling. The insights gained through EDA guide decisions about which models to use, which variables to consider, and which data issues to address.\n\n\n\n\n\n\n\n\nEDA Checklist\n\n\n\nRoger D. Peng in his book (Peng 2012) provide this checklist for conducting EDA.\n\nStart with a clear question: Before you begin EDA, take time to define exactly what you want to find out. A clear question or hypothesis gives your analysis purpose and helps you stay focused along the way.\nLoad your data carefully: Make sure your dataset is fully and correctly loaded into your analysis tool (e.g., R or Python). This first step is essential—it sets the stage for everything you will do next.\nTake a first look at the data: Check that the file type, structure, and layout are what you expected. Make sure everything is organized in a way that works for your analysis.\nUse str() to Peek Inside the Dataset: In R, it will get a quick summary: number of observations (rows), number and names of variables (columns), variable types (e.g., numeric, character, factor), and a preview of the data values\nLook at the Beginning and End of Your Data: Use functions such as head() and tail() to view the first and last few rows. This visual check can help to detect issues such as incorrect headers, blank rows, or unusual formatting.\nCheck the Number of Rows (“n”): Make sure to verify how many observations (rows) are in your dataset. Compare this to what you expected from the original. source. If the number is too high or too low, there may be missing values, duplicate entries, or extra rows (e.g. duplicates or blank lines).\nValidate with an External Source: When possible, compare part of your dataset with a trusted external source, such as official statistics or published reports. This helps confirm the accuracy and reliability of your data.\nTry the Simple Solution First: Start by basic methods–such as summaries, tables, or visualizations– to explore and answer your question. Simple tools can often reveal key patterns or issues. Use more complex techniques only of necessary.\nChallenge Your Findings: Once you find a result, pause and ask yourself: Does this make sense? Check your assumptions and consider possible errors or missing information. Being critical helps make your results stronger and more trustworthy.\nDecide What to Do Next: Use the insights from your EDA to guide your next steps. You may decide to collect more data, use new methods, or refine your question. Sometimes , your initial findings are already enough to answer your main question.\n\nIn Chapter 4 of Midway (2022), interested reader can find some comments for each step.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nEDA is the single most important task to conduct at the beginning of every data science project.\n\n\n\n\n\n\n\n\nTip\n\n\n\nEDA is like exploring a new place – you do not know what you will find until you start looking.\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD ## Landscape\n\n\n\n\n\n\n\nEDA\n\n\nEDA\n\nExploratory Data Analysis\n\n\n\nView\n\nView data\n\n\n\nEDA-&gt;View\n\n\n\n\n\nSummary\n\nSummary\nstatistics\n\n\n\nEDA-&gt;Summary\n\n\n\n\n\nGraphs\n\nBasic\nGraphs\n\n\n\nEDA-&gt;Graphs\n\n\n\n\n\nTests\n\nBasic\nTests\n\n\n\nEDA-&gt;Tests\n\n\n\n\n\nV1\n\nObservation number\nVariable number\nVariable type\nVariable category\nStructure\n\n\n\nView-&gt;V1\n\n\n\n\n\nS1\n\nMean\nMedian\nMode\nRange\nVariance\nStandard deviation\nOutliers\nMissing data\n\n\n\nSummary-&gt;S1\n\n\n\n\n\nG1\n\nHistogram\nBar plot\nBoxplot\nScatter-plot\nQQ-plot\n\n\n\nGraphs-&gt;G1\n\n\n\n\n\nT1\n\nCheck assumptions\nT-tests\nCorrelations\nANOVA\nLinear model\n\n\n\nTests-&gt;T1",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html#enter-the-world-of-eda",
    "href": "chapters/chapter4_EDA.html#enter-the-world-of-eda",
    "title": "4  Exploratory data analysis (EDA)",
    "section": "5.1 Enter the world of EDA",
    "text": "5.1 Enter the world of EDA\nEDA is the process of looking at data to understand it before using statistics or models. It helps us to answer questions like: - what types of data do we have? - Are there missing or strange values? - What patterns can we see? - What kind of summary or visualization is helpful?\nBefore we start any analysis, we must know what kind of data we are working with. Variables can be broadly divided into numeric and categorical types.\n\n5.1.1 Types of Variables\n\n\n\n\n\n\nImportant\n\n\n\nKnowing the types of data matters because:\n\nDifferent statistics method are used for different variable types.\nSome graphs are only useful for some types of variables.\n\n\n\n\n5.1.1.1 Numerical variables\nNumeric variables describe quantities that we can measure — they answer questions like “how many?” or “how much?”. These variables are also called quantitative variables, and the data they produce are known as quantitative data. Briefly, they are numbers that can be measured. Also, we can do math with them.\n\n5.1.1.1.1 Continuous vs. Discrete\nNumeric variables can be:\nContinuous\n\nCan take any value on a number line, including decimals.\nRepresent real-world quantities.\n\nSome examples are: mass, age, temperature, time.\nContinuous variables can be positive only (like mass) or positive and negative (like change in temperature).\nDiscrete\n\nCan take only whole number values (no decimals).\nBased on counting.\n\nSome examples are: number of students, number of offspring, number of infected individuals\nA discrete variable cannot take values between integers — e.g., it makes no sense to have 2.5 individuals.\n\n\n5.1.1.1.2 Ratio vs. Interval\nNumeric variables can be further described based on the scale they are measured on: ratio or interval. This affects how we can interpret differences, proportions, and calculations.\nRatio\n\nHas a true zero point (zero means “none”).\nAllows all mathematical operations, including ratios.\n\nSome examples are: Height, Weight, Age, Income, mass.\nYou can say: “Tree A is twice as tall as Tree B.”\n\n5.1.1.1.2.1 What we can do:\n\nAll descriptive statistics\nUse histograms, box-plots, scatterplots\n\nInterval\n\nHas no true zero — the zero point is arbitrary.\nAllows meaningful differences, but not ratios.\n\nSome examples are Temperature (in Celsius of Fahrenheit), IQ scores, Calendar dates (e.g. year 1000 vs 2000)\nYou can say: “The difference between 20°C and 10°C is 10 degrees.” But not: “20°C is twice as hot as 10°C.”\n\n\n5.1.1.1.2.2 What we can do:\n\ncalculate mean, standard deviation\nplot histograms or line charts\n\n\n\n\n\n\n\nCaution\n\n\n\nThe difference between ratio and interval is about how the variable is measured, not what is being measured.\nFor example, Temperature can be measured on:\n\nThe Celsius scale that results in an interval scale (no true zero).\nThe Kelvin scale that results in a ratio scale (zero means no temperature, i.e. absolute zero).\n\nSo, the same thing (temperature) can be: an interval variable (in °C), or a ratio variable (in K), depending on the scale we use.\n\n\n\n\n\n\n5.1.1.2 Categorical Variables\nCategorical variables describe qualities or characteristics. They answer questions like “what type?” or “which group?”. These are also known as qualitative variables, and the data they produce are qualitative data.\n\n\n\n\n\n\nImportant\n\n\n\nCategorical variables do not have numeric meaning (even if coded as numbers!).\n\n\nCategorical variables fall into two groups:\nNominal Variables\n\nCategories have no logical order.\nOnly show labels or names.\n\nFor example, Gender (male, female), Blood group (A, B, AB, O), City (Lisbon, Porto, Faro, … ), color (red, blue, green)\n\n5.1.1.2.0.1 What we can do:\n\nCount frequencies (e.g. how many students are from each city)\nUse bar charts or pie charts\n\nOrdinal Variables\n\nCategories can be ordered or ranked.\nBut the distance between categories is not exact.\n\nFor example, Satisfaction (low, medium, high), education (primary, secondary, university), rank (1st, 2nd, 3rd), Academic grades (A, B, C)\n\n\n5.1.1.2.0.2 What we can do:\n\nCount frequencies\nCompare medians\nUse bar charts or ordered plots\n\n\n\n\n\n\n\nImportant\n\n\n\nDo not use numbers to label categories.\nEven though categories can be coded as numbers (e.g., Male = 1, Female = 2), this can be confusing. It may suggest an order or imply mathematical operations that don’t make sense.\n\n\n\n\n\n\nTip: Use text labels like \"Male\" and \"Female\" instead of 1 and 2.\n\n\n\n\n\nSummary of Variable Types\n\n\n\n\n\n\n\n\n\n\n\nVariable Type\nScale\nOrdered?\nCan Measure Distance?\nCan Divide?\nExamples\n\n\n\n\nNominal\nCategorical\n❌ No\n❌ No\n❌ No\nGender, City, Color\n\n\nOrdinal\nCategorical\n✅ Yes\n❌ No (not exact)\n❌ No\nRank, Education, Likert\n\n\nInterval\nNumeric\n✅ Yes\n✅ Yes\n❌ No\nTemperature, Year\n\n\nRatio\nNumeric\n✅ Yes\n✅ Yes\n✅ Yes\nAge, Weight, Income\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are not sure about a variable, ask:\n\nDoes the variable have a natural order?\nCan we count it or measure it?\nDoes it have a true zero?\n\n\n\n\n\n\n\n5.1.2 Types of EDA\nEDA helps us answer important questions like:\n\nWhat are the most common values of a variable?\nHow much do observations differ from each other?\nIs one variable related to another?\n\nTo answer these questions, EDA uses two main tools:\n\n5.1.2.1 Descriptive Statistics\nDescriptive statistics give numerical summaries of a dataset. They describe the basic features of a variable, such as:\n\nCenter (e.g. mean or median)\nSpread (e.g. standard deviation or interquartile range)\nShape (e.g. skewness)\nExtremes (e.g. minimum, maximum, or outliers)\n\nThese summaries help us compare groups or make initial conclusions.\nFor example, the mean tells us what the “typical” value might be.\nBut: A few numbers can’t tell the whole story.\n\n\n5.1.2.2 Graphical Summaries\nGraphs help us see patterns, spot outliers, and understand distributions.\nCommon EDA plots include: - Histograms – show the shape and spread of a numeric variable - Boxplots – show center, spread, and outliers - Bar charts – show frequencies of categories - Scatterplots – show relationships between two numeric variables\nGraphs are easier to understand than tables of numbers. They help us and others see what’s going on in the data.\n\n\n\n5.1.3 A Review on Descriptive Statistics\nStatisticians have created more precise terms to describe the patterns in data, called descriptive statistics.\nThe two most important features of a numeric variable’s distribution are:\n\nCentral tendency: What is a typical value?\nDispersion: How spread out are the values?\n\n\n\n5.1.4 Central Tendency\nA measure of central tendency tells us what a “typical” value looks like in the data. There are three main measures: Mean, Median, Mode\n\n5.1.4.1 Arithmetic Mean (Average)\n[We should add the definition]\nfunction mean()\nSometimes, we use na.rm = TRUE to ignore missing values.\nBe careful: The mean is sensitive to extreme values (outliers).\nFor example, using the mean to describe income is misleading because a few very high earners can pull the average up.\n\n\n5.1.4.2 Median\n[We should add the definition]\n\nThe middle value when data are sorted.\nMore robust than the mean when data are skewed.\n\nfunction median()\n\n\n5.1.4.3 Mode\n\nThe most frequent value in the data.\nConceptually useful, but hard to estimate in numeric data.\nUsed more often for categorical variables.\n\n\n\n\n5.1.5 Dispersion\nDispersion tells us how much values differ from one another. A dataset where all values are similar has low dispersion; one with a wide range of values has high dispersion.\nThere are many ways to describe how spread out a sample is. The three common measures are:\n\nVariance – how far values are far from the mean (on average, squared)\nStandard Deviation – the square root of the variance, easier to interpret\nInterquartile range – the spread of the middle \\(50\\%\\) of the values\n\nEach of these gives us a different way to understand how variable the data is.\n\n5.1.5.1 Variance\nThe Sample Variance, written as \\(s^2\\), is the average of the squared differences between each value and the mean.\nKey facts about variance are:\n\nIt is always non-negative\nA larger variance means the data are spread out.\nA variance of zero means all values are exactly the same.\n\n\n\n\n\n\n\nNote\n\n\n\nWhy does variance matter?\nVariance is an important quantity in statistics. Many statistical tests use changes in variance to compare groups or detect effects.\n\n\nBut in EDA, we rarely use the variance.\n\n\n5.1.5.2 Standard Deviation\nA more useful measure if dispersion is the standard deviation, usually written as \\(s\\). The standard deviation is the squared root of the variance.\n\n\n\n\n\n\nNote\n\n\n\nWhy use standard deviation?\n\nIt is on the same scale as the variable\nIt gives us a sense of spread we can understand\nit is more commonly used in EDA than variance\n\n\n\nBut be careful like the mean, the standard deviation is sensitive to:\n\nskewed distributions\noutliers (extreme values)\n\nIn those cases, we often prefer a robust measure like the interquantile range (IQR).\nTo understand the IQR, we need to define quartiles.\nQuartiles divide the data into four equal parts:\n\n\n\n\n\n\n\nQuartile\nMeaning\n\n\n\n\nQ1\nFirst quartile (25%) — one quarter of the data is below this value\n\n\nQ2\nSecond quartile (50%) — this is the median\n\n\nQ3\nThird quartile (75%) — three quarters of the data are below this value\n\n\n\nThese values help summarize the distribution, especially for non-symmetric data.\n\n\n5.1.5.3 Interquantile Range (IQR)\nThe Interquantile Range (IQR) is another measure of dispersion.\nIt is especially useful in EDA because it is robust to outliers and skewed data.\nBecause standard deviation is not squared, it has the same unit as the original data. This makes it easier to interpret than the variance.\nWhat is IQR?\nThe IQR measures the spread of the middle \\(50\\%\\) of the data.\nIt is defined as:\nIQR = Q3 - Q1\nwhere Q1 is the first quartile and Q3 is the third quartile. So, the IQR is the range between the \\(25\\%\\) and \\(75\\%\\) marks.\nThe wider the IQR, the more spread out the middle values are.\nWhy is the IQR useful?\n\nIt does not depend on extreme values.\nIt gives a good summary of the central spread\nIt is preferred in EDA for skewed or messy data.\n\n\n\n\n5.1.6 Associations\nIn EDA, we often want to ask Is one variable associated with another?\nThis can be for:\n\nTwo numeric variables\nTwo categorical variables\nOne numeric and one categorical\n\n\n5.1.6.1 Pairs of Numeric Variables\nThe most common way to measure association between two numeric variables is:\nPearson’s Correlation Coefficient\n\nMeasures Strength and direction of a linear relationship.\nValues range from \\(-1\\) to \\(+1\\):\n\n\\(+1\\): Perfect positive linear association\n\\(-1\\): Perfect negative linear association\n\\(0\\): no linear relationship\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPerson’s \\(r\\) only measures linear relationships.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIf the relationship is curved, \\(r\\) can be misleading. See Anscombe’s Quartet for famous examples.\n\n\n\n\n\n\n\n\nNote\n\n\n\nRank Correlations: For Non-linear Relationships\nIf the relationship is not linear, use a rank correlation:\n\n\n\nMethod\nNotes\n\n\n\n\nSpearman’s \\(\\rho\\)\nMore sensitive to outliers\n\n\nKendall’s \\(\\tau\\)\nBetter for ordinal data\n\n\n\nThese methods:\n\nconvert data into ranks (1st, 2nd, 3rd, …)\nMeasure how well the ranks agree between two variables\n\nThey are interpreted like Pearson’s \\(r\\):\n\nClose to \\(\\pm 1\\) means strong association\n\\(0\\) means no association\n\n\n\n\n\n5.1.6.2 Pairs of Categorical Variables\nFor two categorical variables, we ask Are certain combinations of categories more or less common than expected?\nThe most basic tool is the contingency table. It counts how many times each combination of categories appears in the data.\nThis table helps us to answer questions like:\n\nDo some combinations occur more often than others?\nAre the two categorical variables associated?\n\nExample: Penguins Species and Islands\nImagine we observed \\(344\\) penguins from three species on three islands. We record how many penguins of each species were found on each island.\n\n\n\nSpecies\nBiscoe\nDream\nTorgersen\nTotal\n\n\n\n\nAdelie\n44\n56\n52\n152\n\n\nChinstrap\n0\n68\n0\n68\n\n\nGentoo\n124\n0\n0\n124\n\n\nTotal\n168\n124\n52\n344\n\n\n\nThis table tells us:\nSummary of Correlation and Association Measures in R\nThis table shows which association or correlation measure to use depending on the types of variables, along with the appropriate R function.\n\n\n\n\n\n\n\n\n\nVariable Types\nMeasure\nInterpretation\nR Function / Package\n\n\n\n\nNumeric – Numeric\nPearson’s r\nStrength of linear relationship\ncor(x, y, method = \"pearson\")\n\n\nNumeric – Numeric\nSpearman’s ρ\nStrength of monotonic relationship\ncor(x, y, method = \"spearman\")\n\n\nNumeric – Numeric\nKendall’s τ\nRank-based measure; robust to ties\ncor(x, y, method = \"kendall\")\n\n\nOrdinal – Ordinal\nSpearman’s ρ or Kendall’s τ\nAssociation between ranks\nSame as above\n\n\nNominal – Nominal (2×2)\nPhi (φ) coefficient\nAssociation between two binary variables\npsych::phi(table)\n\n\nNominal – Nominal (k×m)\nCramér’s V\nStrength of association in contingency table\nrcompanion::cramerV(table) or DescTools::CramerV(table)\n\n\nNominal – Numeric\nEta (η) coefficient\nHow much variance in numeric var is explained by group\nDescTools::EtaSq(aov(y ~ group))\n\n\nBinary – Binary\nTetrachoric correlation\nFor binary variables assumed to reflect latent continuous variables\npsych::tetrachoric(table)$rho\n\n\nOrdinal – Nominal\n(No standard measure)\nUse visual summaries or grouped statistics\ntable(), ggplot2::geom_bar(), mosaicplot()\n\n\n\n\n\n\n\n5.1.7 🧭 Notes\n\nSpearman’s ρ and Kendall’s τ both work well for ranked data and are more robust to non-linear trends than Pearson’s r.\nCramér’s V is used for nominal data, and its values range from 0 to 1.\nEta squared (η²) measures how much of the variance in the numeric variable is explained by the group (nominal) variable.\nFor ordinal + nominal, no widely accepted coefficient exists; focus on visual summaries (e.g., bar charts or stacked plots).\n\n\n\n\n5.1.8 ✅ Example: What Should You Use?\n\n\n\nSituation\nUse\n\n\n\n\nHeight vs Weight\nPearson’s r\n\n\nExam Rank vs Satisfaction Level\nKendall’s τ\n\n\nEye Color vs Blood Type\nCramér’s V\n\n\nGender vs Income\nANOVA + Boxplot\n\n\nPlant Type vs Size Category\nCross-tab or Mosaic Plot\n\n\n\n\n\n\n\n\n\n\na16ced031b3d26382bbbaabba295b276d03f88e3\n\n\n\n\n\n\n\n\n\n5.1.9 Airquailty dataset\nWe will explore the airquality dataset, which contains daily measurement of air pollutants and weather conditions in New York City May to September 1973.\nStep 1 – How do ozone levels vary across different months in New York during the summer in 1973?\n\n\n\n\n\n\nNote\n\n\n\nThis question is specific and focused: One location (New York), One variable (ozone), one year and a defined time window (May to September)\n\n\nStep 2 – The airquality dataset is built into R. Load it with:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 3 – Check the size and dimension of the dataset\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 4 – run str()\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nCaution\n\n\n\nThere are some missing values (NA , Not Available) in the dataset.\n\n\nStep 5 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 6 –\nFirst, we count how many rows (i.e., records or observations) are in the dataset.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMissing values (denoted as NA in R) can lead to incorrect calculations or unexpected results. It is good practice to check how many missing values there are per variable.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nUsing `dplyr`\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis command uses across() to apply the same function to all columns, and sum(is.na(.)) to count the number of missing values per column.\n\n\n\n\n\n\nWhat is ~ ?\n\n\n\nThe ~ introduces an anonymous function — a function written inline without a name.\nThis\n\n~ sum(is.na(.))\n\n~sum(is.na(.))\n\n\nis a shorthand for\n\nfunction(x) sum(is.na(x))\n\nfunction (x) \nsum(is.na(x))\n\n\n\n\n\n\nOr, we can check how many rows are from July:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ncheck how many missing values are just in August.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 7 – According to the U.S. EPA, ozone levels above 70 parts per billion (ppb) may be considered unhealthy.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBased on the output, many days have safe levels and some days have extremely high ozone levels.\nStep 8 – Let us check the average ozone level by month:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis gives a quick overview of how ozone levels change over the summer. Let visualize it:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBased on the plot, we observe that July and August have the highest median ozone levels, showing that typical ozone concentrations were significantly higher during mid-summer. This suggests a seasonal pattern, likely driven by higher temperatures and increased sunlight, which promote the formation of ozone. The taller boxes and whiskers for these months reflect a greater variability in ozone levels, including several high outliers. In contrast, May and September show lower median values and less variation, possibly due to cooler temperatures and different atmospheric conditions. The median ozone level is June is slightly higher than in May but lower than in July, with a moderate level of variability. Overall, this seasonal trend is consistent with environmental science – ozone forms more easily in strong sunlight and warm conditions, which are more prevalent in July and August.\nStep 9 – Let us examine whether temperature or wind speed might help explain ozone patterns.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou may observe that there is a positive relationship between temperature and ozone and a negative relationship between wind speed and ozone. These patterns support common environmental science findings.\nStep 10 – Based on our findings, we might fit a simple regression model\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe might also refine our question: On which days was the ozone level unusually high, and what were the weather conditions on those days?\n\n5.1.9.1 Alternative question\nStep 1 – Which weather factor—temperature, wind, or solar radiation—has the strongest relationship with ozone levels during the summer of 1973 in New York?\nThis question is more analytical than descriptive, and focused on relationships between variables. So, we need to examine how ozone changes with respect to other variables.\nStep 2 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 3 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 4 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 5 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 6 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 7 –\nCheck how often ozone exceeds EPA’s 70 ppb guideline:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 8 –\nSince dplyr does not include a cor() function, we will compute correlations manually using summarise() and cor() from base R, keeping within tidy pipelines:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMore advanced:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBut sometimes, a simple code gives the more informative output:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nvisualize the relationships:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 9 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhen we check p-values and coefficients, we see Temp has strong and significant positive effect, Wind has strong and significant negative effect, and Solar.R has weaker effect but still contributes.\nStep 10 –\nNow we know temperature has the strongest effect on ozone levels:\n\nShould we check for nonlinear effects (e.g., does ozone spike at high temps)?\nWould a time-based model (e.g. by day or month) add insight?\nWhat is the temperature threshold where ozone exceeds 70 ppb?\nAre there interactions between variables (e.g. high temp + low wind)?\nDo results change if the we include time (e.g. month)?\n\nWe might now refine our question: How much does temperature need to rise before ozone levels exceed the 70 ppb threshold?\n\n\n\n\n\n\nNote\n\n\n\nAs Tukey emphasized, EDA is about “detecting the unexpected” and learning from the data before attempting to explain it.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAt the beginning, it is hard to ask the perfect question because you do not know the date well yet. But here is the secret:\n\n\n\n\n\n\nThe more questions you ask, the better your questions become.\n\n\n\n\n\nEach time you explore one idea, it leads to a new question. That is how you:\n\nDiscover patterns\nNotice surprises\nUnderstand your data more deeply\n\n\n\n\n\n\n\nCaution\n\n\n\nGood EDA is like this\n\nAsk a question\nMake a plot or summary\nLook at the result\nAsk a new, better question\nRepeat!\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDo not wait for the perfect question - just start. Exploration will lead you to insight.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html#principles-of-analytic-graphics",
    "href": "chapters/chapter4_EDA.html#principles-of-analytic-graphics",
    "title": "4  Exploratory data analysis (EDA)",
    "section": "5.2 Principles of Analytic Graphics",
    "text": "5.2 Principles of Analytic Graphics\nBased on (Tufte 2006), there are six key principles for designing informative and effective graphs.\n\n5.2.1 Principle 1 – Show Comparison\n\n\n\n\n\n\nShowing comparisons is really the basis of all good scientific investigation. (Peng 2012)\n\n\n\nGood data analysis always involves comparing things. A single number or result doesn’t mean much on its own. We need to ask:\n\n\n\n\n\n\nCompared to what?\n\n\n\nFor example, if we see that children with air cleaners had more symptom-free days, that sounds good. But how do we know the air cleaner made the difference? We only know that by comparing to another group of children who didn’t get the air cleaner. When we add that comparison, we can see that the control group didn’t improve — so the improvement likely came from the air cleaner.\n\n\n\nChange in symptom-free days with air cleaner. Source: @Peng2012\n\n\n\n\n\nChange in symptom-free days with air cleaner. Source: @Peng2012\n\n\n\n\n\nChange in symptom-free days by treatment group. Source: @Peng2012\n\n\nGood data graphics should always show at least two things so we can compare and understand what’s really happening.\n\n\n5.2.2 Principle 2: Show Causality and Explanation\nWhen making a data graphics, it is helpful to show why you think something is happening – not just what is happening. Even if you can not prove a cause, you can show your hypothesis of idea about how one thing might lead to another.\n\n\n\n\n\n\nIf possible, it is always useful to show your causal framework for thinking about a question. (Peng 2012)\n\n\n\n\nFor example, in #figEDA2 we saw that children with an air cleaner had more symptom–free days. But that alone does not explain why. A good follow-up question is: “Why did the air cleaner help?“ One possible reason is that air cleaners reduce fine particles in the air – especially in homes with smokers. Breathing in these particles can make asthma worse, so removing them might help children feel better. To show this, we can make a new plot.\n\n\n\nChange in symptom-free days and change in PM2.5 levels in-home. Source: Peng (2012)\n\n\nFrom the plot, we can see:\n\nChildren with air cleaners had more symptom-free days.\nTheir homes also had less PM2.5 after six months.\nIn contrast, the control group had little improvement.\n\nThis pattern supports the idea that air cleaners work by reducing harmful particles — but it is not final proof. Other things might also cause the change, so more data and careful studies are needed to confirm.\n\n\n5.2.3 Principle 3: Show Multivariate Data\n\n\n\n\n\n\nThe real world is multivariate. (Peng 2012)\n\n\n\nIn real life, most problems involve more than one or two variables. We call this multivariate data. Good data graphics should try to show these multiple variables at the same time, instead of reducing everything to just one number or a simple trend.\nLet us look at an example.\nThe mtcars dataset contains information about 32 car models from the 1970s. Each row is a car and each column is a variable. Some of these variables are: mpg: miles per gallon (fuel efficiency), wt: weight (in 1000 lbs), cyl: number of cylanders (engine size), hp: horse power, qsec: 1/4 mile time (acceleration), am: Transmission (0 = auto, 1 = manual). These variables help us to explore relationship between engin size, weight, fuel use, and more.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe want to know how a car’s weight affects its fuel efficiency (miles per gallon). We look at a simple scatter plot of these two variables.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHeavier cars tend to have lower fuel efficiency. But is weight the only thing affecting fuel use?\nCars also have different engine sizes, measured by cylinders (cyl). This affects both weight and fuel efficiency. To understand the relationship better, we add cyl as a third variable by coloring points by the number of cylinders.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, we see that cars with 4, 6, and 8 cylinders have different trends. The overall pattern changes once we include this third variable.\n\n\n\n\n\n\nCaution\n\n\n\nThe number of cylinders cofounds the relationship – it influences both weight and MPG.\n\n\nTo understand how the number of cylinders changes the relationship between weight and fuel efficiency, we can make separate plots for each cylinder group. This is called faceting.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nImportant\n\n\n\nSometimes, looking at groups separately helps us find clearer patterns that get lost in a big mixed dataset.\n\n\nEven, we can add the variable transmission(am) as an additional variable by using shapes or facet.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, each panel shows either automatic automatic or manual cars. Within each panel, colors show the number of cylinders.\nEven we can use facet_grid that allows us to split the plot into rows and columns based on two categorical variables – perfect for showing how relationships vary across combinations. We will use am in rows and cyl in columns.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis way, we can see how the relationship between weight and MPG changes in each group combination.\n\n\n5.2.4 Principle 4: Integrate Evidence — Keep the message clear\nWhen you make a graph, do not rely on points and lines to show your idea. You can also use: Numbers to give exact values, Words or short labels to explain what is happening, Pictures or diagrams to give context.\n\n\n\n\n\n\nImportant\n\n\n\nA good graph tells a complete story.\n\n\nUse all tools you need – not just the ones your software gives you easily.\n\n\n\n\n\n\nImportant\n\n\n\nThe goal is not to just make a nice picture, but to help people understand your message clearly.\n\n\n\n\n5.2.5 Principle 5: Describe and Document the Evidence\nA good graph tells a story – clearly and completely. That means it should include:\n\nA clear title\nLabels for the x-axis and y-axis\nUnits for measurement (e.g. ‘weights in 1000 lbs’)\nTime scale if needed (e.g. ‘daily’, ‘monthly’)\nwhere the data comes from (e.g., ‘New York’, ‘EPA’)\nsource of the data\n\n\n\n\n\n\n\nTip\n\n\n\nImagine someone looking only at your plot without reading anything else. Can they understand the main idea? if yes – your plot is doing a good job.\n\n\n\n\n\n\n\n\nNote\n\n\n\nEven if your graph is not final, it is a good habit to label things early. It helps you and others understand what is going on.\n\n\nFor example, instead of using\ntry this\nwhen we use ggplot2, it is better to add labs() like we did until now.\n\n\n5.2.6 Principle 6: Content is King\n\n\n\n\n\n\nImportant\n\n\n\nA beautiful plot means nothing if the question is weak or the data is poor.\n\n\nData graphics are only powerful when:\n\nthe question is clear and important\nthe data is high quality and relevant\nthe evidence supports the question\n\nNo chart or fancy design can fix a bad question or messy data. That is why it is crucial to start with a strong idea and only show what really matters to answer that idea.\n\n\n\n\n\n\nTip\n\n\n\nDo not just decorate your data – focus on the message!\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD ## Enter the world of EDA\nEDA is the process of looking at data to understand it before using statistics or models. It helps us to answer questions like: - what types of data do we have? - Are there missing or strange values? - What patterns can we see? - What kind of summary or visualization is helpful?\nBefore we start any analysis, we must know what kind of data we are working with. Variables can be broadly divided into numeric and categorical types.\n\n\n5.2.7 Types of Variables\n\n\n\n\n\n\nImportant\n\n\n\nKnowing the types of data matters because:\n\nDifferent statistics method are used for different variable types.\nSome graphs are only useful for some types of variables.\n\n\n\n\n5.2.7.1 Numerical variables\nNumeric variables describe quantities that we can measure — they answer questions like “how many?” or “how much?”. These variables are also called quantitative variables, and the data they produce are known as quantitative data. Briefly, they are numbers that can be measured. Also, we can do math with them.\n\n5.2.7.1.1 Continuous vs. Discrete\nNumeric variables can be:\nContinuous\n\nCan take any value on a number line, including decimals.\nRepresent real-world quantities.\n\nSome examples are: mass, age, temperature, time.\nContinuous variables can be positive only (like mass) or positive and negative (like change in temperature).\nDiscrete\n\nCan take only whole number values (no decimals).\nBased on counting.\n\nSome examples are: number of students, number of offspring, number of infected individuals\nA discrete variable cannot take values between integers — e.g., it makes no sense to have 2.5 individuals.\n\n\n5.2.7.1.2 Ratio vs. Interval\nNumeric variables can be further described based on the scale they are measured on: ratio or interval. This affects how we can interpret differences, proportions, and calculations.\nRatio\n\nHas a true zero point (zero means “none”).\nAllows all mathematical operations, including ratios.\n\nSome examples are: Height, Weight, Age, Income, mass.\nYou can say: “Tree A is twice as tall as Tree B.”\n\n\n\n\n\n\nWhat we can do:\n\n\n\n\nAll descriptive statistics\nUse histograms, box-plots, scatterplots\n\n\n\nInterval\n\nHas no true zero — the zero point is arbitrary.\nAllows meaningful differences, but not ratios.\n\nSome examples are Temperature (in Celsius of Fahrenheit), IQ scores, Calendar dates (e.g. year 1000 vs 2000)\nYou can say: “The difference between 20°C and 10°C is 10 degrees.” But not: “20°C is twice as hot as 10°C.”\n\n5.2.7.1.2.1 What we can do:\n\ncalculate mean, standard deviation\nplot histograms or line charts\n\n\n\n\n\n\n\nCaution\n\n\n\nThe difference between ratio and interval is about how the variable is measured, not what is being measured.\nFor example, Temperature can be measured on:\n\nThe Celsius scale that results in an interval scale (no true zero).\nThe Kelvin scale that results in a ratio scale (zero means no temperature, i.e. absolute zero).\n\nSo, the same thing (temperature) can be: an interval variable (in °C), or a ratio variable (in K), depending on the scale we use.\n\n\n\n\n\n\n5.2.7.2 Categorical Variables\nCategorical variables describe qualities or characteristics. They answer questions like “what type?” or “which group?”. These are also known as qualitative variables, and the data they produce are qualitative data.\n\n\n\n\n\n\nImportant\n\n\n\nCategorical variables do not have numeric meaning (even if coded as numbers!).\n\n\nCategorical variables fall into two groups:\nNominal Variables\n\nCategories have no logical order.\nOnly show labels or names.\n\nFor example, Gender (male, female), Blood group (A, B, AB, O), City (Lisbon, Porto, Faro, … ), color (red, blue, green)\n\n\n\n\n\n\nWhat we can do:\n\n\n\n\nCount frequencies (e.g. how many students are from each city)\nUse bar charts or pie charts\n\n\n\nOrdinal Variables\n\nCategories can be ordered or ranked.\nBut the distance between categories is not exact.\n\nFor example, Satisfaction (low, medium, high), education (primary, secondary, university), rank (1st, 2nd, 3rd), Academic grades (A, B, C)\n\n\n\n\n\n\nWhat we can do:\n\n\n\n\nCount frequencies\nCompare medians\nUse bar charts or ordered plots\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDo not use numbers to label categories.\nEven though categories can be coded as numbers (e.g., Male = 1, Female = 2), this can be confusing. It may suggest an order or imply mathematical operations that don’t make sense.\n\n\n\n\n\n\nTip: Use text labels like \"Male\" and \"Female\" instead of 1 and 2.\n\n\n\n\n\nSummary of Variable Types\n\n\n\n\n\n\n\n\n\n\n\nVariable Type\nScale\nOrdered?\nCan Measure Distance?\nCan Divide?\nExamples\n\n\n\n\nNominal\nCategorical\n❌ No\n❌ No\n❌ No\nGender, City, Color\n\n\nOrdinal\nCategorical\n✅ Yes\n❌ No (not exact)\n❌ No\nRank, Education, Likert\n\n\nInterval\nNumeric\n✅ Yes\n✅ Yes\n❌ No\nTemperature, Year\n\n\nRatio\nNumeric\n✅ Yes\n✅ Yes\n✅ Yes\nAge, Weight, Income\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are not sure about a variable, ask:\n\nDoes the variable have a natural order?\nCan we count it or measure it?\nDoes it have a true zero?\n\n\n\n\n\n\n5.2.8 Types of EDA\nEDA helps us answer important questions like:\n\nWhat are the most common values of a variable?\nHow much do observations differ from each other?\nIs one variable related to another?\n\nTo answer these questions, EDA uses two main tools:\n\n5.2.8.1 Descriptive Statistics\nDescriptive statistics give numerical summaries of a dataset. They describe the basic features of a variable, such as:\n\nCenter (e.g. mean or median)\nSpread (e.g. standard deviation or interquartile range)\nShape (e.g. skewness)\nExtremes (e.g. minimum, maximum, or outliers)\n\nThese summaries help us compare groups or make initial conclusions.\nFor example, the mean tells us what the “typical” value might be.\nBut: A few numbers can’t tell the whole story.\n\n\n5.2.8.2 Graphical Summaries\nGraphs help us see patterns, spot outliers, and understand distributions.\nCommon EDA plots include: - Histograms – show the shape and spread of a numeric variable - Boxplots – show center, spread, and outliers - Bar charts – show frequencies of categories - Scatterplots – show relationships between two numeric variables\nGraphs are easier to understand than tables of numbers. They help us and others see what’s going on in the data.\n\n\n\n5.2.9 A Review on Descriptive Statistics\nStatisticians have created more precise terms to describe the patterns in data, called descriptive statistics.\nThe two most important features of a numeric variable’s distribution are:\n\nCentral tendency: What is a typical value?\nDispersion: How spread out are the values?\n\n\n\n5.2.10 Central Tendency\nA measure of central tendency tells us what a “typical” value looks like in the data. There are three main measures: Mean, Median, Mode\n\n5.2.10.1 Arithmetic Mean (Average)\n[We should add the definition]\nfunction mean()\nSometimes, we use na.rm = TRUE to ignore missing values.\n\n\n\n\n\n\nNote\n\n\n\nBe careful: The mean is sensitive to extreme values (outliers).\nFor example, using the mean to describe income is misleading because a few very high earners can pull the average up.\n\n\n\n\n5.2.10.2 Median\n[We should add the definition]\n\nThe middle value when data are sorted.\nMore robust than the mean when data are skewed.\n\nfunction median()\n\n\n5.2.10.3 Mode\n\nThe most frequent value in the data.\nConceptually useful, but hard to estimate in numeric data.\nUsed more often for categorical variables.\n\n\n\n\n5.2.11 Dispersion\nDispersion tells us how much values differ from one another. A dataset where all values are similar has low dispersion; one with a wide range of values has high dispersion.\nA measure of dispersion tells us how spread out the values of a variable are.\nDispersion measures help us understand how much the data varies — whether most values are close together or far apart.\nIf one distribution is more dispersed than another, it means it includes a wider range of values.\nThere are many ways to measure dispersion, but the most common ones are:\n\nVariance – how far values are from the mean (on average, squared)\nStandard deviation – the square root of the variance, easier to interpret\nInterquartile range (IQR) – the spread of the middle 50% of the values\n\n\n\n\n\n\n\nTip\n\n\n\nEach of these gives us a different way to understand how variable the data is.\n\n\n\n5.2.11.1 Variance\n\nMeasures the average squared distance from the mean.\nHard to interpret because it’s in squared units.\n\nThe sample variance, written as s², is: the average of the squared differences between each value and the mean.\nKey facts about variance:\n\nIt is always non-negative.\nA larger variance means the data are more spread out.\nA variance of zero means all values are exactly the same.\n\nIn R, we can calculate it using the var() function\nvar(penguins$body_mass_g, na.rm = TRUE)\n## [1] 643131.1\nThat’s a large number — but what does it really tell us?\n\n\n\n\n\n\nImportant\n\n\n\nIt’s hard to say if that number is “big” or “small.”\nThat’s because variance is in squared units, which are difficult to interpret directly.\nFor example, here it’s in grams squared, which is not easy to relate to actual body mass.\n\n\nWhy does variance matter?\nVariance is an important quantity in statistics. Many statistical tests use changes in variance to compare groups or detect effects.\nBut in EDA, we rarely use the variance.\nInstead, we often prefer the standard deviation, because it is easier to understand.\nThis is hard to interpret directly — that is why we usually prefer the standard deviation.\n\n\n5.2.11.2 Standard Deviation\nA more useful measure of dispersion is the standard deviation, usually written as s.\nThe standard deviation is: the square root of the variance.\nBecause it is not squared, it has the same unit as the original data.\nThis makes it easier to interpret than the variance.\n\nSquare root of the variance.\nHas the same unit as the variable.\nMore interpretable than variance.\n\nLike the mean, the standard deviation is sensitive to outliers.\nIn R, we use the sd() function:\nsd(penguins$body_mass_g, na.rm = TRUE)\n## [1] 801.9545\nThis tells us that most penguins have body mass values within about 802 grams of the mean.\n\n\n\n5.2.12 Why use standard deviation?\n\nIt’s on the same scale as the variable (grams, not grams²).\nIt gives us a sense of spread we can understand.\nIt is more commonly used in EDA than variance.\n\nBut be careful: Like the mean, the standard deviation is sensitive to:\n\nSkewed distributions\nOutliers (extreme values)\n\n\n5.2.12.1 Interquartile Range (IQR)\nThe interquartile range (IQR) is another measure of dispersion.\nIt is especially useful in EDA because it is robust to outliers and skewed data.\n\nMeasures the range of the middle 50% of the data.\nRobust to outliers.\nPreferred in exploratory work.\n\n\n\n\n\n\n\nWhat Are Quartiles?\n\n\n\nTo understand the IQR, we need to define quartiles:\n\nQ1 (1st quartile): 25% of values are below this point.\nQ2 (2nd quartile): The median, 50% below, 50% above.\nQ3 (3rd quartile): 75% of values are below this point.\n\n\n\nThe IQR measures the spread of the middle 50% of the data. It is defined as:\n\\[\n\\text{IQR} = \\text{Q3} - \\text{Q1}\n\\]\nwhere\n\nQ1 = first quartile = 25th percentile\n\nQ3 = third quartile = 75th percentile\n\nSo, the IQR is the range between the 25% and 75% marks.\nThis range contains the middle half of the dataset.\nThe IQR is used to build boxplots (a.k.a. “box-and-whisker plots”).\nThe wider the IQR, the more spread out the middle values are.\nWhy is the IQR useful?\n\nIt does not depend on extreme values.\nIt gives a good summary of the central spread.\nIt is preferred in EDA for skewed or messy data.\n\nIn R, we can calculate the IQR like this:\nIQR(penguins$body_mass_g, na.rm = TRUE)\n## [1] 1200\nThis means the middle 50% of penguins have body masses that differ by 1200 grams.\n\n\n\n\n5.2.13 The Boxplot Connection\nThe boxplot (also called a “box-and-whisker plot”) is based on the IQR.\n\nThe box shows Q1 to Q3.\nThe line inside the box is the median (Q2).\nThe whiskers extend to show the general range of the data (excluding outliers).\n\nWhat About Skewness?\nSkewness describes the asymmetry of a distribution:\n\nRight-skewed (positive skew): Tail on the right.\nLeft-skewed (negative skew): Tail on the left.\nSymmetric: Looks like a bell curve.\n\nWhat Are Quartiles?\nTo understand the IQR, we need to define quartiles.\nQuartiles divide your data into four equal parts:\n\n\n\n\n\n\n\nQuartile\nMeaning\n\n\n\n\nQ1\nFirst quartile (25%) — one quarter of the data is below this value\n\n\nQ2\nSecond quartile (50%) — this is the median\n\n\nQ3\nThird quartile (75%) — three quarters of the data are below this value\n\n\n\nThese values help summarize the distribution, especially for non-symmetric data.\n\n5.2.13.1 Categorical Variables\nDescriptive statistics for categorical variables aim to answer: “How often does each category occur?”\nSince categorical variables have a limited number of values, we can simply count how many times each category appears.\npenguins %&gt;% count(species)\nCentral Tendency?\nWe can use the mode — the most common category — as a measure of central tendency for categorical variables.\n\n\n\n\n\n\nTip\n\n\n\nThe median only makes sense for ordinal variables (e.g. “Low”, “Medium”, “High”), where there is a natural order.\n\n\nDispersion?\nThere are some measures of dispersion for categorical variables, but they are:\n\nLess intuitive\nRarely used in EDA\n\nSo, we usually don’t use them in exploratory analysis.\n\n\n5.2.13.2 Associations\nStatisticians have created different ways to quantify relationships between variables. These are often called measures of association.\nThe most common type is a correlation coefficient, which gives a number that tells us: How strong is the relationship between two variables?\nAre “association” and “correlation” the same?\nThese terms are often used interchangeably, but they are not exactly the same.\n\nAssociation is the general idea that two variables are related in some way.\nCorrelation is a specific, mathematical way to describe an association — using a correlation coefficient.\n\nSo:\n\nAll correlations are associations, but not all associations are correlations.\n\nIn EDA, we often want to ask:\n\n“Is one variable associated with another?”\n\nThis can be for:\n\nTwo numeric variables\nTwo categorical variables\nOne numeric and one categorical\n\nPairs of Numeric Variables\nThe most common way to measure association between two numeric variables is:\nPearson’s Correlation Coefficient (r)\n\nMeasures strength and direction of a linear relationship.\nValues range from –1 to +1:\n\n+1 → perfect positive linear association\n–1 → perfect negative linear association\n0 → no linear relationship\n\n\nImportant: Pearson’s r only measures linear relationships.\nIf the relationship is curved, r can be misleading.\nSee Anscombe’s Quartet for famous examples.\nRank Correlations: For Non-linear Relationships\nIf the relationship is not linear, use a rank correlation:\n\n\n\nMethod\nNotes\n\n\n\n\nSpearman’s ρ\nMore sensitive to outliers\n\n\nKendall’s τ\nBetter for ordinal data, but slower\n\n\n\nThese methods: - Convert data into ranks (1st, 2nd, 3rd, …) - Measure how well the ranks agree between two variables\nPairs of Categorical Variables\nFor two categorical variables, we ask:\n\n🔍 “Are certain combinations of categories more or less common than expected?”\n\nThe most basic tool is the contingency table, created using xtabs().\nWhat about correlation?\nRank correlations like Spearman’s ρ and Kendall’s τ can also be used with ordinal variables.\nFor nominal variables, specialized measures exist, like:\n\nCramér’s V\nChi-square-based measures\n\nBut these are rarely used in EDA.\nMost people rely on tables and plots (like mosaic plots or stacked bar charts) for categorical associations.\n\n\n\n\n\n\nNote\n\n\n\nFor any data variable,\n\ncheck the meaning of the variable\ncheck expected data type, the expected set of values, the used units when applicable\ncheck the available information to find semantic relationship between variables\n\nHierarchies (i.e. Year, Month, Day)\nComponents of a whole (i.e. latitude and longitude)\n\nNumber of the NULL values (and percentage)\nNumber of Invalid values (and percentage)\nNumber of valid values (and percentage)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor Nominal scales - Arbitrary number of values\n\nNumber of distinct values\nnumber of distinct values / number of rows in the dataset\n\nDo they have the role of a key?\n\nCheck if there is any known or visible structure in the values\nCheck if they are meaningful or friendly\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor Nominal Scales – Large number of categories\n\nNumber of distinct values\nFrequency distributions (absolute and relative)\nBar charts with the categories sorted by decreasing order of the frequency\n\nTop N\n\nPossibly create a new attribute\n\n\n\n======= ## &gt;&gt;&gt;&gt;&gt;&gt;&gt; a16ced031b3d26382bbbaabba295b276d03f88e3\n\n\n\n\nMidway, Steve. 2022. Data Analysis in r. https://bookdown.org/steve_midway/DAR/.\n\n\nPeng, Roger D. 2012. Exploratory Data Analysis with R. https://bookdown.org/rdpeng/exdata/.\n\n\nTufte, Edward. 2006. Beautiful Evidence. Graphics Press LLC.\n\n\nTukey, John Wilder. 1977. Exploratory Data Analysis. Addison-Wesley Publishing Company.",
    "crumbs": [
      "EDA",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5_logistic_regression.html",
    "href": "chapters/chapter5_logistic_regression.html",
    "title": "5  Logistic Regression",
    "section": "",
    "text": "5.1 Modelling Probabilities\nA binary classification problem is one where the outcome variable can take on only two possible values. For example:\nFor this kind of problem, the dataset used to fit the model is composed of observations \\((\\mathbf{x}_1, y_1)\\), \\((\\mathbf{x}_2, y_2)\\), …, \\((\\mathbf{x}_n, y_n)\\) where each \\(y_i\\) is usually labelled 0 (failure, no event, …) or 1 (success, event, …) and \\(\\mathbf{x}_i\\) is a vector of input variables.\nThe task at hand is to build a model that, given an arbitrary \\(\\mathbf{x}\\), will predict the probability that \\(y = 1\\).\nWhy not use a linear regression model to predict the probability that \\(y = 1\\)? After all, linear regression is a well-known and widely used method.\nThere are some problems with this approach:",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5_logistic_regression.html#modelling-probabilities",
    "href": "chapters/chapter5_logistic_regression.html#modelling-probabilities",
    "title": "5  Logistic Regression",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\nthe output values are never (with probability 1) exactly 0 or 1, the values that are taken by \\(y\\);\neven considering a threshold (e.g. 0.5) to classify the predicted values into 0 or 1, this would be mostly an arbitrary chosen value.",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5_logistic_regression.html#sigmoid-functions",
    "href": "chapters/chapter5_logistic_regression.html#sigmoid-functions",
    "title": "5  Logistic Regression",
    "section": "5.2 Sigmoid Functions",
    "text": "5.2 Sigmoid Functions\nInstead of a linear function, we can opt to model the probability of success of the event described by \\(y\\), i.e \\(\\operatorname{P}(Y=1)\\). This is a more sound and natural way to model the prediction of a random binary event. In order to do this, we can use sigmoid functions. A sigmoid function \\(f(x)\\) is a function having an “S” shaped curve (sigmoid curve) with the following properties:\n\n\\(0&lt;f(x)&lt;1\\);\n\\(\\displaystyle \\lim_{x \\to -\\infty} f(x) = 0\\), \\(\\displaystyle \\lim_{x \\to +\\infty} f(x) = 1\\);\n\\(f'(x)&gt;0\\).\n\nA common sigmoid function is the logistic function, defined as:\n\\[\nf(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}}.\n\\]\nA logistic function can be used to model the probability that \\(y = 1\\) given \\(\\mathbf{x}\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis function will play a central row in the logistic regression model, as we will see briefly.",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5_logistic_regression.html#logistic-model",
    "href": "chapters/chapter5_logistic_regression.html#logistic-model",
    "title": "5  Logistic Regression",
    "section": "5.3 Logistic Model",
    "text": "5.3 Logistic Model\nThe first random variable that comes to mind in order to model a random binary event is the Bernoulli random variable, which is a discrete random variable that takes value 1 with probability \\(p\\) and value 0 with probability \\(1-p\\). The parameter \\(p\\) is the probability of success of the event described by the random variable, and it’s probability function is\n\\[\np(y) = p^y (1-p)^{1-y}, \\quad y \\in \\{0, 1\\}.\n\\] The expected value of a Bernoulli random variable is \\(\\operatorname{E}[Y] = p\\) and it’s variance is \\(\\operatorname{V}(Y) = p(1-p)\\).\n\n\n\n\n\n\nNote\n\n\n\nA natural extension is the Binomial random variable, which is the sum of \\(n\\) independent Bernoulli random variables with the same parameter \\(p\\). The parameter \\(p\\) is again the probability of success of the event described by the random variable, and it’s probability function is\n\\[\np(y) = \\binom{n}{y} p^y (1-p)^{n-y}, \\quad y \\in \\{0, 1, ..., n\\}.\n\\]\n\n\nAnother way of expressing the probability of an event is through the odds of the event, defined as the ratio between the probability of the event and the probability of it’s complementary event:\n\\[\n\\operatorname{odds} = \\frac{p}{1-p} = \\frac{1}{1-p^{-1}}, \\quad p \\in ]0, 1[.\n\\]\nThe odds are a number between 0 and \\(+\\infty\\). If the odds are equal to 1, then the event and it’s complementary are equally likely. If the odds are greater than 1, then the event is more likely than it’s complementary event, and vice versa. It’s easy to see that odds are monotonically increasing with respect to \\(p\\):\n\\[\n{(\\operatorname{odds})}' = \\frac{1}{(1-p)^2} &gt; 0.\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOdds are an alternative way of expressing probabilities, still being relatively intuitive concerning interpretation. Nevertheless, they are still confined to the interval \\(]0, +\\infty[\\), because\n\\[\n\\lim_{p \\to 0^+} \\frac{1}{1-p^{-1}} = 0, \\quad \\lim_{p \\to 1^-} \\frac{1}{1-p^{-1}} = +\\infty.\n\\]\nIf we want to have a represantation of probabilities that spans the whole real line, we can use the log-odds, also called logit function, defined as\n\\[\n\\operatorname{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right), \\quad p \\in ]0, 1[.\n\\]\nThis creates a one-to-one mapping between the interval \\(]0, 1[\\) and the whole real line \\(\\mathbb{R}\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe logit function is also monotonically increasing with respect to \\(p\\):\n\\[\n{\\big(\\operatorname{logit}(p)\\big)}' = \\frac{1}{p(1-p)} &gt; 0.\n\\]\nThis representation of probabilities is well suited to be used in a regression model, because it spans the whole real line. In fact, in a logistic regression model, we assume that the log-odds of the probability that \\(y = 1\\) is a linear combination of the input variables: \\[\n\\operatorname{logit}\\big(\\operatorname{P}(Y=1|\\mathbf{x})\\big) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p = \\mathbf{x}^T \\boldsymbol{\\beta} = \\eta (\\mathbf{x}),\n\\] where \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, ..., \\beta_p)^T\\) is the vector of parameters to be estimated.\nNote that the logit function is the inverse of the logistic function! Hence, we can rewrite \\(\\operatorname{P}(Y=1|\\mathbf{X}=\\mathbf{x})\\) as\n\\[\n\\operatorname{P}(Y=1|\\mathbf{x}) = \\frac{e^{\\eta(\\mathbf{x})}}{1 + e^{\\eta(\\mathbf{x})}} = \\frac{1}{1 + e^{-\\eta(\\mathbf{x})}}.\n\\tag{5.1}\\]\n\n\n\n\n\n\nWarning\n\n\n\nEquation 5.1 describes a model for the expected value of the random variable \\(Y\\) given \\(\\mathbf{x}\\), that follows a Bernoulli distribution with parameter \\(p = \\operatorname{P}(Y=1|\\mathbf{x})\\). It does not describe a model for \\(Y\\) itself, which is a discrete random variable taking values 0 or 1.",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  }
]