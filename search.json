[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics and Information Systems",
    "section": "",
    "text": "1 Note\nThis interactive book is designed as a companion to the course Systems Information and Statistics, taught at the NOVA School of Science and Technology (NOVA FCT) in Lisbon, Portugal.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Note</span>"
    ]
  },
  {
    "objectID": "index.html#emails",
    "href": "index.html#emails",
    "title": "Statistics and Information Systems",
    "section": "1.1 Emails",
    "text": "1.1 Emails\nMina Norouzirad\nMiguel Fonseca\nGracinda Guerreiro",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Note</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html",
    "href": "chapters/chapter4_EDA.html",
    "title": "2  Exploratory data analysis (EDA)",
    "section": "",
    "text": "2.1 Plan the Study\nStatProcess\n\ncluster_problem\n\n\ncluster_plan\n\n\ncluster_data\n\n\ncluster_analysis\n\n\ncluster_conclusion\n\n\n\nP1\n\nP\n\n\n\nBOX1\n\n\n\n\n\n\n\nProblem\n• Write down study objectives\n• Identify target/sample population\n• Define the variates\n\n\n\nP1-&gt;BOX1\n\n\n\n\nP2\n\nP\n\n\n\nP1-&gt;P2\n\n\n\n\n\nBOX2\n\n\n\n\n\n\n\nPlan\n• Plan data collection methods\n• Calculate sample size\n• Consider analysis options\n\n\n\nP2-&gt;BOX2\n\n\n\n\nD\n\nD\n\n\n\nP2-&gt;D\n\n\n\n\n\nBOX3\n\n\n\n\n\n\n\nData\n• Collect data per plan\n• Note any deviations\n\n\n\nD-&gt;BOX3\n\n\n\n\nA\n\nA\n\n\n\nD-&gt;A\n\n\n\n\n\nBOX4\n\n\n\n\n\n\n\nAnalysis\n• Analyze data per plan\n\n\n\nA-&gt;BOX4\n\n\n\n\nC\n\nC\n\n\n\nA-&gt;C\n\n\n\n\n\nBOX5\n\n\n\n\n\n\n\nConclusion\n• Draw conclusion in context\n\n\n\nC-&gt;BOX5",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html#after-collecting-the-data",
    "href": "chapters/chapter4_EDA.html#after-collecting-the-data",
    "title": "2  Exploratory data analysis (EDA)",
    "section": "2.2 After collecting the data",
    "text": "2.2 After collecting the data\n\n\n\n\n\n\n\nAfterCollecting\n\n\nS1\n\nReview the hypotheses\n•\nReview the research questions\n•\nConsider sub-questions\n\n\n\nS2\n\nProcess the raw data\n•\nSelect a suitable statistics software\n•\nConvert the data into an acceptable format\n\n\n\nS1-&gt;S2\n\n\n\n\n\nS3\n\nExplore the data\n•\nDescriptive summaries\n•\nData visualization\n\n\n\nS2-&gt;S3\n\n\n\n\n\nS4\n\nAnalyze the data\n•\nInferential analysis\n•\nPrediction\n\n\n\nS3-&gt;S4\n\n\n\n\n\nS5\n\nReport the results\n•\nInterpret the results in the context of the study\n\n\n\nS4-&gt;S5",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html#what-is-eda",
    "href": "chapters/chapter4_EDA.html#what-is-eda",
    "title": "2  Exploratory data analysis (EDA)",
    "section": "2.3 What is EDA?",
    "text": "2.3 What is EDA?\nExploratory data analysis (EDA) was first formally introduced by Tukey (1977) in his influential book, Exploratory Data Analysis.\n\n\n\n\n\n\n\n\n\n\nSince then, its importance has grown significantly, especially in recent years, for several key reasons (Midway 2022):\ni) data is being produced faster and in larger volumes than ever before,\nii) modern computing and software tools make it easier to explore, clean, and visualize data in meaningful ways,\niii) contemporary statistical models are often complex and assumption-dependent, requiring us to thoroughly understand the data before applying formal techniques.\nThe EDA may not be fully described in concrete terms, but most data analysts and statisticians know it when they see it.\n\n\n\n\n\n\nImportant\n\n\n\nThe EDA is important because it helps researchers make thoughtful decisions about which ideas are worth exploring. Sometimes, the data clearly show that a particular question does not have enough support to be studied further—at least not with the current evidence.\n\n\nThe main goals of EDA are:\n\nTo suggest hypotheses about what might be causing the patterns or relationships observed in the data,\nTo guide the choice of appropriate statistical tools or models by helping you understand the structure of the data,\nTo assess key assumptions that must be checked before applying formal statistical analysis (e.g., linearity, normality, independence),\nTo provide a basis for further data collection, by highlighting gaps, inconsistencies, or areas where more information is needed.\n\n\n\n\n\n\n\nNote\n\n\n\nEDA is not typically the final stage of analysis. Rather, it serves as a transitional step between raw data and formal modeling. The insights gained through EDA guide decisions about which models to use, which variables to consider, and which data issues to address.\n\n\n\n\n\n\n\n\nImportantEDA Checklist\n\n\n\nRoger D. Peng in his book (Peng 2012) provide this checklist for conducting EDA.\n\nStart with a clear question: Before you begin EDA, take time to define exactly what you want to find out. A clear question or hypothesis gives your analysis purpose and helps you stay focused along the way.\nLoad your data carefully: Make sure your dataset is fully and correctly loaded into your analysis tool (e.g., R or Python). This first step is essential—it sets the stage for everything you will do next.\nTake a first look at the data: Check that the file type, structure, and layout are what you expected. Make sure everything is organized in a way that works for your analysis.\nUse str() to Peek Inside the Dataset: In R, it will get a quick summary: number of observations (rows), number and names of variables (columns), variable types (e.g., numeric, character, factor), and a preview of the data values\nLook at the Beginning and End of Your Data: Use functions such as head() and tail() to view the first and last few rows. This visual check can help to detect issues such as incorrect headers, blank rows, or unusual formatting.\nCheck the Number of Rows (“\\(n\\)“): Make sure to verify how many observations (rows) are in your dataset. Compare this to what you expected from the original. source. If the number is too high or too low, there may be missing values, duplicate entries, or extra rows (e.g. duplicates or blank lines).\nValidate with an External Source: When possible, compare part of your dataset with a trusted external source, such as official statistics or published reports. This helps confirm the accuracy and reliability of your data.\nTry the Simple Solution First: Start by basic methods–such as summaries, tables, or visualizations– to explore and answer your question. Simple tools can often reveal key patterns or issues. Use more complex techniques only of necessary.\nChallenge Your Findings: Once you find a result, pause and ask yourself: Does this make sense? Check your assumptions and consider possible errors or missing information. Being critical helps make your results stronger and more trustworthy.\nDecide What to Do Next: Use the insights from your EDA to guide your next steps. You may decide to collect more data, use new methods, or refine your question. Sometimes , your initial findings are already enough to answer your main question.\n\nIn Chapter 4 of Midway (2022), interested reader can find some comments for each step.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nEDA is the single most important task to conduct at the beginning of every data science project.\n\n\n\n\n\n\n\n\nTip\n\n\n\nEDA is like exploring a new place – you do not know what you will find until you start looking.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html#enter-the-world-of-eda",
    "href": "chapters/chapter4_EDA.html#enter-the-world-of-eda",
    "title": "2  Exploratory data analysis (EDA)",
    "section": "2.4 Enter the world of EDA",
    "text": "2.4 Enter the world of EDA\n\n\n\n\n\n\n\nEDA\n\n\nEDA\n\nExploratory Data Analysis\n\n\n\nView\n\nView data\n\n\n\nEDA-&gt;View\n\n\n\n\n\nSummary\n\nSummary\nstatistics\n\n\n\nEDA-&gt;Summary\n\n\n\n\n\nGraphs\n\nBasic\nGraphs\n\n\n\nEDA-&gt;Graphs\n\n\n\n\n\nTests\n\nBasic\nTests\n\n\n\nEDA-&gt;Tests\n\n\n\n\n\nV1\n\nObservation number\nVariable number\nVariable type\nVariable category\nStructure\n\n\n\nView-&gt;V1\n\n\n\n\n\nS1\n\nMean\nMedian\nMode\nRange\nVariance\nStandard deviation\nOutliers\nMissing data\n\n\n\nSummary-&gt;S1\n\n\n\n\n\nG1\n\nHistogram\nBar plot\nBoxplot\nScatter-plot\nQQ-plot\n\n\n\nGraphs-&gt;G1\n\n\n\n\n\nT1\n\nCheck assumptions\nT-tests\nCorrelations\nANOVA\nLinear model\n\n\n\nTests-&gt;T1\n\n\n\n\n\n\n\n\n\n\nNow it is time to start working with data directly. To do that, we must first understand how data are structured and what types of variables we are dealing with.\nMost datasets are organized in a rectangular format — like a spreadsheet — where each row represents one observation (e.g. a person, object, or experiment), and each column represents a variable (e.g. name, age, group, result).\n\n\n\nAdapted from Slides of ‘EDA Module I: A Bird’s Eye View’ by Dr. Mark Williamson\n\n\n\nAs an example, let us explore the mpg dataset with a few simple commands in R:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n2.4.1 Types of Variables\nVariables represent the information we collect in data analysis. They can be broadly divided into categorical and numeric types.\n\n2.4.1.1 Categorical Variables\nCategorical variables describe qualities or characteristics. They answer questions like “what type?” or “which group?”.\n\nAlso called qualitative variables, and they produce qualitative data.\nThey do not have numeric meaning (even if represented by numbers)\n\nThey fall into two main groups:\n\nNominal Variables\n\nCategories have no logical order.\nThey are simply labels or names.\n\n\nFor example, Gender (male, female), Blood group (A, B, AB, O), City (Lisbon, Porto, Faro, … ), color (red, blue, green), the types of drinks at Starbucks, a person’s eye color.\n\nOrdinal Variables\n\nCategories can be ordered or ranked.\nHowever, the distance between categories is not exact or consistent\n\n\nFor example, Satisfaction (low, medium, high), education (primary, secondary, university), rank (1st, 2nd, 3rd), Academic grades (A, B, C)\n\n\n\n\n\n\nNote\n\n\n\nAvoid coding categories with numbers (e.g., Male = 1, Female = 2), as this may wrongly suggest order or allow meaningless calculations.\nTip: Use clear text labels like \"Male\" and \"Female\".\n\n\nIn R, categorical variables are usually stored as factors. But sometimes they might appear as character or even numeric, especially if the dataset is not clean or comes from an external file.\nYou can check whether a variable is a factor using:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo see the possible categories (called levels) of a factor, use:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIf a variable is not a factor but should be treated as one, you can convert it like this:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nTip\n\n\n\nSometimes, datasets are not ready for analysis and categorical variables may be incorrectly coded as numeric or character. Always check and convert them to factor if needed before analysis or plotting.\n\n\nYou can use the following commands to help:\nis.factor(mpg$class)\nlevels(mpg$class)\nmpg$class &lt;- factor(mpg$class)\n\nExerciseHintSolutions\n\n\nThe mpg dataset contains information about different car models, including manufacturer, engine size, fuel efficiency, and more.\nYour task is to:\n\nList all the categorical variables in the dataset.\nUse is.factor() to check which variables are already treated as factors (categorical).\nIf a variable is not a factor but should be (e.g., class or trans), convert it using factor().\nUse levels() to inspect the categories.\nFinally, fill in the table to classify each categorical variable as nominal or ordinal.\n\nFill Table to Complete\n\n\n\n\n\n\n\n\n\nVariable\nIs it categorical? (Yes/No)\nScale (Nominal / Ordinal)\nJustify your answer\n\n\n\n\nmanufacturer\n\n\n\n\n\nmodel\n\n\n\n\n\ntrans\n\n\n\n\n\ndrv\n\n\n\n\n\nfl\n\n\n\n\n\nclass\n\n\n\n\n\nyear\n\n\n\n\n\ndispl\n\n\n\n\n\n\n\n\n\n\nUse `str(mpg)` to inspect all variable types.\nUse `is.factor()` to check factor status.\nUse `levels()` to explore categories.\nVariables like `class` and `fl` are good candidates for factors.\nNominal= unordered categories; Ordinal= categories with a meaningful order.\n\n\n\n\nDo it yourself :)\n\n\n\n\n\n\n2.4.1.2 Numerical variables\nNumeric variables describe quantities that can be measured. They answer questions like “how many?” or “how much?”.\n\nAlso called quantitative variables, and they produce quantitative data.\nThey are numbers we can measure, and we can meaningfully perform mathematical operations on them.\nUsually, they have a measurement unit.\n\nNumerical (quantitative) variables can be classified in two complementary ways:\n\nContinuous vs. Discrete – How values occur\nRatio vs. Interval – How the scale is defined\n\nContinuous vs. Discrete\n\nContinuous Variables\n\nCan take any value on a number line, including decimals.\nRepresent real-world quantities.\nThey may be restricted to positivevalues (e.g., mass) or include negatives (e.g., temperature changes).\n\nSome examples are: mass, age, temperature, time.\nDiscrete Variables\n\nTake only whole number values (no decimals).\nBased on counting.\nCannot take values between integers (e.g., 2.5 students does not make sense).\n\n\nSome examples are: number of students, number of offspring, number of infected individuals\nRatio vs. Interval\nNumerical variables can also be distinguished by the scale of measurement, which affects how we interpret differences, proportions, and ratios.\n\nRatio Variables\n\nHave a true zero point (zero means “none”).\nAllows all mathematical operations, including ratios and proportions.\nInterpretation: “Tree A is twice as tall as Tree B.”\n\n\nSome examples are: Height, Weight, Age, Income, mass.\n\nInterval Variables\n\nDo not have a true zero (zero is arbitrary).\nAllows meaningful differences, but not ratios.\nInterpretation: “The difference between 20 °C and 10 °C is 10 degrees,” but not “20 °C is twice as hot as 10 °C.”\n\n\nSome examples are Temperature (in Celsius of Fahrenheit), IQ scores, Calendar dates (e.g. year 1000 vs. 2000)\n\n\n\n\n\n\nNote\n\n\n\nSame Quantity, Different Scales\nThe distinction between ratio and interval depends on how the variable is measured, not on what is being measured.\nFor example,\n\nTemperature in Celsius → interval scale (arbitrary zero).\nTemperature in Kelvin → ratio scale (absolute zero).\n\nSo, the same phenomenon (temperature) can be an interval variable (in °C), or a ratio variable (in K).\n\n\n\n\n\n\n\n\nTip\n\n\n\nVariable Types Are Not Always Fixed\nYou cannot determine a variable’s type just from its name — it depends on how the data is recorded.\nExample: Age\n\nIf recorded as an exact value (e.g., 25, 35.5, 80), it is a numerical variable.\nIf recorded in categories (e.g., &lt;20, 21–25, 80+), it is a categorical variable.\n\n\n\nIn R, numeric variables include both:\n\nIntegers (whole numbers, like 1, 2, 3), and\nDoubles or floats (decimal values, like 3.14, 5.0)\n\nYou can check a variable’s numeric type using:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn R, most numeric data are stored as double even if they look like integers.\nYou can convert to numeric explicitly if needed:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExerciseHintSolutions\n\n\nThe mpg dataset includes several numeric variables related to fuel economy and engine size.\nYour task is to:\n\nIdentify the numeric variables in the dataset using is.numeric().\nCheck if the numeric variable is an integer (is.integer()) or a float (is.double()).\nComplete the table and classify each variable as:\n\nDiscrete (countable values)\nContinuous (can take any value within a range)\n\nComment on any variables that could be treated as categorical instead, depending on context.\n\n\n\n\n\nUse str(mpg) and summary(mpg) to explore data.\nUse is.numeric() to detect numeric variables.\nUse is.integer() and is.double() to check type.\nDiscrete = exact counts; Continuous = measured values\n\n\n\n\nDo it yourself :)\n\n\n\n\nSummary of Variable Types\n\n\n\nSource: https://datatab.net/tutorial/level-of-measurement\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Type\nScale\nOrdered?\nCan Measure Distance?\nCan Divide?\nExamples\n\n\n\n\nNominal\nCategorical\n❌ No\n❌ No\n❌ No\nGender, City, Color\n\n\nOrdinal\nCategorical\n✅ Yes\n❌ No (not exact)\n❌ No\nRank, Education, Likert\n\n\nInterval\nNumeric\n✅ Yes\n✅ Yes\n❌ No\nTemperature, Year\n\n\nRatio\nNumeric\n✅ Yes\n✅ Yes\n✅ Yes\nAge, Weight, Income\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are not sure about a variable, ask:\n\nDoes the variable have a natural order?\nCan we count it or measure it?\nDoes it have a true zero?\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nKnowing the types of data matters because:\n\nDifferent statistical methods apply to different variable types.\nSome graphs and visualizations only make sense for some types of variables.\n\n\n\n\n\n\n2.4.2 Types of EDA\nEDA helps us answer important questions like:\n\nWhat are the most common values of a variable?\nHow much do observations differ from each other?\nIs one variable related to another?\n\nTo answer these questions, EDA is generally cross-classified into two ways:\n\n\n\n\n\n\n\n\n\nNon-graphical (numbers/tables)\nGraphical (plots)\n\n\n\n\nUnivariate (one)\nUnivariate × Non-graphical\nUnivariate × Graphical\n\n\nMultivariate (≥2*)\nMultivariate × Non-graphical\nMultivariate × Graphical\n\n\n\n* Often bivariate.\nIn EDA, non-graphical methods are simply the numbers—the collection of descriptive statistics that summarize data (center, spread, shape, position, and association), while graphical methods are the visual counterparts that show those same properties (e.g., histograms/boxplots for distribution, scatterplots for association), helping us verify, compare, and spot patterns or anomalies that the numbers alone might hide.\nUnivariate methods examine a single variable at a time, whereas multivariate methods consider two or more variables to explore relationships—most often bivariate, though sometimes three or more—and it is generally best practice to first perform univariate EDA on each variable before moving on to multivariate analysis.\nBeyond the four cells of the cross-classification, EDA choices also depend on each variable’s role (outcome vs explanatory) and type (categorical vs quantitative)—and there is an element of art that grows with practice.\n\n\n\n\n\n\n2.4.2.1 Descriptive Statistics\n\n\n\n\n\n\nNote\n\n\n\nWhen we measure the same characteristic (e.g., age, gender, task speed, response to a stimulus) on all subjects in a sample, the resulting values form the sample distribution of that variable. This sample distribution is our imperfect window onto the population distribution.\nDescriptive statistics aims to describe the sample distribution—its location, spread, shape, and unusual observations—and to make tentative statements about which population distributions could plausibly have generated it.\n\n\nDescriptive statistics give numerical summaries of a dataset.\nBefore diving into categorical and numerical summaries, first check for missing values. Missingness is part of the data story and can be informative: count missing values and look for patterns (are they concentrated in certain variables or groups?). In EDA, always report the number (and share) of missing values per variable. We will return to strategies for handling missingness later, since it merits more attention and advanced methods.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nTipR Tip: any(), all(), and anyNA()\n\n\n\n\nany(x) returns TRUE if at least one value in x is TRUE.\nall(x) returns TRUE only if every value in x is TRUE.\nanyNA(x) is a shortcut to check if any value is NA (missing) in a vector or data frame.\n\nExamples\nx &lt;- c(1, 2, 3, NA)\n\nany(is.na(x))     # TRUE – there is at least one NA\nall(is.na(x))     # FALSE – not all are NA\nanyNA(x)          # TRUE – a fast way to check for NA, \nUse any() when you care if a problem exists, and all() when you want to know if everything passes a check.\n\n\n\n\n\n\n\n\nNote\n\n\n\nsummaries like mean/SD ignore NA by default only if you set na.rm = TRUE; silently dropping cases can bias results.\n\n\nUnivariate Case\n\nQualitative Data\n\nFor a categorical variable, the key features are the set of categories (the possible values) and how often each occurs (frequency or relative frequency).\nThe primary tool is a frequency table: list each category with its count and proportion/percent. Always include the total \\(N\\) (and any missing) to verify that every recruited subject is represented. As a quick quality check, proportions should sum to \\(1.00\\) (or \\(100\\%\\)). Once you are comfortable, reporting either proportion or percent is sufficient—they are equivalent.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQuantitative Data\n\nA quantitative variable’s population distribution is described by its center (typical value), spread (how much values vary), modality (how many peaks), shape (including how long or heavy the tails are), and outliers (unusual values). The data we observe are just one random sample among many we could have drawn, so the sample’s features matter only because they help us learn about the population they come from.\nWhat we observe for a given variable is the sample distribution—how the values happen to fall in this particular sample. If we drew another sample from the same population, the distribution (and the numbers we compute) would likely be different because of random sampling (and, in experiments, possibly different treatment assignments or conditions). From each sample we can calculate sample statistics—mean, variance/standard deviation, skewness, kurtosis—but these vary from sample to sample, so they are not exact truths; they are noisy estimates that provide uncertain information about the population distribution and its parameters.\nIf a quantitative variable has only a few distinct values, a simple frequency table (as with categorical data) can be a useful tool. More often, though, we rely on numerical summaries—sample statistics like the mean, median, variance, standard deviation, skewness, and kurtosis. These are best understood as estimates of their corresponding population parameters. We aim to learn what we can about those parameters from a random sample, knowing we can’t know them exactly, because the parameters are “secrets of nature.”\n\n\n\n\n\n\nNote\n\n\n\nQuantitative vs categorical summaries\nFor quantitative variables, it is meaningful to report a variable’s central tendency (mean/median), spread (SD/IQR), skewness, and kurtosis. For categorical variables, these summaries do not make sense—use counts, proportions/percentages, number of levels, mode, and missingness instead.\n\n\nThey describe the key features of a variable:\n\nCenter (e.g., mean or median) — a typical value\nSpread (e.g., standard deviation or interquartile range) — how much values vary\nShape (e.g., skewness) — symmetry or lean and tail behavior\nExtremes (e.g., minimum, maximum, or outliers) — unusual low/high values\n\nCentral Tendency\nA measure of central tendency tells us what a “typical” or “middle” value looks like in the data.\nThe most common measures are the (arithmetic) mean, the median, and sometimes the mode.\n\n\n\n\n\n\nCaution\n\n\n\nIn special situations you may also see geometric, harmonic, trimmed, or Winsorized means used as measures of centrality.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMost authors use average to mean the arithmetic mean, though some use average more broadly to include these other means.\n\n\nIf we have \\(n\\) values \\(x_1\\), \\(x_2\\), \\(\\ldots\\), \\(x_n\\), the sample (arithmetic) mean is\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\nIn words: add up all the values and divide by the number of values.\n\n\n\n\n\n\nTip\n\n\n\nA helpful picture is the fair-share idea—put everything into one pot and split it equally; the amount each person would get is the mean.\n\n\nFor most descriptive quantities, there is a population version and a sample version. In a fixed finite population—or in an idealized infinite population defined by a probability model—there is a single population mean, a fixed (often unknown) parameter. By contrast, the sample mean changes from one sample to another; it is a random variable. The distribution of the sample mean over all possible samples is its sampling distribution. This reflects the idea that, at least in principle, we could repeat the sampling many times and recompute the statistic each time, getting different values. Under suitable assumptions, probability theory lets us derive the exact (or approximate) form of this sampling distribution.\n\n\n\n\n\n\nWarning\n\n\n\nThe mean is sensitive to outliers\nBe careful: the mean can be pulled toward extreme values. For example, using the mean to describe income is often misleading because a few very high earners raise the average.\nWhat to do instead: for skewed data, report the median (and IQR) or show both mean and median; consider trimmed/Winsorized means; always add a quick plot (histogram/boxplot).\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe median is another measure of central tendency. To find it, first sort the \\(n\\) values from smallest to largest. If \\(n\\) is odd, the median is the middle value; if \\(n\\) is even, it is the average of the two middle values. When there are ties around the middle, statistical software applies standard rules so you still get a single number. (For some discrete variables there may not be a unique theoretical median, but software returns a conventional choice.)\nIn the 2004 U.S. Census Bureau Economic Survey, the median family income was $43,318, meaning half of families earned less and half earned more. The mean (average) was $60,828, the amount each family would have if total income were split equally. The gap between these two numbers is large because a few very high incomes pull the mean upward.\n\n\n\n\n\n\nTip\n\n\n\nRobustness of the median\nThe median is robust, a few very large or very small values usually do not change it. You can move the highest or lowest observations far away and, as long as the middle order does not change, the median stays the same.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe mode is the most frequent value. For quantitative data it is rarely a good “typical” summary because with continuous or rounded variables the result depends on binning/rounding, may be non-unique, and many samples have no repeated value. We mostly use mode as shape language—unimodal (one peak), bimodal (two), multimodal (many). In practice, it is conceptually useful but hard to estimate robustly for numeric variables, and is used more for categorical or discrete data.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nMean vs. median\nThe most common measure of central tendency is the mean. When outliers are a concern, the median may be preferred.\n\n\nSpread (Dispersion)\nSpread tells us how far values tend to lie from the center and how different they are from one another. A dataset with very similar values has low dispersion; one with values scattered widely has high dispersion. Three common measures:\n\nVariance — the average squared distance from the mean. Useful mathematically, but harder to interpret because it is in squared units.\nStandard deviation (SD) — the square root of the variance. Same units as the data; think of it as a typical distance from the mean. For bell-shaped data, most observations sit within a few SDs of the mean.\nInterquartile range (IQR) — the distance between the 25th and 75th percentiles; the middle 50% of the data. Robust to outliers, and usually paired with the median.\n\nOther quick checks: the range (max − min) is easy but very sensitive to extremes; MAD (median absolute deviation) is a robust alternative to SD.\nThe MAD can be defined as\n\\[\n\\mathrm{MAD} = \\mathop{\\mathrm{median}}_{1\\leq i \\leq n} |x_i - \\bar{x}|\n\\]\n\n\n\n\n\n\nTip\n\n\n\nWhich measure when?\nUse SD when the distribution is roughly symmetric and free of extreme outliers. Use IQR (often with the median) when the distribution is skewed or contains outliers.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nOutliers inflate variance/SD.\nA few extreme values can make the variance and SD look large even if most data are tightly clustered. Always check a plot and consider IQR/MAD for robustness.\n\n\nTo describe spread, start with each value’s deviation from the mean (value − mean). If you add up all these signed deviations, they cancel and sum to zero. To avoid that cancellation and to give extra weight to big departures, we square the deviations and then average them—that average of squared deviations is the variance.\nThe population variance is the mean of the squared deviations. For a sample, the variance is an estimate of it and will change from sample to sample. For \\(n\\) observations (\\(x_1\\), \\(\\ldots\\) , \\(x_n\\)), define each deviation as \\((x_i - \\bar{x})\\) (negative if below the mean, positive if above), and thus,\n\\[\ns^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\n\\]As a sample statistic, (\\(s^2\\)) varies from sample to sample; we use it to estimate the single, fixed population variance (\\(\\sigma^2\\)). The \\((n-1)\\) in the denominator ensures unbiasedness: across many random samples, the average of \\(s^2\\) equals \\(\\sigma^2\\).\n\n\n\n\n\n\n\nNote\n\n\n\nWhy does variance matter?\nVariance is an important quantity in statistics. Many statistical tests use changes in variance to compare groups or detect effects.\n\n\nBecause we square deviations, the variance is always non-negative and its units are squared. That is why a temperature measured in degrees has variance in degrees², and an area measured in km² would have variance in km⁴. This can feel odd, but it is also why we often report the standard deviation (the square root of variance) when we want a spread measure back in the original units.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nCaution\n\n\n\nDoes var() compute the sample variance or population variance?\n\n\nThe standard deviation is the square root of the variance, so it is in the same units as the data, which makes it easier to interpret. We usually write the sample standard deviation as \\(s\\) (and the population SD as \\(\\sigma\\) ).\n\n\n\n\n\n\nNote\n\n\n\nWhy use standard deviation?\n\nIt is on the same scale as the variable\nIt gives us a sense of spread we can understand\nit is more commonly used in EDA than variance\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSD is sensitive (like the mean)\nThe standard deviation is not robust: it can be distorted by skewed distributions and outliers (extreme values).\nWhen this happens: prefer median + IQR or MAD.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo understand the interquantile range (IQR), first we define quartiles. Quartiles split the data into four equal parts: the first quartile (Q1) is the smallest value below which 25% of observations fall, the second quartile (Q2) is the median (50%), and the third quartile (Q3) marks 75%. These three cut points summarize where the lower quarter ends, where the middle lies, and where the upper quarter begins, which is especially helpful when the distribution is not symmetric.\nThe interquartile range (IQR) is the distance between the third and first quartiles, ($ = Q3 - Q1$ ). It measures how spread out the middle 50% of the data are: a wider IQR means the central values are more dispersed, and a narrower IQR means they are more tightly clustered. Like the standard deviation, the IQR is expressed in the same units as the original data, but unlike the standard deviation it is robust—a few extreme highs or lows have little effect on it because it depends only on the central half of the distribution.\nIn contrast, the range (maximum minus minimum) is very sensitive to outliers and can change dramatically from sample to sample; it is useful for quick checks and for spotting obvious data-entry errors when you know plausible bounds, but it is not a stable measure of spread. By comparison, the variance/standard deviation fluctuate less than the range, and the IQR fluctuates less than either of them.\n\n\n\n\n\n\nNote\n\n\n\nWhy is the IQR useful?\n\nIt does not depend on extreme values.\nIt gives a good summary of the central spread\nIt is preferred in EDA for skewed or messy data.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nShape (Skewness and kurtosis)\nThe normal distribution is a foundational concept in statistics. It is perfectly symmetric: the left and right sides of its bell-shaped curve are mirror images. This means the mean, median, and mode of a normal distribution are all equal.\n\n\n\n\n\nBut real-world data rarely follow perfect symmetry. That’s where skewness comes in.\nSkewness describes asymmetry. A positive skew means a long right tail (very high values are more common than very low ones); a negative skew means a long left tail.\n\n\n\n[source:https://towardsdatascience.com/skewness-kurtosis-simplified-1338e094fc85/]\n\n\nOne common (sample-based) formula is\n\\[\n\\text{Skewness} = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i - \\bar{x}}{s} \\right)^3\n\\]\nwhere \\(\\bar{x}\\) is sample mean, \\(s\\) is sample standard deviations and \\(n\\) is number of observations.\nWhen you don’t have access to the full dataset but know summary values like the mean, median, or mode, skewness can be approximated using\n\nPearson’s First Coefficient:\n\n\\[\n\\text{Skewness} = \\frac{\\bar{x} - \\text{Mode}}{s}\n\\]\n\nPearson’s Second Coefficient:\n\n\\[\n\\text{Skewness} = \\frac{3(\\bar{x} - \\text{Median})}{s}\n\\]\nThese are easier to compute and often agree on the direction (sign) of skew, even if the magnitude differs.\n\n\n\n\n\n\nNote\n\n\n\nHow to Interpret Skewness\n\nSkewness = 0 → The distribution is symmetric.\nSkewness &gt; 0 → The distribution is right-skewed (long tail on the right).\nSkewness &lt; 0 → The distribution is left-skewed (long tail on the left).\n\n\n\nWhen we compute these from a sample, we get estimates with standard er rors. As a rough rule: if an estimate is within about \\(\\pm 2\\) standard errors of zero, there is no strong evidence of skewness or extra tail weight; if it is more than \\(2\\) standard errors away, that suggests positive/negative skew or heavier/lighter tails in that direction. Also, when the Pearson’s second coefficient is between -0.3 and 0.3, the distribution is approximately symmetric.\nIn R, example is\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nKurtosis describes tail weight (and peak shape) relative to a Normal distribution. Positive kurtosis (“heavy tails”) means more extreme values and a sharper center peak; negative kurtosis (“light tails”) means fewer extremes and a flatter, broader peak.\n\n\n\n[Source: https://towardsdatascience.com/skewness-kurtosis-simplified-1338e094fc85/]\n\n\nThe most common formula used to calculate raw kurtosis is\n\\[\n\\text{Kurtosis} = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{x_i - \\bar{x}}{s} \\right)^4\n\\]\nA normal distribution has a kurtosis of 3. This value serves as a benchmark. To make comparisons easier, we often use a version called excess kurtosis, which is defined as:\n\\[\n\\text{Excess Kurtosis} = \\text{Kurtosis} - 3\n\\]\nThis transformation re-centers the scale so that the normal distribution has excess kurtosis equal to 0.\nWhen excess kurtosis is positive, the distribution is called leptokurtic. This means the distribution has heavier tails and a sharper peak than a normal distribution—indicating that extreme values are more likely. When excess kurtosis is negative, the distribution is referred to as platykurtic, meaning it has lighter tails and a flatter peak, with fewer extreme values than a normal distribution.\nThus, kurtosis tells us not just about the peak of the distribution, but more importantly, about how likely we are to observe values that are far from the mean.\nExamples in R are\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn short, skewness tells us whether a distribution is symmetric or lopsided, while kurtosis tells us whether the distribution has unusual peaks or heavy/light tails compared to a normal distribution.\n\n\n\n[Source: https://medium.com/@the_social_byte/why-should-we-care-about-skewness-and-kurtosis-in-data-analysis-bf6bff4b4ab6]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nUse with care. Skewness and kurtosis are sensitive to outliers and can be unstable in small samples.\nTip: Treat them as supporting evidence, and always pair with plots (histogram/density and a QQ-plot) before drawing conclusions.\n\n\nSee this link for more examples in R about skewness and kurtosis.\nExtremes and outliers\nExtremes are simply the minimum and maximum (and other high/low quantiles). They’re expected in any sample.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOutliers are observations unusually far from the bulk relative to a rule or model (context matters).\nTukey’s rule flags an observation as a mild outlier if it lies below Q1 − 1.5×IQR or above Q3 + 1.5×IQR, and as an extreme outlier if it lies beyond Q1 − 3×IQR or Q3 + 3×IQR.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExerciseSolutions\n\n\nIn the dataset mpg,\n\nCompute the mean and median of cty. Which is larger? What does that suggest about skew?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFor displ (engine displacement), report SD and IQR. Which would you prefer if the distribution is skewed? Why?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFind the modal category of drv (f/r/4). Also report proportions.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCompute skewness and kurtosis for hwy, cty, and displ. Which looks most skewed? Which has the heaviest tails?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFor cty, compute min/max and identify outliers using the 1.5×IQR rule. How many outliers?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nDo it yourself! :) \n\n\n\nMultivariate Case\nCross-tabulation is the basic non-graphical way to study the relationship between two (or more) variables. For two categorical variables, make a two-way table of counts, and (depending on your question) add row % (each row sums to 100), column % (each column sums to 100), and/or cell % (table sums to 100). You can extend this to a third variable by producing one two-way table at each level of the third variable (or by using a three-way contingency table).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAssociations\nIn EDA, we often want to ask Is one variable associated with another?\nThis can be for:\n\nTwo numeric variables\nTwo categorical variables\nOne numeric and one categorical\n\nPairs of Numeric Variables\nThe most common way to measure association between two numeric variables is:\nPearson’s Correlation Coefficient\n\nMeasures Strength and direction of a linear relationship.\nValues range from \\(-1\\) to \\(+1\\):\n\n\\(+1\\): Perfect positive linear association\n\\(-1\\): Perfect negative linear association\n\\(0\\): no linear relationship\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPerson’s \\(r\\) only measures linear relationships.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIf the relationship is curved, \\(r\\) can be misleading. See Anscombe’s Quartet for famous examples.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis figure is Anscombe’s quartet—four tiny datasets that share (nearly) the same summary statistics (means of \\(x\\) and \\(y\\), variances, Pearson correlation) but look very different once you plot them.\n\nDataset I (top-left): A textbook linear relationship with ordinary scatter around the line. Here a linear model is sensible.\nDataset II (top-right): A clear curved (non-linear) pattern. The fitted straight line and the correlation suggest “strong linearity,” but that’s misleading—a linear model is inappropriate.\nDataset III (bottom-left): Almost perfectly linear except for one outlier in \\(y\\). That single point inflates the correlation and affects the fit; without it the pattern is very regular.\nDataset IV (bottom-right): Most \\(x\\) values are the same (a vertical stack); a single high-leverage point at large \\(x\\) drags the regression line to create a high correlation/“good” fit. The model is being driven by one point.\n\n\n\n\n\n\n\nTip\n\n\n\nNumbers alone can fool you. Correlation, means, and regression coefficients can look “good” even when the relationship is non-linear or dominated by outliers/high-leverage points. Always plot your data, and follow up with diagnostics (residual plots, leverage/Cook’s distance) before trusting a model.\n\n\nThe population correlation (\\(\\rho\\)) is defined as\n\\[\n\\rho = \\frac{Cov(X, Y)}{\\sqrt{\\sigma_X \\sigma_Y}} =\\frac{E[(X - \\mu_X)(Y - \\mu_Y)]}{\\sqrt{E[(X - \\mu_X)^2E[(Y - \\mu_Y)^2}}\n\\]\nwhere \\(\\mu_X\\) and \\(\\mu_Y\\) are the population means, \\(\\sigma_X\\) and \\(\\sigma_Y\\) are the population standard deviations.\nIn practical, we do not know \\(\\mu\\) or \\(\\sigma\\) , so we replace them with their sample counterparts and compute the Pearson’s \\(r\\) (sample) as\n\\[\nr=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i-\\bar{y})^2}}\n\\]\nRank Correlations: For Non-linear Relationships\nIf the relationship is not linear, use a rank correlation:\n\n\n\nMethod\nNotes\n\n\n\n\nSpearman’s \\(\\rho\\)\nMore sensitive to outliers\n\n\nKendall’s \\(\\tau\\)\nBetter for ordinal data\n\n\n\nThese methods:\n\nconvert data into ranks (1st, 2nd, 3rd, …)\nMeasure how well the ranks agree between two variables\n\nThey are interpreted like Pearson’s \\(r\\):\n\nClose to \\(\\pm 1\\) means strong association\n\\(0\\) means no association\n\nPairs of Categorical Variables\nFor two categorical variables, we ask Are certain combinations of categories more or less common than expected?\nThe most basic tool is the contingency table. It counts how many times each combination of categories appears in the data.\nThis table helps us to answer questions like:\n\nDo some combinations occur more often than others?\nAre the two categorical variables associated?\n\nExample: Penguins Species and Islands\nImagine we observed \\(344\\) penguins from three species on three islands. We record how many penguins of each species were found on each island.\n\n\n\nSpecies\nBiscoe\nDream\nTorgersen\nTotal\n\n\n\n\nAdelie\n44\n56\n52\n152\n\n\nChinstrap\n0\n68\n0\n68\n\n\nGentoo\n124\n0\n0\n124\n\n\nTotal\n168\n124\n52\n344\n\n\n\nSummary of Correlation and Association Measures in R\nThis table shows which association or correlation measure to use depending on the types of variables.\n\n\n\n\n\n\n\n\n\n\nNominal\nOrdinal\nNumeric\n\n\n\n\nNominal\nCramér’s Vφ coefficient (binary)Tetrachoric corr. (binary)\nNo standard measure\nEta (η)\n\n\nOrdinal\nNo standard measure\nSpearman’s ρKendall’s τ\nSpearman’s ρKendall’s τ\n\n\nNumeric\nEta (η)\nSpearman’s ρKendall’s τ\nPearson’s rSpearman’s ρKendall’s τ\n\n\n\nIn this table, we can find the R function for the methods described above with a brief description of its interpretation.\n\n\n\n\n\n\n\n\nMeasure\nInterpretation\nR Function / Package\n\n\n\n\nPearson’s r\nStrength of linear relationship\ncor(x, y, method = \"pearson\")\n\n\nSpearman’s ρ\nStrength of monotonic relationship\ncor(x, y, method = \"spearman\")\n\n\nKendall’s τ\nRank-based measure; robust to ties\ncor(x, y, method = \"kendall\")\n\n\nSpearman’s ρ or Kendall’s τ\nAssociation between ranks\nSame as above\n\n\nPhi (φ) coefficient\nAssociation between two binary variables\npsych::phi(table)\n\n\nCramér’s V\nStrength of association in contingency table\nrcompanion::cramerV(table) or DescTools::CramerV(table)\n\n\nEta (η) coefficient\nHow much variance in numeric var is explained by group\nDescTools::EtaSq(aov(y ~ group))\n\n\nTetrachoric correlation\nFor binary variables assumed to reflect latent continuous variables\npsych::tetrachoric(table)$rho\n\n\nOrdinal vs. Nominal\nUse visual summaries or grouped statistics\ntable(), ggplot2::geom_bar(), mosaicplot()\n\n\n\nNotes\n\nSpearman’s ρ and Kendall’s τ both work well for ranked data and are more robust to non-linear trends than Pearson’s r.\nCramér’s V is used for nominal data, and its values range from 0 to 1.\nEta squared (η²) measures how much of the variance in the numeric variable is explained by the group (nominal) variable.\nFor ordinal + nominal, no widely accepted coefficient exists; focus on visual summaries (e.g., bar charts or stacked plots).\n\n\nExample: What Should You Use?\n\n\n\nSituation\nUse\n\n\n\n\nHeight vs Weight\nPearson’s r\n\n\nExam Rank vs Satisfaction Level\nKendall’s τ\n\n\nEye Color vs Blood Type\nCramér’s V\n\n\nGender vs Income\nEta\n\n\nPlant Type vs Size Category\nCross-tab\n\n\n\nThese summaries help us compare groups and form initial impressions (for example, the mean suggests a “typical” value). But a few numbers can not tell the whole story—different datasets can share the same mean and spread yet look very different, so always pair numbers with plots and context.\n\nExerciseSolutions\n\n\nIn the dataset mtcars,\n\nCompute the Pearson correlation between mpg (miles per gallon) and hp (horsepower).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCompute the Spearman correlation between the same two variables.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nNow, answer these questions:\n\nIs the relationship positive or negative?\nDo Pearson’s r and Spearman’s \\(\\rho\\) give similar results? Why or why not?\n\n\n\nDo it yourself! :)\n\n\n\n\nExerciseSolutions\n\n\nIn the dataset Titanic,\n\nCreate a contingency table of passenger Class vs Survived.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCompute appropriate bivariate association for this association\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nNow, answer this question:\n\nHow strong is the association according to the used coefficient?\n\n\n\nDo it yourself! :)\n\n\n\n\nExerciseSolutions\n\n\nIn the dataset palmerpenguins::penguins,\n\nCompute the correlation between bill_length_mm and bill_depth_mm.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBuild a contingency table of species and island and compute Cramér’s V.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nDo it yourself! :)\n\n\n\n\nExerciseSolutions\n\n\nIn the dataset iris,\nCompute the appropriate coefficient of bi-variate association between Sepal.Length and Species.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nDo it yourself! :)\n\n\n\nComprehensive Exercise: Exploring the airquality Dataset\nThe dataset airquality (built into R) contains daily air quality measurements in New York, May–September 1973.\nVariables include:\n\nOzone (ppb),\nSolar.R (solar radiation),\nWind (mph),\nTemp (°F),\nMonth (5 = May, …, 9 = September),\nDay (day of month).\n\n\nDescriptive StatisticsAssociations\n\n\n\nInspect the dataset: How many rows and columns does it have? Are there missing values?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCompute the mean, median, and standard deviation of Ozone, Wind, and Temp.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nFor Month, create a frequency table. Which month has the most observations?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCompute the Pearson correlation between Ozone and Temp.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCompute the Spearman correlation betweenOzone and Solar.R.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCompute the correlation between Ozone and Month\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhich factor (Temp, Wind, Solar.R, or Month) seems most strongly associated with Ozone levels?\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n2.4.2.2 Graphical Summaries\nGraphs help us see patterns, spot outliers, and understand distributions.\nCommon EDA plots include: - Histograms – show the shape and spread of a numeric variable - Boxplots – show center, spread, and outliers - Bar charts – show frequencies of categories - Scatterplots – show relationships between two numeric variables\nGraphs are easier to understand than tables of numbers. They help us and others see what’s going on in the data.\n\n\n2.4.2.2.1 Principles of Analytic Graphics\nBased on (Tufte 2006), there are six key principles for designing informative and effective graphs.\nPrinciple 1 – Show Comparison\n\n\n\n\n\n\nShowing comparisons is really the basis of all good scientific investigation. (Peng 2012)\n\n\n\nGood data analysis always involves comparing things. A single number or result doesn’t mean much on its own. We need to ask:\n\n\n\n\n\n\nCompared to what?\n\n\n\nFor example, if we see that children with air cleaners had more symptom-free days, that sounds good. But how do we know the air cleaner made the difference? We only know that by comparing to another group of children who didn’t get the air cleaner. When we add that comparison, we can see that the control group didn’t improve — so the improvement likely came from the air cleaner.\n\n\n\nChange in symptom-free days with air cleaner. Source: @Peng2012\n\n\n\n\n\nChange in symptom-free days with air cleaner. Source: @Peng2012\n\n\n\n\n\nChange in symptom-free days by treatment group. Source: @Peng2012\n\n\nGood data graphics should always show at least two things so we can compare and understand what’s really happening.\nPrinciple 2: Show Causality and Explanation\nWhen making a data graphics, it is helpful to show why you think something is happening – not just what is happening. Even if you can not prove a cause, you can show your hypothesis of idea about how one thing might lead to another.\n\n\n\n\n\n\nIf possible, it is always useful to show your causal framework for thinking about a question. (Peng 2012)\n\n\n\n\nFor example, in #figEDA2 we saw that children with an air cleaner had more symptom–free days. But that alone does not explain why. A good follow-up question is: “Why did the air cleaner help?“ One possible reason is that air cleaners reduce fine particles in the air – especially in homes with smokers. Breathing in these particles can make asthma worse, so removing them might help children feel better. To show this, we can make a new plot.\n\n\n\nChange in symptom-free days and change in PM2.5 levels in-home. Source: Peng (2012)\n\n\nFrom the plot, we can see:\n\nChildren with air cleaners had more symptom-free days.\nTheir homes also had less PM2.5 after six months.\nIn contrast, the control group had little improvement.\n\nThis pattern supports the idea that air cleaners work by reducing harmful particles — but it is not final proof. Other things might also cause the change, so more data and careful studies are needed to confirm.\nPrinciple 3: Show Multivariate Data\n\n\n\n\n\n\nThe real world is multivariate. (Peng 2012)\n\n\n\nIn real life, most problems involve more than one or two variables. We call this multivariate data. Good data graphics should try to show these multiple variables at the same time, instead of reducing everything to just one number or a simple trend.\nLet us look at an example.\nThe mtcars dataset contains information about 32 car models from the 1970s. Each row is a car and each column is a variable. Some of these variables are: mpg: miles per gallon (fuel efficiency), wt: weight (in 1000 lbs), cyl: number of cylanders (engine size), hp: horse power, qsec: 1/4 mile time (acceleration), am: Transmission (0 = auto, 1 = manual). These variables help us to explore relationship between engin size, weight, fuel use, and more.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe want to know how a car’s weight affects its fuel efficiency (miles per gallon). We look at a simple scatter plot of these two variables.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHeavier cars tend to have lower fuel efficiency. But is weight the only thing affecting fuel use?\nCars also have different engine sizes, measured by cylinders (cyl). This affects both weight and fuel efficiency. To understand the relationship better, we add cyl as a third variable by coloring points by the number of cylinders.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, we see that cars with 4, 6, and 8 cylinders have different trends. The overall pattern changes once we include this third variable.\n\n\n\n\n\n\nCaution\n\n\n\nThe number of cylinders cofounds the relationship – it influences both weight and MPG.\n\n\nTo understand how the number of cylinders changes the relationship between weight and fuel efficiency, we can make separate plots for each cylinder group. This is called faceting.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nImportant\n\n\n\nSometimes, looking at groups separately helps us find clearer patterns that get lost in a big mixed dataset.\n\n\nEven, we can add the variable transmission(am) as an additional variable by using shapes or facet.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, each panel shows either automatic automatic or manual cars. Within each panel, colors show the number of cylinders.\nEven we can use facet_grid that allows us to split the plot into rows and columns based on two categorical variables – perfect for showing how relationships vary across combinations. We will use am in rows and cyl in columns.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis way, we can see how the relationship between weight and MPG changes in each group combination.\nPrinciple 4: Integrate Evidence — Keep the message clear\nWhen you make a graph, do not rely on points and lines to show your idea. You can also use: Numbers to give exact values, Words or short labels to explain what is happening, Pictures or diagrams to give context.\n\n\n\n\n\n\nImportant\n\n\n\nA good graph tells a complete story.\n\n\nUse all tools you need – not just the ones your software gives you easily.\n\n\n\n\n\n\nImportant\n\n\n\nThe goal is not to just make a nice picture, but to help people understand your message clearly.\n\n\nPrinciple 5: Describe and Document the Evidence\nA good graph tells a story – clearly and completely. That means it should include:\n\nA clear title\nLabels for the x-axis and y-axis\nUnits for measurement (e.g. ‘weights in 1000 lbs’)\nTime scale if needed (e.g. ‘daily’, ‘monthly’)\nwhere the data comes from (e.g., ‘New York’, ‘EPA’)\nsource of the data\n\n\n\n\n\n\n\nTip\n\n\n\nImagine someone looking only at your plot without reading anything else. Can they understand the main idea? if yes – your plot is doing a good job.\n\n\n\n\n\n\n\n\nNote\n\n\n\nEven if your graph is not final, it is a good habit to label things early. It helps you and others understand what is going on.\n\n\nFor example, instead of using\ntry this\nwhen we use ggplot2, it is better to add labs() like we did until now.\nPrinciple 6: Content is King\n\n\n\n\n\n\nImportant\n\n\n\nA beautiful plot means nothing if the question is weak or the data is poor.\n\n\nData graphics are only powerful when:\n\nthe question is clear and important\nthe data is high quality and relevant\nthe evidence supports the question\n\nNo chart or fancy design can fix a bad question or messy data. That is why it is crucial to start with a strong idea and only show what really matters to answer that idea.\n\n\n\n\n\n\nTip\n\n\n\nDo not just decorate your data – focus on the message!",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4_EDA.html#airquailty-dataset",
    "href": "chapters/chapter4_EDA.html#airquailty-dataset",
    "title": "2  Exploratory data analysis (EDA)",
    "section": "2.5 Airquailty dataset",
    "text": "2.5 Airquailty dataset\nWe will explore the airquality dataset, which contains daily measurement of air pollutants and weather conditions in New York City May to September 1973.\nStep 1 – How do ozone levels vary across different months in New York during the summer in 1973?\nThis question is specific and focused: One location (New York), One variable (ozone), one year and a defined time window (May to September)\nStep 2 – The airquality dataset is built into R. Load it with:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 3 – Check the size and dimension of the dataset\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 4 – run str()\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nCaution\n\n\n\nThere are some missing values (NA , Not Available) in the dataset.\n\n\nStep 5 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 6 –\nFirst, we count how many rows (i.e., records or observations) are in the dataset.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMissing values (denoted as NA in R) can lead to incorrect calculations or unexpected results. It is good practice to check how many missing values there are per variable.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nUsing `dplyr`\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis command uses across() to apply the same function to all columns, and sum(is.na(.)) to count the number of missing values per column.\n\n\n\n\n\n\nTip\n\n\n\nWhat is ~ ?\nThe ~ introduces an anonymous function — a function written inline without a name.\nThis\n\n~ sum(is.na(.))\n\n~sum(is.na(.))\n\n\nis a shorthand for\n\nfunction(x) sum(is.na(x))\n\nfunction (x) \nsum(is.na(x))\n\n\n\n\nOr, we can check how many rows are from July:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ncheck how many missing values are just in August.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 7 – According to the U.S. EPA, ozone levels above 70 parts per billion (ppb) may be considered unhealthy.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBased on the output, many days have safe levels and some days have extremely high ozone levels.\nStep 8 – Let us check the average ozone level by month:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis gives a quick overview of how ozone levels change over the summer. Let visualize it:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBased on the plot, we observe that July and August have the highest median ozone levels, showing that typical ozone concentrations were significantly higher during mid-summer. This suggests a seasonal pattern, likely driven by higher temperatures and increased sunlight, which promote the formation of ozone. The taller boxes and whiskers for these months reflect a greater variability in ozone levels, including several high outliers. In contrast, May and September show lower median values and less variation, possibly due to cooler temperatures and different atmospheric conditions. The median ozone level is June is slightly higher than in May but lower than in July, with a moderate level of variability. Overall, this seasonal trend is consistent with environmental science – ozone forms more easily in strong sunlight and warm conditions, which are more prevalent in July and August.\nStep 9 – Let us examine whether temperature or wind speed might help explain ozone patterns.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou may observe that there is a positive relationship between temperature and ozone and a negative relationship between wind speed and ozone. These patterns support common environmental science findings.\nStep 10 – Based on our findings, we might fit a simple regression model\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe might also refine our question: On which days was the ozone level unusually high, and what were the weather conditions on those days?\nAlternative question\nStep 1 – Which weather factor—temperature, wind, or solar radiation—has the strongest relationship with ozone levels during the summer of 1973 in New York?\nThis question is more analytical than descriptive, and focused on relationships between variables. So, we need to examine how ozone changes with respect to other variables.\nStep 2 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 3 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 4 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 5 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 6 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 7 –\nCheck how often ozone exceeds EPA’s 70 ppb guideline:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 8 –\nSince dplyr does not include a cor() function, we will compute correlations manually using summarise() and cor() from base R, keeping within tidy pipelines:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nMore advanced:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBut sometimes, a simple code gives the more informative output:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nvisualize the relationships:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nStep 9 –\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhen we check p-values and coefficients, we see Temp has strong and significant positive effect, Wind has strong and significant negative effect, and Solar.R has weaker effect but still contributes.\nStep 10 –\nNow we know temperature has the strongest effect on ozone levels:\n\nShould we check for nonlinear effects (e.g., does ozone spike at high temps)?\nWould a time-based model (e.g. by day or month) add insight?\nWhat is the temperature threshold where ozone exceeds 70 ppb?\nAre there interactions between variables (e.g. high temp + low wind)?\nDo results change if the we include time (e.g. month)?\n\nWe might now refine our question: How much does temperature need to rise before ozone levels exceed the 70 ppb threshold?\nAs Tukey emphasized, EDA is about “detecting the unexpected” and learning from the data before attempting to explain it.\n\n\n\n\n\n\nTip\n\n\n\nAt the beginning, it is hard to ask the perfect question because you do not know the date well yet. But here is the secret:\n\n\n\n\n\n\nThe more questions you ask, the better your questions become.\n\n\n\n\n\nEach time you explore one idea, it leads to a new question. That is how you:\n\nDiscover patterns\nNotice surprises\nUnderstand your data more deeply\n\n\n\n\n\n\n\nCaution\n\n\n\nGood EDA is like this\n\nAsk a question\nMake a plot or summary\nLook at the result\nAsk a new, better question\nRepeat!\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDo not wait for the perfect question - just start. Exploration will lead you to insight.\n\n\n\n\n\n\nMidway, Steve. 2022. Data Analysis in r. https://bookdown.org/steve_midway/DAR/.\n\n\nPeng, Roger D. 2012. Exploratory Data Analysis with R. https://bookdown.org/rdpeng/exdata/.\n\n\nTufte, Edward. 2006. Beautiful Evidence. Graphics Press LLC.\n\n\nTukey, John Wilder. 1977. Exploratory Data Analysis. Addison-Wesley Publishing Company.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploratory data analysis (EDA)</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html",
    "href": "chapters/chapter3_dplyr.html",
    "title": "3  Data Wrangling",
    "section": "",
    "text": "3.1 Filter observations - filter()\nThe dplyr package is a part of the R tidyverse : an ecosystem of several libraries designed to work together by representing data in common formats.\nThe data frame is a key data structure in statistics and in R. The basic structure of a data frame is that there is one observation per ro and each column represents a variable, a measure, feature, or characteristic of that observation. Given the importance of managing data frames, it is important that we have good tools for dealing with them.\nThe dplyr package is a relatively new R package that allows you to do all kinds of analyses quickly and easily.\nOne important contribution of the dplyr package is that it provides a “grammar” (in particular, verbs) for data manipulating and for operating on data frames. With this grammar, you can sensibly communicate what it is that you are doing to a data frame that other people can understand (assuming they also know the grammar). This is useful because it provides an abstraction for data manipulation that previously did not exist.\nSome of the key “verbs” provided by the dplyr package are\nTo install the dplyr package from CRAN, just run\nAfter installing the package, it is important to load it into the R session.\nTo better understanding, let us continue within analyzing a dataset, penguins, available within the palmerpenguins package. we need to load the dataset and if it is necessary, we need to install the package.\n‌The data frame contain data for \\(344\\) penguins and \\(8\\) variables describing the species (species), the island (island), some measurements of the size of the bill (bill_length_mm and bill_depth_mm), flipper (flipper_length_mm) and body mass (body_mass_g), the sex (sex) and the study year (year). More information about the data frame can be found by running ?penguins .\nYou can see some basic characteristics of the dataset with the dim() and str() functions.\nThe first and last 6 rows can be displayed by head() and tail(), respectively.\nThe summary information of data frame can be found by summary() .\nThis function works on both quantitative and qualitative variables.\nYou can combine multiple conditions using & if all conditions must be true (cumulative), or | if at least one condition must be true (alternative). For example,\nAs you can see, the filter() functions require the name of the data frames as the first argument, then the condition (with the usual logical operators &gt;, &lt;, &gt;=, &lt;=, ==, !=, %in%, etc.) as second argument.\nSo with the pipe operator, the code above becomes:\nInstead of listing the data frame’s name as the initial argument within functions like filter() (or other {dplyr} functions), you simply specify the data frame once, then use the pipe operator to connect it to the desired function.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#filter-observations---filter",
    "href": "chapters/chapter3_dplyr.html#filter-observations---filter",
    "title": "3  Data Wrangling",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nVariable names should be used directly, without enclosing them in single or double quotation marks (' or \").\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nTo use any functions in the dplyr package, you must specify the data frame’s name as the first argument. Alternatively, you can use the pipe operator (|&gt; or %&gt;% ) to avoid explicitly naming the data frame within each function.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe keyboard shortcut for the pipe operator is ctrl+shift+M on Windows or cmd + shift + M on Mac. By default, this will produce %&gt;%, but if you have configured RStudio to use the native pipe operator, it will print |&gt; .\n\n\n\n\n\n\nthe %&gt;% pipe, originating from the magrittr package (included in the tidyverse), was superseded in R 4.1.0 (released in 2021) by the native |&gt; pipe. We recommend |&gt; because it’s a simpler, built-in feature of base R, always available without needing additional package loads.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe pipe operator lets you chain multiple operations together, which is especially handy for performing several calculations on a data frame without saving the result of each intermediate step.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nThe pipe operator streamlines your code by feeding the output of one operation directly into the next, making your code much easier to write and read.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#extract-observations",
    "href": "chapters/chapter3_dplyr.html#extract-observations",
    "title": "3  Data Wrangling",
    "section": "3.2 Extract observations",
    "text": "3.2 Extract observations\nYou can extract observations from a dataset based on either their positions or their values.\n\n3.2.1 Based on Their Positions\nTo extract observations based on their positions, you can use the slice() function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFurthermore, for extracting specific rows like the first or last, you can use specialized functions:\n\nslice_head(): Extracts rows from the beginning of the dataset.\nslice_tail(): Extracts rows from the end of the dataset.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.2.2 Based on their values\nWhen you need to extract observations based on the values of a variable, you can use:\n\nslice_min(): Selects rows with the lowest values, allowing you to define a specific proportion.\nslice_max(): Selects rows with the highest values, also with the option to define a proportion.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#sample-observations",
    "href": "chapters/chapter3_dplyr.html#sample-observations",
    "title": "3  Data Wrangling",
    "section": "3.3 Sample Observations",
    "text": "3.3 Sample Observations\nSampling observations can be achieved in two ways:\n\nsample_n(): Takes a random sample of a specified number of rows.\nsample_frac(): Takes a random sample of a specified fraction of rows.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nIt is important to note that, similar to the base R sample() function, the size argument can exceed the total number of rows in your data frame. If this happens, some rows will be duplicated, and you will need to explicitly set the argument replace = TRUE.\n\n\nAlternatively, you can obtain a random sample (either a specific number or a fraction of rows) using slice_sample(). For this, you use:\n\nThe argument n to select a specific number of rows.\nThe argument prop to select a fraction of rows.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#sort-observations",
    "href": "chapters/chapter3_dplyr.html#sort-observations",
    "title": "3  Data Wrangling",
    "section": "3.4 Sort observations",
    "text": "3.4 Sort observations\nObservations can be sorted using the arrange() function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBy default, arrange() sorts in ascending order. To sort in descending order, simply use desc() within the arrange() function, like arrange(desc(variable_name)).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSimilar to filter(), arrange() can sort by multiple variables and works with both quantitative (numerical) and qualitative (categorical) variables. For example, arrange(sex, body_mass) would first sort by sex (alphabetical order) and then by body_mass (ascending, from lowest to highest).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to note that if a qualitative variable is defined as an ordered factor, the sorting will follow its defined level order, not alphabetical order.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#select-variables",
    "href": "chapters/chapter3_dplyr.html#select-variables",
    "title": "3  Data Wrangling",
    "section": "3.5 Select variables",
    "text": "3.5 Select variables\nYou can select variables using the select() function based on their position or name.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTo remove variables, use a - sign before their position or name.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can also select a sequence of variables by name (e.g., select(df, var1:var5)).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFurthermore, select() provides a straightforward way to rearrange column order in your data frame.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n3.5.1 Select using helper functions\nThe select() function also supports helper functions for matching column names based on patterns:\nstarts_with(\"abc\") selects all columns whose names begin with the specified string.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nends_with(\"xyz\") selects all columns whose names end with the specified string.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ncontains(\"ijk\") selects all columns that contain the specified substring anywhere in their name.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe everything() helper selects all remaining columns that have not been explicitly mentioned. It is useful when you want to: move some variables to the front, or keep all others in their existing order.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nnum_range(\"prefix\", 1:3) selects variables with names like \"prefix1\", \"prefix2\", \"prefix3\", etc. This is useful for selecting numbered variables.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nmatches(\"(.)\\\\1\") selects columns whose names match a regular expression (regex).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis example selects aa and bb but not ab, since the pattern (.)\\\\1 means “a character repeated twice”.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#renaming-variables",
    "href": "chapters/chapter3_dplyr.html#renaming-variables",
    "title": "3  Data Wrangling",
    "section": "3.6 Renaming Variables",
    "text": "3.6 Renaming Variables\nTo rename variables, use the rename() function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nRemember the syntax: new_name = old_name. This means you always write the desired new name first, followed by an equals sign, and then the current old name of the variable.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#create-or-modify-variables",
    "href": "chapters/chapter3_dplyr.html#create-or-modify-variables",
    "title": "3  Data Wrangling",
    "section": "3.7 Create or Modify Variables",
    "text": "3.7 Create or Modify Variables\nThe mutate() function allows you to create new variables or modify existing ones. You can base these operations on another existing variable or a vector of your choice.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nIf you create a variable with a name that already exists, the old variable will be overwritten.\n\n\nSimilar to rename(), mutate() requires the argument to be in the format name = expression, where name is the column being created or modified, and expression is the formula for its values.\nThere is another function transmute() that is also used to create a new variable in a data frame by transforming existing ones. However, unlike mutate(), which keeps all original columns and simply adds the new ones, transmute() returns only the newly created variables. This means that when you use transmute(), the resulting data frame will include just the variables you explicitly define inside the function. It is particularly useful when you want a clean output focused only on the transformed results, without retaining the original dataset’s columns. In contrast, mutate() is ideal when you want to preserve the full structure of your data while adding new insights.\nLet us create a new column (body mass in kg).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#summarize-observations",
    "href": "chapters/chapter3_dplyr.html#summarize-observations",
    "title": "3  Data Wrangling",
    "section": "3.8 Summarize Observations",
    "text": "3.8 Summarize Observations\nTo get descriptive statistics of your data, use the summarize() (or summarise()) function in conjunction with statistical functions like mean(), median(), min(), max(), sd(), var(), etc.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nRemember to use na.rm = TRUE to exclude missing values from calculations.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#identify-distinct-values",
    "href": "chapters/chapter3_dplyr.html#identify-distinct-values",
    "title": "3  Data Wrangling",
    "section": "3.9 Identify Distinct Values",
    "text": "3.9 Identify Distinct Values\nThe distinct() function helps you find unique values within a variable.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhile typically used for qualitative or quantitative discrete variables, it works for any variable type and can identify unique combinations of values when multiple variables are specified. For instance, distinct(species, study_year) would return all unique combinations of species and study year.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#group-by",
    "href": "chapters/chapter3_dplyr.html#group-by",
    "title": "3  Data Wrangling",
    "section": "3.10 Group By",
    "text": "3.10 Group By\nThe group_by() function changes how subsequent operations are performed. Instead of applying functions to the entire data frame, operations will be applied to each defined group of rows. This is particularly useful with summarize(), as it will produce statistics for each group rather than for all observations.\nFor example, to calculate the mean and standard deviation of body_mass separately for each species, you would first group_by(species) and then summarize() the body_mass. The pipe operator smoothly passes the grouped data from group_by() to summarize().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can also group by multiple variables (e.g., group_by(var1, var2)), and the data frame’s name only needs to be specified in the very first operation of a chained sequence.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#managing-groups-ungroup",
    "href": "chapters/chapter3_dplyr.html#managing-groups-ungroup",
    "title": "3  Data Wrangling",
    "section": "3.11 Managing Groups: ungroup()",
    "text": "3.11 Managing Groups: ungroup()\nAfter performing operations on grouped data, the ungroup() function allows you to revert to a normal data frame, enabling operations on entire columns again or switching to new grouping criteria. Remember, you can also group_by() multiple columns simultaneously.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#number-of-observations",
    "href": "chapters/chapter3_dplyr.html#number-of-observations",
    "title": "3  Data Wrangling",
    "section": "3.12 Number of Observations",
    "text": "3.12 Number of Observations\nThe function n() returns the number of observations. It can only be used inside summarize().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhen combined with group_by(), you can easily get the number of observations per group. n() takes no parameters, so it’s always written as n().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNotably, the count() function is a convenient shortcut, equivalent to summarize(n = n()).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#number-of-distinct-values",
    "href": "chapters/chapter3_dplyr.html#number-of-distinct-values",
    "title": "3  Data Wrangling",
    "section": "3.13 Number of Distinct Values",
    "text": "3.13 Number of Distinct Values\nTo count the number of unique values or levels in a variable (or combination of variables), use n_distinct(). Like n(), it’s exclusively used within summarize().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou do not have to explicitly name the output; the operation’s name will be used by default (e.g., summarize(n_distinct(variable))).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#first-last-or-nth-value",
    "href": "chapters/chapter3_dplyr.html#first-last-or-nth-value",
    "title": "3  Data Wrangling",
    "section": "3.14 First, Last, or \\(n\\)th Value",
    "text": "3.14 First, Last, or \\(n\\)th Value\nAlso available only within summarize(), you can retrieve the first, last, or nth value of a variable. Functions like first(), last(), and nth() enable this.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThese functions offer arguments to handle missing values; for more details, consult their documentation (e.g., ?nth()).",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#conditional-transformations",
    "href": "chapters/chapter3_dplyr.html#conditional-transformations",
    "title": "3  Data Wrangling",
    "section": "3.15 Conditional Transformations",
    "text": "3.15 Conditional Transformations\n\n3.15.1 If Else\nThe if_else() function (used with mutate()) is ideal for creating a new variable with two levels based on a condition. It takes three arguments:\n\nThe condition (e.g., body_mass_g &gt;= 4000).\nThe output value if the condition is TRUE (e.g., “High”).\nThe output value if the condition is FALSE (e.g., “Low”).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nTip\n\n\n\nA key advantage is that if_else() propagates missing values (NA) if the condition’s input is missing, preventing misclassification.\n\n\n\n\n3.15.2 Case When\nFor categorizing a variable into more than two levels, case_when() is far more appropriate and readable than nested if_else() statements.\nWhile nested if_else() functions can technically work, they are prone to errors and result in difficult-to-read code. For instance, to classify body mass into “Low” (&lt;3500), “High” (&gt;4750), and “Medium” (otherwise), nested if_else() would look like this:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis code first checks if body_mass_g is less than 3500. If true, it assigns “Low”. If false, it then checks if body_mass_g is greater than 4750. If that’s true, it assigns “High”; otherwise, it assigns “Medium”. While functional, this structure can become complex and error-prone with more conditions.\ncase_when() evaluates conditions sequentially. To improve this workflow, we now use the case when technique:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis workflow is much simpler to code and read!\nIf there are no missing values in the variable(s) used for the condition(s), it can even be simplified to:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhile a .default argument can be used to specify an output for observations not matching any condition, exercise caution with missing values. If NA values in the conditioning variable are not explicitly handled, they might be incorrectly assigned to the default category. A safer approach is to explicitly define all categories or ensure NAs remain NA.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nRegardless of whether you use if_else() or case_when(), it’s always good practice to verify the newly created variable to ensure it aligns with your intended results.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#exploring-further-dplyr-functions",
    "href": "chapters/chapter3_dplyr.html#exploring-further-dplyr-functions",
    "title": "3  Data Wrangling",
    "section": "3.16 Exploring Further dplyr Functions",
    "text": "3.16 Exploring Further dplyr Functions\nUntil now, we have focused on analyzing the penguins dataset. To effectively explain some of dplyr’s other powerful functions, we’ll now shift to creating custom datasets tailored to demonstrate their specific functionalities.\n\n3.16.1 Separate and Unite\nYou can separate a character column into two or more new columns using separate().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nConversely, to combine two or more columns into a single character column, use unite().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n3.16.2 Reshaping Data: gather() and spread()\ngather() transforms “wide” format data into “long” or “tall” format by collapsing columns into key-value pairs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nConversely, spread() converts “long” or “tall” format data into “wide” format by separating key-value pairs across multiple columns.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3_dplyr.html#combining-datasets-joins",
    "href": "chapters/chapter3_dplyr.html#combining-datasets-joins",
    "title": "3  Data Wrangling",
    "section": "3.17 Combining Datasets: Joins",
    "text": "3.17 Combining Datasets: Joins\nSometimes, your data is split across multiple tables. For example, one table may have demographic information (like gender, marital status, height, weight), and another table may have medical records (like visits and surgeries).\nWhen working with data spread across multiple tables, you’ll often need to combine them based on a shared column (a “key column”)—a process known as joining or merging. dplyr provides several functions for common data joins.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\ninner_join() keeps only the rows where “key column” exists in both tables and drops all rows that do not have a match in both.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nlefy_join() keeps all rows from the left table (here, table1) and fills in matching information from the right table (here, table2). If no match is found, you will get NAs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nfull_join() keeps all rows from both tables. Rows without a match in the other table will have NAs.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can match on more than one column. For example, match ID and also make sure the gender in table1 matches sex in table2.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis is useful when one key (ID) is not unique enough by itself.\nFilter-based joins: These do not add new columns. They just filter rows:\nsemi_join() keeps only rows in table1 that have a match in table2. It does not add any columns from table2.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nanti_join() keeps only rows in table1 that do not have a match in table2. It is good for finding “missing” mathches.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nFor more information, plesee see chapter 19 of Wickham, Cetinkaya-Rundel, and Grolemund (2023) .\n\n\n\n\nExerciseHintsSolution\n\n\nUsing the starwars dataset and dplyr functions, perform the following data manipulations.\nPart 1: Initial Exploration and Filtering\n\nFilter for Human Characters: Create a new data frame called human_characters that only includes characters of the “Human” species.\nSelect Key Attributes: From human_characters, select only the name, height, mass, and homeworld columns.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPart 2: Calculating BMI and Identifying Extremes\n\nCalculate BMI: Add a new variable called bmi (Body Mass Index) to your human_characters data frame. The formula for BMI is \\(\\text{mass}/(\\text{height}/100)^2\\). Ensure that mass is in kilograms and height in centimeters as provided in the dataset.\nSort by BMI: Arrange the human_characters data frame in descending order based on their bmi.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPart 3: Grouped Summaries\n\nSpecies Statistics: Calculate the number of characters (n) and the average height for each species in the original starwars dataset.\nFilter Significant Species: From the previous summary, filter out species that have fewer than 5 characters and an average height greater than 100.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPart 4: Conditional Categorization\n\nCategorize Height: Add a new variable called height_category to the starwars dataset using mutate() and case_when().\n\nIf height is less than or equal to 100, categorize as “Short”.\nIf height is greater than 100 but less than or equal to 180, categorize as “Medium”.\nIf height is greater than 180, categorize as “Tall”.\nHandle NA values for height appropriately so they remain NA in height_category.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPart 1: Initial Exploration and Filtering\n\nFilter for Human Characters: which dplyr function helps you select rows based on a condition? Remember the syntax for checking equality.\nThere is a dplyr function specifially for choosing columns.\n\nPart 2: Calculating BMI and Identifying Extremes\n\nCalculate BMI: Which dplyr function is used to create or modify columns? Pay attention to the order of operations in the formula.\nSort by BMI: The arrange() function is key here. How do you specify descending order?\n\nPart 3: Grouped Summaries\n\nSpecies Statistics: You will need two main functions here: one to define groups and another to calculate summary statistics within those groups. Do not forget to handle NA values for the mean.\nFilter Significant Species: You will apply another filter() operation, but this time on the summarized data. Remember how to combine two conditions.\n\nPart 4: Conditional Categorization\n\nCategorize Height: case_when() is perfect for multiple conditions. How do you specify the conditions and their corresponding outputs? Remember to check for NAs first to ensure they do not get misclassified by other conditions.\n\n\n\n\n\nSolution. \nlibrary(dplyr)\n\n# Part 1: Initial Exploration and Filtering\nhuman_characters &lt;- starwars |&gt; \n  filter(species == \"Human\") |&gt; \n  select(name, height, mass, homeworld)\n\n# Part 2: Calculating BMI and Identifying Extremes\nhuman_characters_bmi &lt;- human_characters  |&gt; \n  mutate(bmi = mass / ((height / 100)^2))  |&gt; \n  arrange(desc(bmi))\n\n# Part 3: Grouped Summaries\nspecies_stats &lt;- starwars  |&gt; \n  group_by(species) %&gt;%\n  summarise(\n    n = n(),\n    avg_height = mean(height, na.rm = TRUE)\n  ) %&gt;%\n  filter(\n    n &gt;= 5, \n    avg_height &gt; 100\n  )\n\n# Part 4: Conditional Categorization\nstarwars_with_height_category &lt;- starwars %&gt;%\n  mutate(\n    height_category = case_when(\n      is.na(height) ~ NA_character_, # Handle NA values first\n      height &lt;= 100 ~ \"Short\",\n      height &gt; 100 & height &lt;= 180 ~ \"Medium\",\n      height &gt; 180 ~ \"Tall\"\n    )\n  )\n\n\n\n\nTo learn more about the dplyr package, here are some recommended resources:\n\ndplyr.tidyverse.org\nChapter “Data transformation” in the book “R for Data Science”\nCheatsheet\nBlog Stats and R\nKaggele\n\n\n\n\n\nWickham, Hadley, Mine Cetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd ed. https://r4ds.hadley.nz/.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html",
    "href": "chapters/chapter2_data_visualization.html",
    "title": "4  Data Visualization",
    "section": "",
    "text": "4.1 Data\nThe most famous package in R that is used for visualizing of data is ggplot2 that is based on the grammar of graphics. It allows you to `speak’ a graph from composable elements, instead of being limited to a predefined set of charts.\nBefore using ggplot2, a user must first install the package and then load it into their R session. Installation is done only using the command install.packages('ggplot2'). Alternatively, the following code can be used to install if only if the package is not already installed on the computer.\nwhich downloads the package from CRAN (the Comprehensive R Archive Network). After installation, the package needs to be loaded each time R is restarted. This is done with the following command:\nA brief overview about using this package is available at this website and more complete information about how to use ggplot2 can be found in Wickham, Navarro, and Pederson (2019).\nThe structure of the package includes 7 composable parts that come together as a set of instructions on how to draw a chart.\nThe package ggplot2 requires a minimum of three main components to create a chart: data, mapping, and a layer. Other components – like scales, facets, coordinates, and themes – are optional because ggplot2 gives them automatic settings that usually work well, so you do not need to adjust them too much.\nIn the following, we briefly describe these components:\nEvery plot made with ggplot2 starts with data. This data should be in a tidy format that means the data is in a table (called a rectangular data frame) where:\nThe first step to create a plot with ggplot2 is to pass this data to the ggplot() function, which stores the data to be used later by other parts of the plot.\nFor example, if we want to make a plot using the mpg dataset, we begin like this:",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#data",
    "href": "chapters/chapter2_data_visualization.html#data",
    "title": "4  Data Visualization",
    "section": "",
    "text": "Each row is one observation.\nEach column is one variable.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNoteYou should see an empty plot. It is correct. Can you explain why the plot is empty?",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#mapping",
    "href": "chapters/chapter2_data_visualization.html#mapping",
    "title": "4  Data Visualization",
    "section": "4.2 Mapping",
    "text": "4.2 Mapping\nIn ggplot2 package, mapping means telling the system how to connect parts of the data to aesthetic properties of the plot.\nAesthetic (say: es-THE-ik) is a word that describes how something looks – its style, color, shape, and beauty. In ggplot2, an aesthetic is a visual feature of a plot like:\n\n Position (x and y) – controls where data appears on the plot\n\n Color – changes color based on data values\n\n Size – adjusts the size of points\n\n Shape – uses different shapes for different categories\n\nThese help us turn numbers into pictures. It is how we ’’dress up” the data so it speaks to our eyes.\n\n\n\n\n\n\nTipMemory tip\n\n\n\nJust like fashion has aesthetic styles (like modern, classic, or colorful), plots also have aesthetics – they decide how the data looks!\n\n\nA mapping can be made by using aes() function to make pairs of graphical attributes and parts of the data. Inside aes(), we match parts of the data (like column names) with visual elements (like \\(x\\) and \\(y\\) position).\nFor the dataset mpg, if we want to show cty (city miles per gallon) on the \\(x\\)-axis and hwy (highway miles per gallon) on the \\(y\\)-axis, we write\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#layers",
    "href": "chapters/chapter2_data_visualization.html#layers",
    "title": "4  Data Visualization",
    "section": "4.3 Layers",
    "text": "4.3 Layers\nThe layer is the heart of any plot in ggplot2. A layer takes the mapped data and turns it into something that a human can see and understand – like points, lines, or bars.\nEach layer has three main parts: 1. Geometry – decides how the data is shown (for example: points, lines, bars) 2. Statistical transformation – can create new values from the data (like averages or smooth curves). 3. Position adjustment – controls where each part of the plot appears, especially when things overlap.\nA layer can be constructed using functions that start with geom_ (for geometry) and stat_ (for statistics). These function help us choose how the data looks and what to display.\n\n\n\n\n\n\nNote\n\n\n\nThe geom_*() and stat_*() functions usually control one part of the layer – like the geometry or the statistics – but you can still manullay choose the other two parts if you want.\n\n\nThe code below shows how to make a scatter plot with a trend line using two layers:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#scales",
    "href": "chapters/chapter2_data_visualization.html#scales",
    "title": "4  Data Visualization",
    "section": "4.4 Scales",
    "text": "4.4 Scales\nA scale translates what we see on the plot into something we can understand from the data – like showing how far, how big, or what category a point represents.\nEach scale is connected to an aesthetic – for example, the \\(x\\)-axis, \\(y\\)-axis, color, or size – and it controls things like - The limits of the plot (minimum and maximum values) - The breaks (where ticks or labels appear) - The format of the labels (like numbers or percentages) - Any transformation (like logarithmic scale)\nScales also create guides for the reader – like axes or legend – so we can better understand the meaning of the plot.\nScale functions in ggplot2 usually follow the format scale_[aesthetic]_[type](), where {aesthetic} is one of the pairing made in the mapping part of a plot. For example, scale_x_continuous() – for a numeric \\(x\\)-axis; scale_colour_viridis_d() – for discrete color with the Viridis palette.\nHere is how to use a custom color scale for the class variable in the mpg dataset:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis tells ggplot2 to use Viridis colors for the different car classes – making the plot easier to read and more accessible.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#facets",
    "href": "chapters/chapter2_data_visualization.html#facets",
    "title": "4  Data Visualization",
    "section": "4.5 Facets",
    "text": "4.5 Facets\nFacets are used to split a plot into smaller subplots, each showing a subet of the data. This is helpful when you want to compare groups – for example, different years, categories, or types – in separate panels.\nFacets are a powerful way to quickly see patterns, trends, or even no patterns in different parts of the data.\nFacets have their own mapping, written as a formula.\nThe following code creates one scatter plot of two variables cty and hwy of the dataset mpg for each combination of year and drv (driver type) in different panels; so, you can compare them side by side.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn facet_grid(year ~ drv), the rows are based on year and the columns are based on drv. As you see, this creates a grid of plots that makes it easy to explore how variables interact.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#coordinates",
    "href": "chapters/chapter2_data_visualization.html#coordinates",
    "title": "4  Data Visualization",
    "section": "4.6 Coordinates",
    "text": "4.6 Coordinates\nThe coordinates is the part if the plot that controls how position aesthetics (like \\(x\\) and \\(y\\)) are shown. You can think of it as the interpreter that tells the plot where to place things.\nMost ggplot2 plots use Cartesian coordinates, where data is shown on regular \\(x\\)-\\(y\\) grid. But you can also use other systems, like: Polar cooridinate (for circular plots) and Map projections (for geographic data).\nYou can also use coordinates to make sure that one unit on the \\(x\\)-axis is the same size as one unit on the \\(y\\)-axis. This is called a fixed aspect ratio. The function coord_fixed() does this ratio automatically.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#theme",
    "href": "chapters/chapter2_data_visualization.html#theme",
    "title": "4  Data Visualization",
    "section": "4.7 Theme",
    "text": "4.7 Theme\nThe theme controls parts of the plot that are not related to the data, such as background, text, grid, and legend position. It helps define the overall look and feel of the plot.\nFor instance, you can use the theme to: - Move or hide the legend - Change font size or colors - Set a new background style - Adjust the axes or grid lines\nSome them settings are hierarchical, meaning that if you change the general axis style, it also changes both the \\(x\\)-axis and \\(y\\)-axis unless you set them separately.\nTo change the appearance of a plot, you can use the built-in theme_*() functions (like theme_minimal(), theme_classic(), or theme_light()), or you can customize specific details using the theme() function. With element_*() functions (e.g. element_line(), element_text()) let you adjust the visual style of theme parts, such as lines, texts, or backgrounds.\nThe following code creates a scatter plot and applies a minimal theme. It also moves the legend to the top, makes axis lines thicker, and colors the bottom \\(x\\)-axis blue.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2_data_visualization.html#mastering-plot-layers-guided-examples-and-student-activities",
    "href": "chapters/chapter2_data_visualization.html#mastering-plot-layers-guided-examples-and-student-activities",
    "title": "4  Data Visualization",
    "section": "4.8 Mastering Plot Layers: Guided Examples and Student Activities",
    "text": "4.8 Mastering Plot Layers: Guided Examples and Student Activities\nAs we mentioned before, a ggplot2 is made by layering different parts. You can combine data, mapping, geoms, scales, facets, coordinates, and themes to build a fully customized plot.\nBelow is an example that brings everything together:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nThere is no need to type argumants.\n\n\n\n4.8.1 Scatter Plot – geom_point()\nA scatter plot shows the relationship between two numeric variables. each point on the plot represents one observation.\n\n\n\n\n\n\nTip\n\n\n\nScatter plots are useful for seeing patterns, trends, or outliers in data.\n\n\nAn example for seeing the basic scatter plot is showing how height (in inches) changes with age (in years) for each person in the dataset heightweight.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nwe can customize the size of points by adding size = 1.5 to the function geom_point().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, we can add two aesthetics: shape and color, based on the sex variable.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, we are mapping colour or size to a numeric variable (e.g. weightLb)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThese plots show how weight varies with age and height using color or size.\nNow, we are mapping colour to the variable sex and size to a numeric variable weightLb\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExerciseHintsSolution\n\n\nUsing the heightweight dataset, make a scatter plot to explore the relationship between heightIn and weightLb variables. Your tasks are:\n\nUse points to represent the data.\nShow the variable sex using color so that we can see the difference between males and females.\nMake the points larger than usual, with a size of \\(2.5\\).\nFinally, give your plot a title that describes what it shows, The title should be: “Height vs, Weight by Sex”\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nYou will need\n\naes(x = ______, y = ______, colour = ______) -\ngeom_point(size = ______)\nggtitle(______)\n\n\n\n\n\nSolution. \nggplot(heightweight, aes(x = weightLb, y = heightIn, colour = sex)) + \n  geom_point(size = 2.5) +\n  ggtitle(\"Height vs. Weight by Sex\")\n\n\n\n\n\n\n4.8.2 Line Plot – geom_line()\nUse when: You want to show how a variable changes over time or another ordered variable (like day, year, index)\nWhat is shows: Trends, increases or decreases, cycles over time\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis plot shows how unemployment has changed over time.\n\n\n4.8.3 Barplot – geom_bar() and geom_col()\nUse when: You want to compare counts or values between categories.\n\ngeom_bar() counts how many times each category appears.\ngeom_col() shows values you provided directly.\n\nWhat it shows: Frequencies or values across different categories.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.4 Histogram – geom_histogram()\nUse when : You want to see the distribution of a numeric variable.\nWhat is shows : How values are spread across intervals (called “bins”).\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis plot shows many cars fall into different highway mpg ranges.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.5 Boxplot – geom_boxplot()\nUse when: You want to compare summary statistics (like median, quartiles, ouliers) across different groups.\nWhat it shows : Median, spread, and outliers for each group.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis plot compare highway mpg for different type of vehicles.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.6 Violin Plot – geom_violin()\nUse when: You want to see the distribution shape plus summary info for groups.\nWhat it shows: Like a boxplot, but with a mirrored density curve – great for showing the full distribution and comparison across groups.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can also add add points or boxplots inside the violins:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.7 Dot plot – geom_dotplot()\nUse when: You want to show individual values, especially for small datasets.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.8 Density Plot – geom_density()\nUse when: You want a smoothed version of a histogram.\nWhat is shows: Estimate of the data’s distribution, good for seeing shapes or peaks.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can combine the histogram and density plot to show exact counts (histogram) and smooth distribution (density) in one plot.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNoteWhat is after_stat(density)?\n\n\n\nIn ggplot2, when you use geom_histogram(), the default \\(y\\)-axis is count (how many observations fall in each bin). But if you write aes(y = after_stat(density)), you are asking ggplot2 to scale the height of the bars so they represent probability density instead of raw counts. This allows you to compare this histogram with a density curve (like the one from geom_density()), which also shows density.\n\n\nThis shows both frequency and smoothed distribution of highway mpg.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.9 Pie Chart (using Polar Coordinates)\nUse when: You want to show parts of a whole, but use with caution – bar plots are often easier to read.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.10 2D Density Plot\nA 2D density plot is used to show the distribution of two numeric variables. Instead of plotting each point (like in a scatter plot), it shows where points are concentrated using contour lines (geom_density_2d()) and filled contour areas (geom_density_2d_filled()).\nUse when: you have many overlapping points (overlapping) in a scatter plot and you want to highlight patterns or clusters in two numeric variables and see a smoother version of the joint distribution.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAdding points makes it easier to see what the contours represent.\n\nggplot(faithful, aes(x = waiting, y = eruptions)) +\n  # Add Data Points for Context\n  geom_point(alpha = 0.3) +\n  geom_density_2d()\n\n\n\n\n\n\n\n\nThe following code results in a plot that shows filled bands of density that it is great for visual impact.\n\nggplot(faithful, aes(x = eruptions, y = waiting)) + \n  geom_density_2d_filled(alpha = 0.6) \n\n\n\n\n\n\n\n\nCombining points and filled contours gives both precision and overview.\n\nggplot(faithful, aes(x = eruptions, y = waiting)) +\n  geom_point(alpha = 0.3) +\n  geom_density_2d_filled(alpha = 0.5) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nYou can map density levels to color using after_stat(level)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou can also different code to produce the plot.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.11 2D Binned Plot for Big Data – stat_bin2d()\nUse when: You have a lot of points and scatter plots are too dense.\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.12 Frequency Polygon – geom_freqpoly()\nA frequency plot is similar to a histogram, but it uses lines instead of bars to show how a variable is distributed. It is especially useful when you want to compare distributions across groups.\nUse when: You want to show the shape of a distribution and also compare multiple groups using lines (which may be easier to read than overlapping histograms).\nExamples\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n4.8.13 Correlation Plot (Cor plot)\nA correlation plot (or cor plot) is a visual way to show how strongly variables are related to each other. Correlation values range from \\(-1\\) to \\(1\\), and the plot helps you quickly see positive, negative, or no relationships.\nThis is not built into ggplot2, but we can use it together with\n\nThe cor() function to compute correlation\nThe corrplot or ggcorrplot package to visualize it.\n\nExample using ggcorrplot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nExample using corrplot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExercisesHints\n\n\nFor the following datasets (data description: DataDescriptions.pdf):\n\nCeoCompensation\nMassBodilyInjury\nRefrigerator\n\nPerform an exploratory data analysis:\n\nUnivariate analysis\nBivariate analysis\nOutlier analysis\n\nReport any conclusions using appropriate descriptive statistics and graphical visualizations.\n\n\n\nConsider the distribution of each variable in the univariate analysis.\nIn the bivariate analysis, look for relationships between pairs of variables.\nFor outlier analysis, identify any unusual observations that may affect your results. scatter plots to visualize potential outliers.\n\n\n\n\n\n\n\n\nWickham, Hadley, Danielle Navarro, and Thomas Lin Pederson. 2019. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. https://ggplot2-book.org/.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "chapters/project.html",
    "href": "chapters/project.html",
    "title": "5  Project",
    "section": "",
    "text": "5.1 Exercise 1\nPlease copy this R code into your computer and try to solve it.\ndownload.file(\n  url = \"https://raw.githubusercontent.com/mnrzrad/SIE/main/exercises/Exercise_EDA.Rmd\",\n  destfile = \"Exercise_EDA.Rmd\",\n  method = \"curl\"\n)\nThis code saves the file into your current working directory.\nAfter, chech where the file was saved using the code\ngetwd()\nThis code tells you the working directory (the folder where R is looking and saving files).\nThen, you can list files in that folder to confirm.\nlist.files()\nNow, open it in R studio.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project</span>"
    ]
  },
  {
    "objectID": "chapters/project.html#exercise-1",
    "href": "chapters/project.html#exercise-1",
    "title": "5  Project",
    "section": "",
    "text": "In RStudio, go to File → Open File\nNavigate to the folder shown by getwd()\nSelect Exercise_EDA.Rmd → Open",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Project</span>"
    ]
  },
  {
    "objectID": "chapters/Exam-Sample.html",
    "href": "chapters/Exam-Sample.html",
    "title": "6  Sample of Questions",
    "section": "",
    "text": "6.1 Sample 1\nIntroduction\nIn this exercise, we will perform Exploratory Data Analysis (EDA) using the penguins dataset from the palmerpenguins package.\nAt each step, you will find a question to test your understanding.\nStep 1: First Look\nThe dataset has 344 rows and 8 variables. Some values are missing.\nQuestion 1. How many variables are categorical?\nCorrect Answer: B\nStep 2. Numerical Summaries\nThe body mass ranges from about 2700g to 6300g.\nQuestion 2. Which measure is most robust to outliers?\nCorrect Answer: C\nStep 3: Visualization – Boxplot\nGentoo penguins appear heavier than Adelie and Chinstrap.\nQuestion 3. What does each box in the boxplot represent?\nCorrect Answer: C\nStep 4: Relationships Between Variables\nFlipper length and body mass are positively related.\nQuestion 4. What does a positive relationship mean here?\nCorrect Answer: A\nStep 5: Missing Data\nThe variable sex has missing values.\nQuestion 5. Which R function checks if any missing values exist in the dataset?\nCorrect Answer: B",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sample of Questions</span>"
    ]
  },
  {
    "objectID": "chapters/Exam-Sample.html#sample-1",
    "href": "chapters/Exam-Sample.html#sample-1",
    "title": "6  Sample of Questions",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n2\n3\n4\n5\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nMean\nStandard deviation\nInterquartile Range (IQR)\nVariance\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe entire dataset    \nDistribution of body mass for one species\nOnly the mean of each species    \nOutliers only\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nLarger flipper length tends to go with higher body mass\nFlipper length causes body mass\nThere is no association\nOutliers explain the pattern\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nis.na()\n\nanyNA()\nall()\nna.rm()",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sample of Questions</span>"
    ]
  },
  {
    "objectID": "chapters/Exam-Sample.html#sample-2",
    "href": "chapters/Exam-Sample.html#sample-2",
    "title": "6  Sample of Questions",
    "section": "6.2 Sample 2",
    "text": "6.2 Sample 2\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nwe will explore the mtcars dataset, which contains fuel consumption and performance information about \\(32\\) car models from 1974.\nWe will perform EDA and answer questions along the way.\nStep 1. First Look\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe dataset has \\(32\\) observations and \\(11\\) variables.\nQuestion 1. How many variables are numeric?\n\n8\n9\n10\n11\n\nCorrect Answer: B\nStep 2: Descriptive Statistics\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe miles-per-gallon (mpg) ranges from \\(10.4\\) to \\(33.9\\).\nQuestion 2. Which value best describes the typical mpg while being robust to outliers?\n\n\\(20.9\\)\n\\(19.20\\)  \n\\(7.375\\)   \n\\(6.026948\\)\n\nCorrect Answer: B\nStep 3: Categorical Variable – Cylinders\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCars come with \\(4\\), \\(6\\), or \\(8\\) cylinders.\nQuestion 3. Which cylinder type is most common in the dataset?\n\n4 cylinders\n6 cylinders\n8 cylinders\n\nAll equally common\n\nStep 4: Visualization – Boxplot\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nQuestion 4. What does the plot suggest?\n\nCars with more cylinders have higher mpg\nCars with more cylinders have lower mpg\nCars with more cylinders have identical mpg\nNo relationship is visible\n\nCorrect Answer: B\nStep 5: Relationship Between Variables\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nQuestion 5. What trend do we see between horsepower (hp) and miles per gallon (mpg)?\n\nHigher horsepower cars tend to have lower mpg\nHigher horsepower cars tend to have higher mpg\nNo relationship\nOnly automatic cars follow this trend\n\nCorrect Answer: A\nStep 6: Correlation\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nQuestion 6. What does the correlation between hp and mpg mean?\n\nStrong positive relationship\nStrong negative relationship\nWeak negative relationship\nNo relationship\n\nCorrect Answer: B",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Sample of Questions</span>"
    ]
  },
  {
    "objectID": "chapters/Outlier_Detection.html",
    "href": "chapters/Outlier_Detection.html",
    "title": "7  Outlier Detection: Understading and Handling",
    "section": "",
    "text": "7.1 Definition and Causes of Outliers",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Outlier Detection: Understading and Handling</span>"
    ]
  },
  {
    "objectID": "chapters/Outlier_Detection.html#definition-and-causes-of-outliers",
    "href": "chapters/Outlier_Detection.html#definition-and-causes-of-outliers",
    "title": "7  Outlier Detection: Understading and Handling",
    "section": "",
    "text": "7.1.1 What are outliers?\nAn outlier is an observation that differs significantly from other observations.\nOutliers may appear as unusually large or small values and are sometimes referred to as anomalies.\nThe ouliers can represent important, rare, or faulty observations. Therefore, detecting and interpreting outliers should always involve contextual and domain-specific judgment — not just automated rules.\n\n\n\n\n\n\nNote\n\n\n\nAn observation must always be compared to other observations made on the same phenomenon before actually calling it an outlier. Indeed, someone who is 2 meters tall will most likely be considered an outlier compared to the general population, but that same person may not be considered an outlier if we measured the height of basketball players.\n\n\n\n\n7.1.2 Common Causes of Outliers\nOutliers can arise for many reasons, including:\n\nMeasurement or data entry errors – inaccurate recordings or manual input mistakes.\nData corruption – issues during data transmission or storage.\nExperimental anomalies – unexpected events or conditions during data collection.\nTrue but rare events – legitimate observations that occur infrequently.\nSampling from different populations – combining heterogeneous data sources.\n\nFor example, it is often the case that there are outliers when collecting data on salaries, as some people make much more money than the rest.\nOutliers can also arise due to an experimental, measurement, or encoding error. For instance, a human weighing 786 kg is clearly an error when encoding the weight of the subject. Her or his weight is most probably 78.6 kg (173 pounds) or 7.86 kg, depending on whether the weights of adults or babies have been measured.\nFor this reason, it sometimes makes sense to formally distinguish two classes of outliers: (i) extreme values and (ii) mistakes. Extreme values are statistically and philosophically more interesting, because they are possible but unlikely responses.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Outlier Detection: Understading and Handling</span>"
    ]
  },
  {
    "objectID": "chapters/Outlier_Detection.html#methods",
    "href": "chapters/Outlier_Detection.html#methods",
    "title": "7  Outlier Detection: Understading and Handling",
    "section": "7.2 Methods",
    "text": "7.2 Methods\n\n7.2.1 Minimum and Maximum\nThe first step to detect outliers is to start with some descriptive statistics. In particular, with the minimum and maximum. ‍‍\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSome clear mistake like a weight of \\(786\\)kg for a human will already be easily detected by this simple technique.\n\n\n7.2.2 Check the summary statistics\nOutliers can drastically affect the results of data analysis and statistical modeling. They can distort summary statistics, such as the mean and standard deviation, leading to misleading conclusions.\nLet’s look at a simple example:\n\nlibrary(dplyr)\n\n# Data\n# Without outliers\nset1 &lt;- c(4,4,5,5,5,5,6,6,6,7,7)\n# With outliers\nset2 &lt;- c(4,4,5,5,5,5,6,6,6,7,7,300)\n\n# Function to compute summary statistics\nsummary_stats &lt;- function(x) {\n  tibble(\n    Mean = mean(x),\n    Median = median(x),\n    Mode = as.numeric(names(which.max(table(x)))),\n    SD = sd(x)\n  )\n}\n\n# Apply to both sets and combine results\nresults &lt;- bind_rows(\n  summary_stats(set1) %&gt;% mutate(Dataset = \"Set 1\"),\n  summary_stats(set2) %&gt;% mutate(Dataset = \"Set 2\")\n) %&gt;%\n  select(Dataset, Mean, Median, Mode, SD)\n\n# Display results\nresults\n\n# A tibble: 2 × 5\n  Dataset  Mean Median  Mode    SD\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Set 1    5.45    5       5  1.04\n2 Set 2   30       5.5     5 85.0 \n\n\nBased on the output, you will notice that the mean and standard deviation of the dataset containing outlier are much larger than those of the dataset without outlier. Here, the average increases from \\(5.45\\) in the dataset without outliers to \\(30\\) when outlier is included – completley changing the estimate.\nYou can also see the difference between the standard deviation values in the two datasets. standard deviation can be a warning sign that outliers may be present.\n\n\n\n\n\n\nTip\n\n\n\nAlways compare mean vs. median and check the standard deviation – sudden jumps in these values may indicate the presence of outliers.\n\n\nAnother simple way to detect ouliers is by plotting a histogram of the data. This allows you to visualize the overall distribution and spot unusually high or low values.\n\n\n\n\n\n\nTip\n\n\n\nA good rule of thumb for the number of bins is to set it to the square root of the number of observations.\n\n\nWe can create a histogram using base R or ggplot2:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFrom the histogram, we can see a few observations that are noticeably higher than the rest – indicated by the bar on the far right side of the plot.\nIn addition to histogram, boxplots are also useful to detect potential outliers.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAs we discussed previously, a boxplot shows the distribution of a numerical variable using five key values – the minimum, first quantile (\\(Q_1\\)), median, third quantile (\\(Q_3\\)), and maximum – and highlights any suspected outliers based on the interquartile range (\\(\\text{IQR} = Q_3 - Q1\\)) rule.\n\n\n\n7.2.3 Tukey’s IQR Rule\nTukey defined outliers as points that fall below \\(Q_1 - 1.5 * \\text{IQR}\\) or above \\(Q_3 + 1.5 * \\text{IQR}\\), providing a simple quantitative rule for detecting values that differ significantly from the main body of the data.\n\n\n\n\n\n\nTip\n\n\n\nThe coefficient \\(1.5\\) is not fixed – it can be adjusted depending on the context and assumptions. However, in practice, the \\(1.5\\) rule is the most commonly used standard.\n\n\nWe can define a function in R to find outliers based on the IQR rule:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInstead of creating a separate function, we can directly extract potential outliers using R’s built-in boxplot.stats()$out function, which applies Tukey’s IQR rule automatically.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAs you can see, there are three potential outliers: two observations with a value of 44 and one with a value of 41.\nWe can easily find their row numbers in the dataset using the which() function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWith this information, you can now easily return to the corresponding rows in the dataset to verify them or review all variables for those observations.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nTip\n\n\n\nThis step helps verify whether these unusual observations are data entry errors or genuine extreme cases that may need special attention.\n\n\n\n\n\n\n\n\nTip\n\n\n\nAnother method to display these specific rows is with the identify_outliers() function from the {rstatix} package:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n7.2.4 Standard deviation method\nIf the data follows a roughly normal (bell-shaped) distribution, we can use the standard deviation (SD) to identify outliers.\nThe standard deviation measures how far the data values are spread from the mean.\nFor a normal distribution:\n\n\\(68\\%\\) of data lies within \\(\\pm 1\\) SD\n\n\\(69\\%\\) of data lies within \\(\\pm 2\\) SD\n\n\\(99.7\\%\\) of data lies within \\(\\pm 3\\) SD\n\nValues that lie more than 2 or 3 SDs away from the mean are often considered potential outliers.\n\n\n\n\n\n\n\nNote\n\n\n\nThis method can sometimes miss outliers because extreme values make the standard deviation larger.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n7.2.5 \\(Z\\)-score method\nIf the dataset follows a normal distribution, instead of the standard deviation method, we can transform the data into standard scores (or \\(Z\\)-scores) so that the mean becomes \\(0\\) and the standard deviation becomes \\(1\\).\nThis transformation gives us the \\(Z\\)-score for each observation, which indicates how many standard deviations an observation is from the mean. It can be done with the scale() function in R.\nAccording to the empirical rule, observations with a \\(Z\\)-score greater than \\(3\\) or less than \\(-3\\) are considered as outliers (extremly rare). Values bewtween \\(|Z| = 2\\) and \\(|Z| = 3\\) are often described as “moderatly unusal” or rare.\n\n\n\n\n\n\n\nTip\n\n\n\nSome analysts use a slightly stricter rule — considering values with \\(Z &lt; −3.29\\) or \\(Z &gt; 3.29\\) as outliers.\nWhy? The \\(Z = \\pm 3\\) rule comes from the empirical (\\(68\\)-\\(95\\)-\\(99.7\\)) rule, which is an approximately for normal distribution. It says that about \\(99.7\\%\\) of the data lies within \\(3\\) standard deviations from the mean. That means roughly \\(0.3\\%\\) (or 3 in 1000) of observations would lie outside this range. However, if we calculate it more precisely from standard normal distribution, we find the \\(Z\\)-value that cuts off exactly \\(0.1\\%\\) in each tail (so that only %0.2%$ total or 2 in 1000 of obeservations lie beyond this point) is approximately \\(3.29\\). So, \\(|Z| &gt; 3\\) means roughly \\(0.3\\%\\) of data, while \\(|Z| &gt; 3.29\\) means exactly \\(0.1\\%\\) per tail.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n7.2.6 Modified \\(Z\\)-score method\nThe \\(Z\\)-score can be strongly affected by extreme values since it relies on the mean and standard deviation.\nTo make the method more robust, we use the modified \\(Z\\)-score, which is based on the median and median absolute deviation (MAD) instead.\nThe formula is: \\[\n\\text{Modified $Z$-Score} = \\frac{0.6745 (x_i - \\text{Median})}{\\text{MAD}}\n\\] where \\(x_i\\) is individual observation, and \\(\\text{MAD}=\\text{median}(|x_i - \\text{median}(x_i)|)\\).\n\n\n\n\n\n\nTip\n\n\n\nThe constant 0.6745 scales the MAD so that, under normality, the median absolute deviation estimates the standard deviation.\n\n\nThe MAD messures the spread of the data using the median instead of the mean, making it less sensitive to outliers than the standard deviation.\nIf the data are normally distributed, the standard deviation is typically preferref but for non-normal data, the MAD provides a more reliable measure of variablilty.\nThe outliers are identifed by \\[\n|\\text{Modified $Z$-Score}| &gt; 3.5\n\\] The value of \\(3.5\\) was suggested by Iglewicz and Hoaglin (1993) in “How to Detect and Handle Outliers”, and is widely used because it balances sensitivity (detecting real anomalies) with robustness (avoiding false positives).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n7.2.7 Hampel Filte\nThe Hample filter is an bust outlier detection method based on the median and the MAD – just like the modified \\(Z\\)-score method.\nInstead of using the mean and standard deviation, it defines an interval around the median and flags as outliers any observations that fall outside this interval.\nThe interval is defined as: \\[\nI = [\\text{Median} - k * \\text{MAD}, \\text{Median} + k * \\text{MAD}]\n\\] where \\(k\\) is typically \\(3\\).\nThis method is particularly useful for data that may not be normally distributed or when the dataset contains extreme values that could skew the mean and standard deviation.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n7.2.8 Percentiles\nThe percentile method detects outliers by defining an interval that includes most of the data, based on selected percentiles. Any observations outside this interval are considered potential outliers.\nThe most common choice is to use the \\(2.5\\)th and \\(97.5\\)th percentiles, which corresponds to keeping \\(95\\%\\) of the central data and treating the remaining \\(5\\%\\) as potential outliers.\nOther cutoff points, such as \\(1\\)st and \\(99\\)th or \\(5\\)th and \\(95\\)th percentiles, can also be used depending on how strict you want the detection to be.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Outlier Detection: Understading and Handling</span>"
    ]
  },
  {
    "objectID": "chapters/Outlier_Detection.html#what-to-do-after-detecting-outliers",
    "href": "chapters/Outlier_Detection.html#what-to-do-after-detecting-outliers",
    "title": "7  Outlier Detection: Understading and Handling",
    "section": "7.3 What to Do After Detecting Outliers",
    "text": "7.3 What to Do After Detecting Outliers\nDetecting outliers is only the first step — the most important part is deciding how to handle them. Outliers are not always “bad data” — they can be errors, rare but real events, or important signals.\nThe correct action depends on context and domain knowledge.\n\n7.3.1 Verify and Investigate\nBefore taking any decision:\n\nCheck for data entry errors, unit mismatches, or measurement problems.\nPlot the data again with and without outliers — see how much they influence the result.\nCompare summaries (mean, median, variance) before and after removing outliers.\n\n\n\n7.3.2 Keep the Outliers (if they are valid)\nIf the outliers represent real, meaningful phenomena, they should remain in the dataset. For example,\n\nextremely high incomes in economics data\nRare but valid sensor readings in engineering.\nExceptional performance in experiments.\n\nIn such cases, you may use robust statistical methods (e.g. median, MAD) that are less sensitive to extreme values.\n\n\n7.3.3 Remove or Winsorize (if they are incorrect)\nIf you find that the outliers come from a typographical or sensor errors, sampling issues, or wrong data units (e.g., °C vs. °F), then it’s reasonable to:\n\nRemove them, or\nWinsorize them — i.e., replace extreme values with the nearest valid percentile (e.g. 1st or 99th).\n\nThis keeps the data’s overall structure but limits the impact of extreme points.\n\n\n7.3.4 Analyze With and Without Outliers\nA good analytical habit is to perform the analysis both ways: once with all data and once excluding outliers. This helps assess how much the outliers influence the results and conclusions. If the results differ greatly, the outliers deserve closer attention.\n\n\n7.3.5 Report Your Decision\nAlways document your reasoning:\n\nWhich method you used for detection,\nHow many outliers were found,\nWhat you decided to do, and why.\n\nTransparency ensures your analysis remains reproducible and credible.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Outlier Detection: Understading and Handling</span>"
    ]
  },
  {
    "objectID": "chapters/Outlier_Detection.html#exercise",
    "href": "chapters/Outlier_Detection.html#exercise",
    "title": "7  Outlier Detection: Understading and Handling",
    "section": "7.4 Exercise",
    "text": "7.4 Exercise\nYou are given the following dataset:\n\ndata &lt;- c(\n  47.19, 48.84, 57.79, 50.35, 50.64, 58.57, 52.30,\n  43.67, 46.56, 47.77, 56.12, 51.79, 52.00, 50.55,\n  47.22, 58.93, 52.48, 40.16, 53.50, 47.63, 44.66,\n  48.91, 44.86, 46.35, 46.87, 41.56, 54.18, 50.76,\n  44.30, 56.26, 52.13, 48.52, 54.47, 54.39, 54.10,\n  53.44, 52.76, 49.69, 48.47, 48.09, 46.52, 48.96,\n  43.67, 60.84, 56.03, 44.38, 47.98, 47.66, 53.89,\n  49.58, 51.26, 49.85, 49.78, 56.84, 48.87, 57.58,\n  42.25, 52.92, 50.61, 51.07, 51.89, 47.48, 48.33,\n  44.90, 44.64, 51.51, 52.24, 50.26, 54.61, 60.25,\n  47.54, 38.45, 55.02, 46.45, 46.55, 55.12, 48.57,\n  43.89, 50.90, 49.30, 50.02, 51.92, 48.14, 53.22,\n  48.89, 51.65, 55.48, 52.17, 48.37, 55.74, 54.96,\n  52.74, 51.19, 46.86, 56.80, 46.99, 60.93, 57.66,\n  48.82, 44.86, 100.58, 95.47, 81.32, 10.11, -1.87,\n  -25.52, -34.14, -6.05, 20.78, 0.01\n)\n\n\n7.4.1 Part 1 — Using Tukey’s IQR Rule\n\nCompute the first quartile (Q1), third quartile (Q3), and interquartile range (IQR).\nIdentify outliers using the standard \\(1.5 \\times IQR\\) rule.\nHighlight them in a scatter plot.\nHow many potential outliers are there, and where are they located?\n\n\n\n7.4.2 Part 2 — Using the Standard Deviation Method\n\nCalculate the mean and standard deviation of the dataset.\nIdentify outliers as points that lie more than 2 standard deviations away from the mean.\nHighlight them in a scatter plot.\n\n\n\n7.4.3 Part 3 — Using the Z-score Method\n\nCompute the Z-score for each observation (standardize the data).\nMark as outliers the values with \\(|Z| &gt; 3\\) (or \\(|Z| &gt; 3.29\\) for a stricter rule).\nCompare your list of outliers with the ones found using the IQR method.\nPlot both methods’ results — do they detect the same observations?\n\n\n\n7.4.4 Part 4 – Discussion\n\nWhich method detected more outliers?\nAre there points that one method flagged as outliers but not the other?\nWhich method do you think is more reliable for this dataset — and why?\n\n\n\n7.4.5 Optional Challenge\nApply the Modified Z-score method or Percentile method (1%–99%) to the same dataset. Do the results change? What does that tell you about the robustness of different detection methods?",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Outlier Detection: Understading and Handling</span>"
    ]
  },
  {
    "objectID": "chapters/missing_values.html",
    "href": "chapters/missing_values.html",
    "title": "8  Missing Values",
    "section": "",
    "text": "8.1 Importance of Handling Missing Values\nMissing values happen when data for an observation is not recorded. This can occur if, for example, a participant drops out of a study, a machine fails, someone skips a survey question, or a recording error is made.\nMissing data are usually shown as an empty cell, a \\(.\\) (dot), or a \\(*\\) (asterisk). Invalid entries – like text in a numeric column or a N/A error – are also treated as missing.\nHandeling missing values is essential for accurate and reliable analysis. Key reasons include:",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Missing Values</span>"
    ]
  },
  {
    "objectID": "chapters/missing_values.html#importance-of-handling-missing-values",
    "href": "chapters/missing_values.html#importance-of-handling-missing-values",
    "title": "8  Missing Values",
    "section": "",
    "text": "Better Model Accuracy: Fixing missing data improves predictions and model performance.\nMaintained Sample Size: Imputation or careful removal keeps more data available for analysis.\nReduced Bias: Proper handeling prevents distorted or misleading results.\nStronger Decisions: Clean data leads to more confident and trustworthy conclusions.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Missing Values</span>"
    ]
  },
  {
    "objectID": "chapters/missing_values.html#challenges-caused-by-missing-values",
    "href": "chapters/missing_values.html#challenges-caused-by-missing-values",
    "title": "8  Missing Values",
    "section": "8.2 Challenges Caused by Missing Values",
    "text": "8.2 Challenges Caused by Missing Values\nMissing data can create problems such as\n\nSmaller Sample Size: Removing rows with missing values reduces data and reliability.\nBiased Results: Non-random missing data can lead to false conclusions.\nLimited Analysis: Many methods can not handle missing data, restricting analysis options.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Missing Values</span>"
    ]
  },
  {
    "objectID": "chapters/missing_values.html#common-reasons-for-missing-data",
    "href": "chapters/missing_values.html#common-reasons-for-missing-data",
    "title": "8  Missing Values",
    "section": "8.3 Common Reasons for Missing Data",
    "text": "8.3 Common Reasons for Missing Data\nData may be missing due to:\n\nTechnical issues: Equipment failure or data transfer errors.\nHuman errors: Mistakes in entry or recording.\nPrivacy concerns: Withholding sensitice information.\nProcessing problems: Errors during data preparation or transformation.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Missing Values</span>"
    ]
  },
  {
    "objectID": "chapters/missing_values.html#figure-out-why-the-data-is-missing",
    "href": "chapters/missing_values.html#figure-out-why-the-data-is-missing",
    "title": "8  Missing Values",
    "section": "8.4 Figure out Why the Data Is Missing",
    "text": "8.4 Figure out Why the Data Is Missing\nBefore handling missing values, it is IMPORTANT to understand why they are missing – this is part of what we can call data intuition, or the ability to look closely at your data and reason about what is happening.\nAsk yourself this key question:\nIs the value missing because it was recorded, or because it does not exist?\n\nIf it does not exist (for example, the height of the oldest child of someone who does not have any children), it does not make sense to fill it in – keep it as NaN.\nIf it was not recorded (for example, a measurement that someone forgot to take), you may estimate it using other available data. This process is called imputation.\n\nUndertanding the reason behind missing values you decide whether to leave them as missing or fill them in intelligently.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Missing Values</span>"
    ]
  },
  {
    "objectID": "chapters/missing_values.html#example-dataset",
    "href": "chapters/missing_values.html#example-dataset",
    "title": "8  Missing Values",
    "section": "8.5 Example Dataset",
    "text": "8.5 Example Dataset\nLet us start with an example, the airquality dataset, which records daily air quality measurements in New York (1973).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIt has missing values in Ozone and Solar.R columns.\nAlso, we can cofirm this using anyNA().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nTRUE indicates a presence of missing values in our data.\nThere are some libraries that can help us visualize missing data patterns, such as naniar, VIM, and mice\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n# Install (if needed)\n# install.packages(\"VIM\")\nlibrary(VIM)\n\n# Aggregation plot of missingness\naggr(airquality, numbers = TRUE, prop = FALSE)\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Missing Values</span>"
    ]
  },
  {
    "objectID": "chapters/missing_values.html#types-of-missing-data",
    "href": "chapters/missing_values.html#types-of-missing-data",
    "title": "8  Missing Values",
    "section": "8.6 Types of Missing Data",
    "text": "8.6 Types of Missing Data\nSometimes, we expect a value in our dataset, but it is not there. For example, a survey respondent may skip a question, or a sensor may fail to record a measurement, or a file got lost. These are missing values. But not all missing values happen for the same reason – and the reason matters, because it affects how we should handle them.\n\nMissing Completely at Random (MCAR): The missingness is entirely random and unrelated to any other data.\n\nThink of this as:\n\nThe data went missing by accident.\n\nThis missingness has nothing to do with any variable in your dataset. It is just a bad luck – like someone spilled coffee on some forms or a sensor failed. Everyone had an equal chance of having missing data.\nFor example, a few lab samples are lost because a fridge broke. This has nothing to do with the people or the measurements. ::: callout-important If data are MCAR, your analysis will still be fair and unbiased, even if you remove those cases. :::\n\nMAR – Missing at Random\n\nThink of this as:\n\nThe data is missing for a reason we know about.\n\nThis missingness is related to other variables that we have data for. But it is not related to the value that is missing itself.\nFor example, you ask people about their income, and you notice that younger people are less likely to answer. So the missingness depends on age (which you know), not on income itself.\n\n\n\n\n\n\nImportant\n\n\n\nYou can fix this by using the other variables (like age) to estimate or impute the missing ones.\n\n\n\nMNAR – Missing Not at Random\n\nThink of this as:\n\nThe data is missing because of the value itself.\n\nThe missingness is related to the thing that is missing or maybe you do not know the reason at all.\nFor example, some people do not report their weight because they feel uncomfortable about it – so the missingness depends on weight itself.\n\n\n\n\n\n\nImportant\n\n\n\nThis is the hardes case – because we can not fix it easily. You may need special models or collect more data.\n\n\n\nExerciseSolution\n\n\nFor each of situation below, decide whether the missing data are: (A) Missing Completly at Random (MCAR), (B) Missing at Random (MAR), or (C) Missing Not at Random (MNAR).\nQuestion 1 During data entry, some survey responses were lost because the computer crashed before saving. What type of missingness is this?\nQuestion 2 In a health survey, people with higher income are less likely to reveal their income, but everyone answered all other questions. What type of missingness is this?\nQuestion 3 A researcher measures blood pressure, but the cuff sometimes fails to inflate due to a machine error, affecting random participants. What type of missingness is this?\nQuestion 4 In a questionnaire about mental health, people with higher levels of stress are more likely to skip the “stress level” question. What type of missingness is this?\nQuestion 5 Some students forgot to write their age, but we notice that older students tend to complete the survey more carefully and rarely miss answers. What type of missingness is this?\nQuestion 6 A dataset on patient weight is missing for extremely obese individuals because the hospital’s scale cannot record weights above \\(200\\) kg. What type of missingness is this?\nQuestion 7 An old version of a form did not include the “education level” question, but the new form does. What type of missingness is this?\nQuestion 8 After a customer service call, customers are asked to rate their experience with the representative. Some customers give feedback, and others do not. Assume that the decision to give feedback is completely random, with no connection to the customer’s experience or any other factor. What type of missing data best describes the missing values in the feedback column?\nQuestion 9 In a dermatology clinic, a survey is conducted asking patients about their gender and their skincare routine. It turns out that women are more likely to respond, while men often skip the question. What type of missing data best describes the missing values in the skincare routine column?\n\n\n\nSolution.  Question 1: MCAR – Data lost by accident; unrelated to any variable.\nQuestion 2: MNAR – Missingness depends on the income value itself.\nQuestion 3: MCAR – Random machine fault.\nQuestion 4: MNAR – Missingness depends on stress level itself.\nQuestion 5: MAR – Missingness depends on another variable (age).\nQuestion 6: MNAR – Missing because of the actual value (very high weight).\nQuestion 7: MAR – Missingness linked to the form version (observed variable). Question 8 MCAR – The missing feedback is unrelated to any variable — it is random. Question 9 MAR – The missingness is related to another variable (gender) that is observed, not to the skincare routine itself.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Missing Values</span>"
    ]
  },
  {
    "objectID": "chapters/missing_values.html#handeling-missing-data",
    "href": "chapters/missing_values.html#handeling-missing-data",
    "title": "8  Missing Values",
    "section": "8.7 Handeling Missing Data",
    "text": "8.7 Handeling Missing Data\nOnce we identify that some data are missing, we need to decide how to deal with them. The goal is to reduce bias and make sure our analysis remains reliable. The best method depends on why the data are missing (MCAR, MAR, or MNAR), how much is missing, and what type of variable we are dealing with (numeric, categorical, or time-series)\n\n8.7.1 Collecting or Recovering Data\nIf possible, the best option is to go back and collect the missing data – by recontacting participants, checking logs, or re-running measurements.\nThis method is best for small and important datasets but it is time-consuming or impossible after data collection.\n\n\n8.7.2 Deletion (Removing Missing Data)\nThis means removing the rows or columns that contain missing values.\n\n8.7.2.1 Deleting Columns with Many Missing Values\nIf a column has too many missing values (for example, more than \\(70\\)-\\(80\\%\\) of the entries are NA), it may be better to remove it completely.\n\ndata &lt;- data[, colSums(is.na(data)) &lt; 0.8 * nrow(data)]\n\n# or equivalently\ndata &lt;- data[, colMeans(is.na(data)) &lt; 0.8]\n\n\n\n\n\n\n\nTip\n\n\n\nWe use this method when most values in a column are missing.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nUsing this method results in loss in information\n\n\n\n\n8.7.2.2 Deleting Rows with Missing Values\nYou can also remove entire rows that contain at least one missing value. This is simple but can lead to data loss if missingness is common.\n\n8.7.2.2.1 Listwise Deletion\nListwise deletion (also called case deletion) is the default behaviour in R. This methods removes all rows (cases) that contain at least one missing value, regardless of which variable it occues in. For example,\n\nlinear_model &lt;- lm(Ozone ~ Solar.R, data = airquality)\nsummary(linear_model)\n\n\nCall:\nlm(formula = Ozone ~ Solar.R, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-48.292 -21.361  -8.864  16.373 119.136 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.59873    6.74790   2.756 0.006856 ** \nSolar.R      0.12717    0.03278   3.880 0.000179 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.33 on 109 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.1213,    Adjusted R-squared:  0.1133 \nF-statistic: 15.05 on 1 and 109 DF,  p-value: 0.0001793\n\n\nIf you check the model output, R shows at the bottom something like:\n\n 42 observations deleted due to missingness\n\nThis message means R autoamtically exculded rows with any missing values before fitting the model.\nYou can also perform listwise deletion manually:\n\ndata_without_missing &lt;- na.omit(data)\n\nAnother way is to use only complete cases (rows without any NA).\n\ndata_without_missing &lt;- data[complete.cases(data),]\n\n\n\n\n\n\n\nTip\n\n\n\nUse this method when only a few rows have missing values.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nAvoid this method when many rows have missing values since you may lose too much data.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nListwise deletion gives valid (unbiased) results only when the missing data are MCAR and the amount of missing data is small.\n\n\n\n\n8.7.2.2.2 Pairwise Deletion\nInstead of deleting a whole row, each pair of variables is analyzed using all available data for that pair.\nHere, rows are only excluded when data are missing for the specific variables being compared.\nIn R, some correlation functions support this:\n\ncor(data, use = 'pairwise.complete.obs')\n\nAlthough using this method uses more data and does not discard rows entirely, different analyses may use different subsets of data which can make results inconsistent or hard to reproduce.\n\n\n\n\n8.7.3 Imputation (Filling in Missing Values)\nThis means replacing missing values with estimated values. There are many ways to do this – from simple to advanced.\n\n8.7.3.1 Simple Methods\n\n8.7.3.1.1 Constant Value\nreplace misisng values with a fixed number (e.g., \\(0\\) or “unknown”)\n\n\n8.7.3.1.2 Mean/Median/Mode Imputation:\nReplace missing values with the column mean or median (for numeric data), median or mode (for categorical).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to impute multiple numeric columns at once (for example, using the mean), you can use across() like this\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis automatically fills missing values with mean of each numeric column.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nUsing this method reduces variability and can distort relationships\n\n\n\n\n\n\n\n\nNote\n\n\n\nGroup-wise Mean/Median Imputations Sometimes, imputing within groups makes more sense.\nFor instance, if passengers of similar class share similar ages, we can use:\n\nlibrary(dplyr)\ndata &lt;- data %&gt;%\n  group_by(Class) %&gt;%\n  mutate(Age = if_else(is.na(Age), mean(Age, na.rm = TRUE), Age))%&gt;%\n  ungroup()\n\nUse this when a grouping variable logically explains variation in missing data.\n\n\n\n\n8.7.3.1.3 Imputing Categorcal Columns\nFor categorical variabels, replace missing values with the most frequent category or, if many values are missing, a new category such as “unknown”. For example, in a dataset for the variable Gender, we can have\n\n# replace NA with the most common category \nmode_value &lt;- names(sort(table(data$Gender), decreading = TRUE))[1]\ndata$Gender[is.na(data$Gender)] &lt;- mode_value \n\n# Or add a new category \ndata$Gender &lt;- ifelse(is.na(data$Gender), \"unknown\", data$Gender)\n\n\n\n\n8.7.3.2 Forward Fill and Backward Fill (Time Series)\nFor time-series data, you can fill missing values using the previous or next observed value.\n\n# install.packages(\"zoo\")\n\nlibrary(zoo)\n\n# Forward fill\ndata$Temperature &lt;- na.locf(data$Temperatue, fromLast = FALSE)\n\n# Backward fill \ndata$Temperature &lt;- na.locf(data$Temperatue, fromLast = TRUE)\n\nUse this method when data are sequential and values change smoothly over time.\n\n\n8.7.3.3 Interpolation\nInterpolation estimates missing values based on neighboring points – ideal for regularly spaced time-series.\n\n# install.packages(\"imputeTS\")\n\nlibrary(imputeTS)\n\ndata$Temperature &lt;- na_interpolation(data$Temperature, option = \"linear\")\n\nUse this method when values follow a continues trend (e.g., temperature, stock prices).\n\n\n\n8.7.4 Model-based Methods\nSome models can handle missing data automatically, without imputation.\n\nRegression Imputation: predict missing values using other variables.\n\nFor example,\n\nmodel &lt;- lm(Ozone ~ Wind + Temp + Solar.R, data = airquality)\npred &lt;- predict(model, newdata = airquality)\nairquality[is.na(airquality$Ozone)] &lt;- pred[is.na(airquality$Ozone)]\n\n\n\\(k\\)-Nearest Neighbors (kNN): replace missing values based on similar rows. i.e, kNN filles missing values based on the \\(k\\) most similar observations (neighbors).\n\n\n# install.packages(\"VIM\")\nlibrary(VIM) \n\ndata_kNN &lt;- kNN(airquality, k = 5)\n\nUse this method when variables have meaningful similarity structure. It is important to note that this method is computationally expensive for large datasets.\n\nMachine learning methods: Decision trees, random forests, and XGBoost can often deal with missing values internally.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Missing Values</span>"
    ]
  },
  {
    "objectID": "chapters/missing_values.html#exercise-1",
    "href": "chapters/missing_values.html#exercise-1",
    "title": "8  Missing Values",
    "section": "8.8 Exercise 1",
    "text": "8.8 Exercise 1\nDataset1 airquality dataset\n\ndata(\"airquality\")\n\nThe dataset records daily air quality measurements in New York (1973). It has missing values in Ozone and Solar.R columns.\nQuestion 1:\n\nExamine the distribution of Ozone and Solar.R columns. Are they normally distributed or skewed? (Hint: use summary(), histogram or boxplot)\nBased on the shape of the distribution, decide whether to use mean or median imputation for each column.\nReplace missing values in Ozone and Solar.R using your chosen method with dplyr.\n\nQuestion 2:\n\nNow, perform group-wise imputation by Month (i.e., replace missing values with the mean or median within each month), whichever you think is more appropriate.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Missing Values</span>"
    ]
  },
  {
    "objectID": "chapters/missing_values.html#exercise-2",
    "href": "chapters/missing_values.html#exercise-2",
    "title": "8  Missing Values",
    "section": "8.9 Exercise 2",
    "text": "8.9 Exercise 2\n** Dataset2** nhanes dataset from mice package\n\n# install.packages(\"mice\")\nlibrary(mice)\ndata(\"nhanes\")\n\nQuestion 1:\n\nExplore the nhanes dataset to identify columns with missing values. Use summary() and md.pattern() from the mice package.\nChoose a categorical column with missing value and impute missing values using mode.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Missing Values</span>"
    ]
  },
  {
    "objectID": "chapters/Decision_trees.html",
    "href": "chapters/Decision_trees.html",
    "title": "9  Decision Trees",
    "section": "",
    "text": "9.1 Types of Decision Trees\nA Decision Tree is one of the most intuitive and powerful tools in data analysis and machine learning.\nIt mimics the way humans make decisions – by asking a sequence of simple questions that split a problem into smaller, clearer parts.\nA decision tree functions like a flowchart that asks a sequence of if-then questions about the data:\nSo, you can see that a decision tree is organized as a hierarchy of nodes connected by branches, similar to an inverted tree structure.\nFor a more interactive explanation, see the MLU-Explain Decision Trees tutorial.\nDecision trees can be divided into two main types based on the nature of the target variable. Although the overall structure of the tree remains the same – with nodes, branches, and leaves – the way they make predictions differs depending on whether the target variable is categorical or numerical.\nThere are two main types of Decision Trees:",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/Decision_trees.html#types-of-decision-trees",
    "href": "chapters/Decision_trees.html#types-of-decision-trees",
    "title": "9  Decision Trees",
    "section": "",
    "text": "Classification Trees\nRegression Trees\n\n\n9.1.1 Classification Trees\nA classification tree is used when the target variable is categorical, meaning it represents distinct classes, categories or labels (e.g., “Yes/No”, “Low/Medium/High”, “Survived/Not Survived”). The goal of a classification tree is to assign each observation to one of the predefined classes based on the features of the data.\nThe tree splits the data into groupd that are as similar as possible in terms of the target class. Each question (or split) tries to separate different classes more clearly. At the end of the tree, each leaf node contains cases that mostly belong to the same category – ans that category is assigned as the prediction for any new observation that falls into that leaf.\nExample The iris dataset is one of the most famous examples in datascience and machine learning. It contains measurements of 150 iris flowers from three different species: Iris setosa, Iris versicolor, and Iris virginica. The dataset includes four variables: Sepal.Length (Length of the sepal), Sepal.Width (Width of the sepal), Petal.Length (Length of the petal), and Petal.Width (Width of the petal). The goal is to classify the species of an iris flower based on these measurements. So, the target variable is Species, which is categorical with three levels: setosa, versicolor, and virginica.\n\n\n\nThe three species of iris flowers (Source: https://www.datacamp.com/tutorial/machine-learning-in-r)\n\n\nThe goal is to build a Classification Tree that predicts the species of a flower based on its measured characteristics.\n\n\n\n\n\n\n\n\n\nThis is a classification tree built to predict the species of iris flowers based on their sepal and petal measurements.\nEach box (node) represents a decision or a group of flowers with similar characteristics.\nThe top box is the root node, which contains all 150 flowers. The first split is based on Petal.Length. If Petal.Length is less than or equal to \\(2.5\\) cm, the flower is classified as Iris setosa (left branch). If Petal.Length is greater than \\(2.5\\) cm, the tree asks another question about Petal.Width.”Is ‍Petal.width &lt; 1.8?” If yes, the flower is classified as Iris versicolor (middle branch). If no, it is classified as Iris virginica (right branch). These two species are harder to separate than Iris setosa, but the tree still achieves a very high accuracy (around \\(90-98\\%\\)).\nThe numbers in the boxes provide numbers like 1.00 .00 .00 that means \\(100\\%\\) of samples in that groupd belong to the Iris setosa species. The numbers .00 .91 .09 means \\(91\\%\\) of samples in that group belong to Iris versicolor and \\(9\\%\\) belong to Iris virginica. And, .02 .98 .00 means \\(98\\%\\) of samples in that group belong to Iris virginica and \\(2\\%\\) belong to Iris versicolor. Thus, the model’s decisions are based on probabilities – and each leaf node represents the most likely class for that group of samples.\nSo, each path from the top to a leaf node represents a classification rule.\n\nIf Petal.Length &lt; 2.5, thenSpecies` = Iris setosa.\nIf Petal.Length \\(\\ge\\) 2.5andPetal.Width&lt;= 1.8, then Species = Iris versicolor.\nIf Petal.Length \\(\\ge\\) 2.5andPetal.Width&gt; 1.8, then Species = Iris virginica.\n\n\n\n\n\n\n\nNote\n\n\n\nWhy Only Two Variables Appear in the Tree?\nThe tree does not try to use every variable – it picks the ones that best separate the data. In the iris dataset, the petal measurements are much more distinctive between species than the sepal ones, so the algorithm quickly finds that using just Petal.Length and Petal.Width gives almost perfect classification. If we removed the petal variables and built a new tree, we would see the sepal variables appear instead.\n\n\n\n\n9.1.2 Regression Trees\nA regression tree is used when the target variable is continuous or numerical (e.g., house prices, temperatures, or ages).\nEach split in a regression tree aims to minimize the variance (or mean squared error) within each group, creating subsets of data that are as similar as possible in terms of the target variable. At the end of the tree, each leaf node contains a predicted value, which is typically the average of the target variable for all observations that fall into that leaf.\nExample The mtcars dataset contains data on fuel consumption and performance for \\(32\\) automobiles (1973–74 models). The goal is to predict the miles per gallon (mpg) of a car based on its features: disp (Engine displacement), hp (Gross horsepower), wt (Weight), and cyl (number of cylinders).\n\n\n\n\n\n\n\n\n\nBased on this regression tree, we can see how the model predicts mpg based on number of cylinders (cyl) and horsepower (hp).\nEach box (node) represents a decision or a group of cars with similar characteristics, and the numbers in the boxes represent the average mpg for that group of cars.\nThe first split is based on cyl and tells us that the number of cylinders is the most important factor in predicting mpg. “Is cyl \\(\\ge 5\\)?” If no, go to right branch that leaf node shows that cars with small engines ( less than 5 cylinders) have an average mpg of \\(27\\) (\\(34\\%\\) of cars). If yes, go to left branch that asks another question about hp. “Is hp \\(\\ge\\) 193?” If no, go to right branch that leaf node shows that cars with large engines (5 or more cylinders and lower horsepower) have an average mpg of \\(18\\) (\\(44\\%\\) of cars). If yes, go to left branch that leaf node shows that cars with large engines (5 or more cylinders and higher horsepower) have an average mpg of \\(13\\) (\\(22\\%\\) of cars).\nSo, this tree tells us:\n\nFewer cylinders generally lead to better fuel efficiency (higher mpg).\nHigher horsepower tends to decrease mpg, especially in cars with more cylinders.\n\nThis model translates to a few simple, interpretable rules that summarize how car design affects performance.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/Decision_trees.html#classification-and-regression-tree-cart-algorithm",
    "href": "chapters/Decision_trees.html#classification-and-regression-tree-cart-algorithm",
    "title": "9  Decision Trees",
    "section": "9.2 Classification and Regression Tree (CART) Algorithm",
    "text": "9.2 Classification and Regression Tree (CART) Algorithm\nThe term classification and regression tree (CART) refers to a specific algorithm developed by Breiman et al. in 1984 for constructing decision trees. It provides a unified framework for building both Classification Trees and Regression Trees.\nThe CART works by recursively splitting the dataset into smaller and smaller groups (nodes) based on the variable and threshold that best separate the data according to the target variable.\nIt follows these main steps:\n\nStart with the full dataset (root node) All observations are included in one group.\nFind the best split For every variable, the CART tests possible cut points and it chooses the one that makes the resulting groups as pure (homogeneous) as possible.\nSplit the data into two subgroups The CART always creates binary splits (two branches) at each node.\nRepeat the process (recursion) Each subgroup becomes a new node, and the CART continues splitting until a stopping criterion is met (e.g., minimum node size, maximum tree depth, or no further improvement in purity).\nPrun the tree (optional) To avoid overfitting, the CART may prune the tree by removing branches that do not provide significant predictive power.\n\n\n9.2.1 Metrics used in CART\nTo decide best split, CART uses different metrics depending on whether it is building a Classification Tree or a Regression Tree.\n\n9.2.1.1 For Classification Trees\nThe CART algorithm typically uses impurity measures to evaluate how well a split separates the classes.\nThe most common impurity measures are:\n\nGini impurity\nEntropy (information gain)\nClassification error\n\nGini Index\nThe Gini index is defined as \\[\n\\text{Gini} = 1 - \\sum\\limits_{i=1}^{k} p_i^2\n\\] where \\(p_i\\) is the proportion of class \\(i\\) in the node.\nIf \\(\\text{Gini} = 0\\), the node is pure (all observations belong to one class). If \\(\\text{Gini} = 0.5\\) (binary case), the node is completely mixed (\\(50\\%\\) each class).\nFor example, if a node has \\(90\\%\\) setosa and \\(10\\%\\) versicolor, \\(\\text{Gini} = 1 - (0.9^2 + 0.1^2) = 0.18\\). The node is fairly pure.\nEntropy (Information Gain)\nThe Entropy is defined as \\[\n\\text{Entropy} = - \\sum\\limits_{i = 1}^k p_i \\log_2(p_i)\n\\] where \\(p_i\\) is the proportion of class \\(i\\) in the node.\nIf \\(\\text{Entropy} = 0\\), it means the node is perfectly pure while \\(\\text{Entropy} = 1\\) means that the node has maximum disorder (completely mixed).\nCART usually uses the Gini Index because it is computationally faster and produces results very similar to those obtained with Entropy.\nClassification Error\nThe classification error is a simpler impurity measure defined as \\[\n\\text{Error} = 1 - \\max(p_i)\n\\] where \\(\\max(p_i)\\) is the proportion of the most common class in the node.\nIf all samples belong to one class, \\(\\max(p_i) = 1\\) and Error = 0 (perfectly pure). If the classes are equally mixed, \\(\\max(p_i) = 0.5\\) and Error = 0.5.\nThis measure is easy to interpret but less sensitive than the Gini Index or Entropy, so it is mainly used for reporting accuracy, not for choosing splits.\n\n\n9.2.1.2 For Regression Trees\nWhen the target variable is numeric, CART measures how similar the values are within each node.\nInstead of impurity, it uses the variance or the Mean Squared Error (MSE).\nVariance\nThe variance of a node is defined \\[\n\\text{Var}(Y) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (y_i - \\bar{y})^2\n\\] where \\(y_i\\)s are observed values, \\(\\bar{y}\\) is the mean of the node, and \\(n\\) is the number of observation in each node.\nA lower variance means that the values in that node are closer to each other, so the node is more ‘pure’ in terms of numeric outcomes.\nMean Squared Error (MSE)\nCART typically uses MSE as the splitting criterion, which is conceptually similar to variance.\nThe MSE is defined as \\[\\text{MSE} = \\frac{1}{n} \\sum\\limits_{i=1}^n (y_i - \\hat{y})^2\n\\] where \\(\\hat{y}\\) is the predicted value for that node (usually the mean).\nThe best split is the one that reduces the total MSE the most — in other words, the children nodes have smaller errors than the parent node.\n\n\n\n\n\n\nNote\n\n\n\nIn a Regression Tree, the predicted value for each node is simply the mean of that node – so, \\(\\hat{y} = \\bar{y}\\). When that is the case: \\[\n\\text{MSE} = \\text{Var}(Y)\n\\]\n\n\nExample: Suppose a node contains five cars with their fuel efficiency (mpg) values:\n\n\n\nCar\nmpg\n\n\n\n\nA\n10\n\n\nB\n12\n\n\nC\n15\n\n\nD\n20\n\n\nE\n22\n\n\n\nFirst, Compute the variance before splitting as: \\[\n\\bar{y} = \\frac{10 + 12 + 15 + 20 + 22}{5} = 15.8\n\\] and then \\[\n\\text{Var}_{\\text{parent}} = \\frac{(10-15.8)^2 + (12-15.8)^2 + (15-15.8)^2 + (20-15.8)^2 + (22-15.8)^2}{5} = 21.36\n\\] Then, we try a possible split\n\nLeft node: cars with small engines → mpg = [10, 12, 15]\n\nRight node: cars with large engines → mpg = [20, 22]\n\nCompute the variances for each node:\nLeft node: \\[\n\\bar{y}_L = 12.33,\\quad \\text{Var}_L = \\frac{(10-12.33)^2 + (12-12.33)^2 + (15-12.33)^2}{3} = 4.22\n\\] Right node: \\[\n\\bar{y}_R = 21.0,\\quad \\text{Var}_R = \\frac{(20-21)^2 + (22-21)^2}{2} = 1.0\n\\] Then, Compute the weighted average of child variances \\[\n\\text{Var}_{\\text{after split}} = \\frac{3}{5}(4.22) + \\frac{2}{5}(1.0) = 2.93.\n\\] So, we calculate the variance reduction as [ = 21.36 - 2.93 = 18.43. ] This means the split greatly reduces the variability of mpg within each node, so CART would consider it a very good split.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/Decision_trees.html#implementations-in-r",
    "href": "chapters/Decision_trees.html#implementations-in-r",
    "title": "9  Decision Trees",
    "section": "9.3 Implementations in R",
    "text": "9.3 Implementations in R\nThere are some packages in R that implements the CART algorithm. The most famous are {rpart} and {rpart.plot}.\nThe rpart Function:\nThe rpart() function is the core of CART in R. It fits Classification, Regression, (and two other) trees depending on the target variable and the method argument.\nThe basic syntax is\n\nrpart(formula, \n      data, \n      weights, \n      subset, \n      na.action = na.rpart, \n      method,\n      model = FALSE, \n      x = FALSE, \n      y = TRUE, \n      parms, \n      control, \n      cost, \n      ...)\n\nThe main arguments are:\n\nformula: Defines the relationship between the target variable and predictors. The left side of ~ is the response, and the right side are the predictors. You can use . to include all predictors automatically.\ndata: The data frame containing the variables used in the formula.\nweights: Optional vector giving observation weights.\nsubset: Logical condition to select part of the data.\nna.action: Specifies how to handle missing values. The default removes rows with missing response, but can handle missing predictors using surrogate splits (unique to CART).\nmethod: Specifies the type of model: “class”, “anova”, “poisson”, or “exp”. method = \"class\" means Classification tree (categorical response). method =  \"anova\" means Regression tree (numeric response). method = \"poisson\" means Count data; and method = \"exp\" means Survival data.\nmodel, x, y: They are logical and store model frame, predictors, or response in the output. Usually, we left them as default.\nparms: Additional parameters for the splitting rule. For classification, you can set parms = list(split = \"information\") to use entropy instead of Gini.\ncontrol: A list of options controlling tree complexity, pruning, and stopping criteria. Controls include: minsplit: minimum observations to attempt a split; minbucket: minimum observations in a terminal node; cp: complexity parameter (pruning strength); and now, maxdepth: maximum depth of tree.\ncost: Vector of variable costs. You can penalize certain variables if they are expensive to measure.\n...: Additional arguments\n\nExample of Customization\n\nfit &lt;- rpart(Species ~ ., \n             data = iris, \n             method = \"class\", \n             parms = list(split = \"information\"),  # use entropy instead of Gini\n             control = rpart.control(cp = 0.02, minsplit = 10))\n\nThe rpart.plot() Function:\nThe rpart.plot() function from the {rpart.plot} package creates clean, colored diagrams of decision trees.\nThe basic syntax is\n\nrpart.plot(x,\n           type = 2,\n           extra = \"auto\",\n           fallen.leaves = TRUE,\n           digits = 2,\n           varlen = 0,\n           box.palette = \"auto\",\n           shadow.col = 0,\n           ...)\n\nThe main arguments are: - x: The fitted model object from rpart() - type: Controls how node labels and split labels are displayed.\n\n\n\n\n\n\n\nValue\nDescription\n\n\n\n\n0\nDraws a split label at each split and a node label only at leaves.\n\n\n1\nLabels all nodes, not just leaves (similar to text.rpart(all = TRUE)).\n\n\n2\n(Default) Like 1, but split labels appear below node labels — similar to the style used in the original CART book.\n\n\n3\nDraws separate split labels for the left and right branches.\n\n\n4\nLike 3, but labels all nodes, similar to text.rpart(fancy = TRUE).\n\n\n5\nDisplays split variable names in the interior nodes.\n\n\n\n\nextra: Specifies what additional information should be shown in each node.\n\n\n\n\n\n\n\n\nValue\nMeaning\n\n\n\n\n\"auto\"\n(Default) Automatically selects based on model type.  – extra = 106 for binary classification  – extra = 104 for multiclass  – extra = 100 for regression\n\n\n0\nShow no extra information.\n\n\n1\nNumber of observations in the node (or per class).\n\n\n2\nClassification rate (correct vs total) or number of events.\n\n\n3\nMisclassification rate (incorrect vs total).\n\n\n4\nClass probabilities for each category (sum = 1).\n\n\n5\nLike 4, but hides the predicted class.\n\n\n6\nProbability of the second class (useful for binary).\n\n\n7\nLike 6, but hides the fitted class.\n\n\n8\nProbability of the predicted class only.\n\n\n9\nProbability relative to all observations (sum of all leaves = 1).\n\n\n10\nProbability of second class relative to all data.\n\n\n11\nLike 10, but hides the fitted class.\n\n\n+100\nAdds percentage of total observations in each node.\n\n\n\nFor example extra = 104 is ideal for multiclass problems; extra = 100 is typically used for regressiont rees.\n\nunder: Applies only if extra &gt; 0. The default value is FALSE that means extra information appears inside the box. Also, TRUE places the extra text below the box (useful for crowded trees).\nfallen.leaves: Places all leaves at the same height. The default is TRUE that aligns all leaf nodes at the same level (bottom of the plot). If the tree is large and text overlaps, we set it FALSE.\ndigits: Controls the number of significant digits in numeric labels. The default value is \\(2\\).\nvarlen: Controls the length of variable names in split labels. The default value is \\(0\\). &gt; 0 abbreviate names to the specified number of characters. &lt; 0 truncate names automatically to shortest unique length.\nfaclen: Similar to varlen, but applies to factor level names. The default value is \\(0\\) that shows full level names. \\(1\\) shows letters (a, b, c) instead of full names — useful for compact display.\nroundint: If TRUE (default), integer splits are rounded for readability. Set to FALSE if predictors have possible decimal values.\ncex: Sets the text size directly (NULL = automatic).\ntweak: Adjusts the automatic size by a multiplier (e.g., tweak = 1.2 makes text \\(20\\%\\) larger).\nclip.facs: The default value is FALSE. If TRUE, shortens factor splits by removing the variable name and equals sign. e.g., shows female instead of sex = female.\nbox.palette Colors nodes by fitted value or class.\n\n\n9.3.1 Classification Tree in R\nNow, we are giving the R code for classification tree that aimed to predict the species of iris flowers using their measurments.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n9.3.2 Regression Tree in R\nWe will predict miles per gallon (mpg) from car features in the mtcars dataset.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/Decision_trees.html#handling-missing-values-in-cart",
    "href": "chapters/Decision_trees.html#handling-missing-values-in-cart",
    "title": "9  Decision Trees",
    "section": "9.4 Handling Missing Values in CART",
    "text": "9.4 Handling Missing Values in CART\n\ndata('airquality')\ncolSums(is.na(airquality))\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n     37       7       0       0       0       0 \n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Explanatory Data Analysis",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "chapters/clustring.html",
    "href": "chapters/clustring.html",
    "title": "10  Clustering",
    "section": "",
    "text": "10.1 Applications of Clustering\nCluster analysis, or Clustering, is a technique used to find groups of objects such that the objects within the same group are similar (or closely realted) to one another, while the objects from different groups are dissimilar (or unrelated).\nClustering techniques are widely used across multiple disciplines to discover patterns, structure data, and support decision-making. Below are some common and illustrative applications.\nCustomer Segmentation\nIn marketing and business analytics, clustering helps identify groups of customers with similar behaviors, preferences, or purchasing habits. This segmentation allows companies to optimize advertising strategies, personalize product offerings, and design campaigns for specific focus groups.\nExample: Grouping supermarket customers based on purchase frequency, product categories, and spending level.\nWeb Visitor Segmentation\nWeb analytics platforms use clustering to classify website visitors according to their browsing patterns, time spent on pages, or interaction behaviors. This segmentation supports personalized content delivery and improved user experience.\nExample: Optimizing web navigation or recommendations for distinct user segments such as new visitors vs. returning users.\nData Aggregation and Reduction\nClustering can be used to represent large datasets by a smaller set of representative elements (centroids or medoids).\nThis reduces computational complexity and facilitates data visualization.\nExample:\nReducing the color palette of an image to k representative colors using algorithms like k-means.\nText Collection Organization\nIn natural language processing (NLP), clustering is applied to group similar documents or texts into topics or themes.\nIt is particularly useful when the number or nature of topics is unknown in advance.\nExample:\nGrouping news articles, research abstracts, or emails into topic clusters.\nBiology and Taxonomy\nIn biological sciences, clustering supports the classification of living organisms based on genetic, morphological, or behavioral similarities.\nIt forms the basis for hierarchical structures such as kingdom, phylum, class, order, family, genus, and species.\nExample:\nClustering DNA sequences to identify genetic relationships among species.\nInformation Retrieval\nIn computer science and library systems, clustering helps organize large document collections to improve search and retrieval efficiency. By grouping related documents, search engines can return more contextually relevant results.\nExample:\nDocument clustering for semantic search or grouping research papers by field of study.",
    "crumbs": [
      "Clustering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/clustring.html#distance-and-similarity-in-clustering",
    "href": "chapters/clustring.html#distance-and-similarity-in-clustering",
    "title": "10  Clustering",
    "section": "10.2 Distance and Similarity in Clustering",
    "text": "10.2 Distance and Similarity in Clustering\nThe notion of distance or similarity lies at the heart of clustering. Since clustering aims to group similar observations together, we must have a way to measure how close or far two observations are from each other in the feature space. These measures quantify the degree of resemblance (similarity) or difference (dissimilarity) between data points.\n\nThe main goal is to partition a set of data points into clusters where intra-cluster distances (distances between points within the same cluster) are minimized, and inter-cluster distances (distances between points from different clusters) are maximized.\n\nThus, the main goal of clustering can be rewritten as:\n\nThe main goal is to partition a set of data points into clusters where intra-cluster similarities (similarities between points within the same cluster) are maximized, and inter-cluster similarities (similarities between points from different clusters) are minimized.\n\nTo avoid confusion, remember that distance and similarity are inverse concepts: when two objects are close (small distance), they are considered similar (high similarity).\nThe table below summarizes their relationship and how each measure behaves in clustering.\n\n\n\n\n\n\n\n\n\n\nConcept\nMeaning\nHigh Value Indicates\nLow Value Indicates\nGoal in Clustering\n\n\n\n\nDistance\nA measure of dissimilarity between two objects\nObjects are far apart (dissimilar)\nObjects are close together (similar)\nMinimize intra-cluster distances and maximize inter-cluster distances\n\n\nSimilarity\nA measure of closeness or resemblance between two objects\nObjects are close together (similar)\nObjects are far apart (dissimilar)\nMaximize intra-cluster similarities and minimize inter-cluster similarities\n\n\n\n\n\n\n\n\nRelationship between distance and similarity — points in the same cluster have high similarity (low distance).\n\n\n\n\nA distance metric \\(d(\\mathbf{x}, \\mathbf{y})\\) measures the dissimilarity between two points \\(\\mathbf{x}=(x_1, x_2, \\ldots, x_p)\\) and \\(\\mathbf{y} = (y_1, y_2, \\ldots, y_p)\\). Formally, a function \\(d: X \\times X \\rightarrow \\mathbb{R}\\) is a metric if it satisfies: \\[\n\\begin{aligned}\n1.\\;& d(\\mathbf{x}, \\mathbf{y}) \\ge 0 && \\text{(non-negativity)} \\\\\n2.\\;& d(\\mathbf{x}, \\mathbf{y}) = 0 \\iff \\mathbf{x} = \\mathbf{y} && \\text{(identity)} \\\\\n3.\\;& d(\\mathbf{x}, \\mathbf{y}) = d(\\mathbf{y}, \\mathbf{x}) && \\text{(symmetry)} \\\\\n4.\\;& d(\\mathbf{x}, \\mathbf{y}) \\le d(\\mathbf{x}, \\mathbf{z}) + d(\\mathbf{z}, \\mathbf{y}) && \\text{(triangle inequality)}\n\\end{aligned}\n\\]\nA similarity measure \\(s(\\mathbf{x}, \\mathbf{y})\\) expresses how close or related two points are.\nIt usually satisfies: \\[\n\\begin{aligned}\n1.\\;& s(\\mathbf{x}, \\mathbf{y}) \\ge 0 && \\text{(non-negativity)} \\\\\n2.\\;& s(\\mathbf{x}, \\mathbf{y}) = s(\\mathbf{y}, \\mathbf{x}) && \\text{(symmetry)} \\\\\n3.\\;& s(\\mathbf{x}, \\mathbf{y}) \\in [0, 1] && \\text{(boundedness)} \\\\\n4.\\;& s(\\mathbf{x}, \\mathbf{x}) = 1 && \\text{(maximum self-similarity)}\n\\end{aligned}\n\\] A simple transformation links both concepts: \\[\ns(\\mathbf{x}, \\mathbf{y}) = \\frac{1}{1 + d(\\mathbf{x}, \\mathbf{y})}, \\quad \\text{where } s(\\mathbf{x}, \\mathbf{y}) \\in [0, 1].\n\\]\n\n10.2.1 Common Distance Metrics\nDifferent clustering algorithms may behave very differently depending on the metric used.\nBelow are the most widely used distance measures.\n\n10.2.1.1 Euclidean Distance\nThe most common metric, representing the straight-line distance between two points in \\(\\mathbb{R}^p\\): \\[\nd_E(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{i=1}^{p} (x_i - y_i)^2}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nEuclidean distance is sensitive to scale; therefore, variables should usually be standardized before applying it.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n10.2.1.2 Manhattan (or City-Block) Distance\nThis distance sums the absolute differences across all coordinates: \\[\nd_M(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{p} |x_i - y_i|\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe Manhattan distance is more robust to outliers and suitable when features represent grid-like or discrete steps.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nComparison of Euclidean and Manhattan distance between two points\n\n\n\n\n\n\n10.2.1.3 Minkowski Distance\nA generalization of Euclidean and Manhattan distances, controlled by a parameter \\(r\\): \\[\nd_r(\\mathbf{x}, \\mathbf{y}) = \\left( \\sum_{i=1}^{p} |x_i - y_i|^r \\right)^{1/r}\n\\]\n\nWhen \\(r = 1\\), it becomes Manhattan distance.\nWhen \\(r = 2\\), it becomes Euclidean distance.\nLarger values of \\(r\\) emphasize large coordinate differences.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nShapes of Minkowski (Lp) distances around a point for different p values\n\n\n\n\n\n\n10.2.1.4 Chebyshev Distance\nAlso known as the \\(L_\\infty\\) norm, this distance takes the largest coordinate difference: \\[\nd_C(\\mathbf{x}, \\mathbf{y}) = \\max_i |x_i - y_i|\n\\] It measures how far apart two points are along the dimension where they differ most.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nComparison of Chebyshev, Euclidean, and Manhattan distances between A and B\n\n\n\n\n\n\n10.2.1.5 Mahalanobis Distance\nA scale-invariant distance that accounts for correlations between variables: \\[\nd_{Mah}(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(x - y)^{\\top} \\boldsymbol{\\Sigma}^{-1} (x - y)}\n\\] where \\(\\boldsymbol{\\Sigma}\\) is the covariance matrix of the data. Two points with the same Mahalanobis distance from the mean are equally likely under a multivariate normal model, regardless of the variable scales or correlations.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nTip\n\n\n\nChoosing an Appropriate Metric\nThe choice of metric depends on the data characteristics and the clustering goal:\n\nUse Euclidean when features are continuous and scaled.\n\nUse Manhattan for grid-like or sparse data.\n\nUse Mahalanobis when features are correlated.\n\nUse Chebyshev for problems sensitive to maximum deviations.\n\nUse Minkowski for flexible control between Manhattan and Euclidean.\n\n\n\n\n\n10.2.1.6 Cosine and Correlation Distance\nFor data where direction or orientation matters more than magnitude — such as text represented by TF-IDF vectors or normalized embeddings — Euclidean distance is not ideal. Instead, we use cosine similarity or correlation distance.\nThe cosine similarity between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is defined as \\[\ns_{\\text{cosine}}(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\, \\|\\mathbf{y}\\|}\n\\] where \\(\\mathbf{x} \\cdot \\mathbf{y}\\) is the dot product of the two vectors, and \\(\\|\\mathbf{x}\\|\\) and \\(\\|\\mathbf{y}\\|\\) are their**Euclidean norms*.\nThis similarity measures the cosine of the angle between the two vectors in a multidimensional space.\n\nA value of 1 means the vectors point in the same direction (perfectly similar).\nA value of 0 means the vectors are orthogonal (no similarity).\nA value of –1 means they point in opposite directions (perfectly dissimilar).\n\nThe corresponding cosine distance is: \\[\nd_{\\text{cosine}}(\\mathbf{x}, \\mathbf{y}) = 1 - s_{\\text{cosine}}(\\mathbf{x}, \\mathbf{y})\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCosine-based metrics are particularly useful for directional data, where the pattern or orientation of features matters more than their magnitude —\nfor example, in text mining (TF-IDF vectors), recommendation systems, or image feature embeddings.\n\n\n\n\n10.2.2 Distances for Binary Data\nWhen the data are binary (0 or 1), such as presence/absence, success/failure, or yes/no attributes, specialized similarity and distance measures are used.\nLet \\(o_1, o_2 \\in \\{0,1\\}^d\\) be two binary observations described by \\(d\\) attributes.\nWe define: \\[\n\\begin{aligned}\nf_{11} &= \\text{number of attributes where } o_1 = 1 \\text{ and } o_2 = 1, \\\\\nf_{00} &= \\text{number of attributes where } o_1 = 0 \\text{ and } o_2 = 0, \\\\\nf_{10} &= \\text{number of attributes where } o_1 = 1 \\text{ and } o_2 = 0, \\\\\nf_{01} &= \\text{number of attributes where } o_1 = 0 \\text{ and } o_2 = 1.\n\\end{aligned}\n\\]\n\n10.2.2.1 Simple Matching Coefficient (Similarity)\n\\[\ns_{SMC}(o_1, o_2) = \\frac{f_{11} + f_{00}}{f_{11} + f_{00} + f_{10} + f_{01}} = \\frac{f_{11} + f_{00}}{d}\n\\]\nThis coefficient measures the proportion of attributes where the two observations match — whether both are 1s or both are 0s.\n\n\n10.2.2.2 Simple Matching Distance\n\\[\nd_{SMC}(o_1, o_2) = 1 - s_{SMC}(o_1, o_2) = \\frac{f_{01} + f_{10}}{d}\n\\] This represents the proportion of mismatches between two binary objects.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\n\nIf two binary vectors are identical,\\(s_{SMC} = 1\\) and \\(d_{sMC} = 0\\).\nIf they are completely opposite, \\(s_{SMC} = 0\\) and \\(d_{sMC} = 1\\).\nThe SMC treats \\(0\\)s and \\(1\\)s symmetrically, so it is best used when both states are equally meaningful.\n\n\n\n\n\n\n10.2.3 Distances for Categorical Data\nWhen data contain categorical or mixed-type variables, standard numeric distances (like Euclidean) are not suitable.\nInstead, we use measures that handle qualitative comparisons directly.\nLet \\(\\mathbf{x} = (x_1, \\ldots, x_p)\\) and \\(\\mathbf{y} = (y_1, \\ldots, y_p)\\) be two observations described by \\(p\\) categorical or mixed-type attributes.\n\n10.2.3.1 Hamming Distance\nThe Hamming distance counts the number of mismatches between two vectors: \\[\n\\text{d}_{Hamming}(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^{p} \\delta(x_i, y_i)\n\\] where \\[\n\\delta(x_i, y_i) =\n\\begin{cases}\n0, & \\text{if } x_i = y_i \\\\\n1, & \\text{if } x_i \\neq y_i\n\\end{cases}\n\\] It measures how many positions differ between two categorical strings or binary vectors.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n10.2.3.2 Gower Distance\nThe Gower distance (Gower 1971) allows comparing mixed-type data (numerical, categorical, binomial).\nFor two observations \\(i\\) and \\(k\\), and each variable \\(k\\), Gower’s method computes a partial similarity score \\(s_{ijk}\\) as follows:\n\nNumeric variables (continuous or discrete): \\[\ns_{ijk} = 1 - \\frac{|x_{ik} - x_{jk}|}{R_k}\n\\] where \\(R_k\\) is the range of variable \\(k\\).\nCategorical (or binary) variables: \\[\ns_{ijk} = \\begin{cases}\n1 & \\quad \\text{if} \\quad x_{ik} = x_{jk}\\\\\n0 & \\quad \\text{if} \\quad x_{ik} \\neq x_{jk}\n\\end{cases}\n\\] Then, the overall similarity between \\(i\\) and \\(j\\) is \\[\nS_{ij} = \\frac{\\sum_k s_{ijk} \\delta_{ijk}}{\\sum_k \\delta_{ijk}}\n\\] where \\(\\delta_{ijk} = 1\\) if variable \\(k\\) is valid for both \\(i\\) and \\(j\\) (i.e, non-missing), else \\(0\\).\n\nFinally, to convert similarity to a distance, one often uses \\[\nD_{ij} = 1 - S_{ij}\n\\]\nExample\nAs an example, consider the following table which shows information about a number of individuals (identified by “Subject ID”) with four attributes:\n\nAge – a continuous numeric variable\nHandedness – a binary variable, whether the individual is left- or right-handed.\nEye colour - a categorical variable\nKnows Python - a dichotomous variable (while two people who know python may have a similar education, both of them not knowing Python does not imply they have similar backgrounds).\n\n\n\n\nSubject ID\nAge\nHandedness\nEye Colour\nKnows Python\n\n\n\n\n001\n28\nRight\nBlue\nYes\n\n\n002\n34\nLeft\nBlue\nNo\n\n\n003\n22\nRight\nGreen\nYes\n\n\n004\n45\nRight\nHazel\nNo\n\n\n005\n30\nLeft\nBrown\nYes\n\n\n\nLet us look at individuals 001 and 002, and calculate the score for each variable in turn:\n\nAge: \\[\ns_{\\textrm{age}} = 1 - \\frac{|28 - 34|}{23} = 0.74\n\\] where the range \\(R_k = 23\\) is the range of ages in the sample, which has a minimum of \\(22\\) and a maximum of \\(45\\).\nHandedness:\n\nSince the individuals have different handedness, \\(s_{\\text{handedness}} = 0\\).\n\nEye colour: They have the same eyes, \\(s_{\\text{eyes}} = 1\\).\nKnows Python:\n\nIndividuals 001 knows python whereas 002 does not; so, \\(s_{\\text{python}} = 0\\).\nWe have no missing data, so all the \\(\\delta_{ijk} = 1\\). The overall similarity score between 001 and 002 is therefore \\[\nS = \\frac{0.74 \\times 1 + 0 \\times 1 + 1 \\times 1 + 0 \\times 1}{1 + 1 + 1 + 1} = \\frac{1.74}{4} = 0.435\n\\] So, the Gower distance between these two individuals is \\[\nD = 1 - 0.435 = 0.565\n\\]\nRepeating the calculation for all pairs of indivials, we obtain the distance matrix, which can be used for clustering them into groups.\nFor calculating Gower distance in R, we can use daisy() from the library cluster. However, it is important to note that always convert categorical characters to factors before using this function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAs you see that the individuals who are most similar are 001 and 003 \\((D = 0.315)\\) and the most different are 004 and 005 \\((D = 0.913)\\).\n\n\n\n10.2.4 Visualizing distance matrices\nDatasets usually contain many observations (more than just two points).\nWhen we compute pairwise distances between all observations, we obtain a distance matrix — a square table where each cell represents the dissimilarity between two observations.\nMathematically, if a dataset has \\(n\\) observations, the distance matrix has \\(n \\times n\\) entries: \\[\nD_{ij} = d(\\mathbf{x}_i, \\mathbf{x}_j)\n\\] where \\(D_{ij}\\) is the distance between observation \\(i\\) and \\(j\\).\nBecause this matrix contains all pairwise comparisons, it provides a complete picture of how observations relate to one another.\nHowever, as the number of observations grows, the matrix quickly becomes difficult to interpret numerically — it is just a sea of numbers.\nTo extract insights, we often visualize the distance matrix as a heatmap or similarity map.\nVisualizing distance matrices helps us:\n\nDetect structure – clusters appear as blocks or dark/light patches.\nSpot outliers – points that are dissimilar from everyone else.\nCompare metrics – see how distance measures produce different patterns.\nBuild intuitio – for what closeness or difference means in multidimensional space.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe color level is proportional to the value of the dissimilarity between observations: pure red if \\(\\text{dist}(\\mathbf{x}_i, \\mathbf{x}_j) = 0\\) nad pure blue if \\(\\text{dist}(\\mathbf{x}_i, \\mathbf{x}_j) = 1\\), here. Objects belonging to the same cluster are displayed in consecutive order.\nIn this example, subjects 001 and 002 are quite similar (close in age, same eye color). Subject 004 (older, hazel eyes, does not know Python) stands out as most dissimilar from others.\n\nExercise 1Exercise 2Exercise 3Exercise 3\n\n\n1. Create two 3-dimensional points and calculate the distance\nLet \\(A = (1, 3, 5)\\) and \\(B = (4, 9, 6)\\).\nCompute manually and in R the following distances between \\(A\\) and \\(B\\):\n\nEuclidean\nManhattan\nMinkowski with \\(r = 3\\)\nChebyshev\n\n2. Explore sensitivity to scale\nMultiply the second coordinate of both points by 10 and recompute all distances. How does this change the results?\n\n\nTwo species are described by four binary characteristics:\n\n\n\nFeature\nSpecies A\nSpecies B\n\n\n\n\nHas fins\n1\n1\n\n\nLays eggs\n1\n0\n\n\nHas scales\n1\n1\n\n\nWarm-blooded\n0\n1\n\n\n\n\nCompute manually:\n\n( f_{11}, f_{00}, f_{10}, f_{01} )\nThe Simple Matching Coefficient (SMC)\n\nThe Simple Matching Distance (SMD)\n\nVerify your results using R.\nWhich pair of features contributes to dissimilarity?\n\n\n\nTwo students rate three projects as “Good”, “Average”, or “Poor”:\n\n\n\nProject\nStudent 1\nStudent 2\n\n\n\n\nA\nGood\nGood\n\n\nB\nPoor\nAverage\n\n\nC\nGood\nPoor\n\n\n\nCompute the Hamming distance (number of mismatches).\nHint: Use\n\ndata[] &lt;- lapply(data, function(col) if (is.character(col)) as.factor(col) else col)\n\n\n\nWe want to analyse the flower dataset which is available at library cluseter.\nBased on the following information, a) find the best distance matrix. and b) visulize the distance matrix.\n\nlibrary(cluster)\ndata(flower)\n\nhead(flower, 3)\n\n  V1 V2 V3 V4 V5 V6  V7 V8\n1  0  1  1  4  3 15  25 15\n2  1  0  0  2  1  3 150 50\n3  0  1  0  3  3  1 150 50\n\n#Data structure\nstr(flower)\n\n'data.frame':   18 obs. of  8 variables:\n $ V1: Factor w/ 2 levels \"0\",\"1\": 1 2 1 1 1 1 1 1 2 2 ...\n $ V2: Factor w/ 2 levels \"0\",\"1\": 2 1 2 1 2 2 1 1 2 2 ...\n $ V3: Factor w/ 2 levels \"0\",\"1\": 2 1 1 2 1 1 1 2 1 1 ...\n $ V4: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 4 2 3 4 5 4 4 2 3 5 ...\n $ V5: Ord.factor w/ 3 levels \"1\"&lt;\"2\"&lt;\"3\": 3 1 3 2 2 3 3 2 1 2 ...\n $ V6: Ord.factor w/ 18 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 15 3 1 16 2 12 13 7 4 14 ...\n $ V7: num  25 150 150 125 20 50 40 100 25 100 ...\n $ V8: num  15 50 50 50 15 40 20 15 15 60 ...",
    "crumbs": [
      "Clustering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/clustring.html#partition-based-clustering-k-means-and-k-medoids",
    "href": "chapters/clustring.html#partition-based-clustering-k-means-and-k-medoids",
    "title": "10  Clustering",
    "section": "10.3 Partition-Based Clustering: \\(k\\)-Means and \\(k\\)-Medoids",
    "text": "10.3 Partition-Based Clustering: \\(k\\)-Means and \\(k\\)-Medoids\nPartitioning clustering are clustering methods used to classify observations, within a dataset, into multiple groups based ion their similrity. The algorithms require the analyst to specify the number of clusters to be generated.\nThe commonly used partitioning clustering includes:\n\n\\(K\\)-means clustering (MacQueen 1967) in which ,each cluster is represented by the center or means of the data points belonging to the cluster. The \\(K\\)-means method is sensitive to anomalous data points and outliers.\n\\(K\\)-medoids clustering or PAM (Partitioning Around Medoids) (Kaufman and Rousseeuw 1990) in which , each cluster is reprsented by one of the objects in the cluster. PAM is less sensitive to outliers compared to \\(K\\)-means.\nCLARA (Clustering Large Applications) algorithm, which is an extension to PAM adapted for large data sets.\n\n\n10.3.1 \\(K\\)-Means Clustering\nK-Means aims to partition \\(n\\) observations into \\(K\\) clusters such that each observation belongs to the cluster with the nearest centroid (mean of points).\nEach cluster is represented by its centroid, which may not be an actual data point.\n\n10.3.1.1 \\(K\\)-means basic ideas\nThis basic idea behind \\(K\\)-means clustering consists of defining clusters so that the total intra-cluster variation (known as total within-cluster variation) is minimized.\nThere are several \\(K\\)-means algorithms available. The standard algorithm is the Hartigan-Wong algorithm (Hartigan and Wong 1979) which defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid: \\[\nW(C_k) = \\sum_{x_i \\in C_k} (x_i - \\mu_k)^2\n\\] where \\(x_i\\) design a data point belonging to the cluster \\(C_k\\) and \\(\\mu_k\\) is the mean value of the points assigned to the cluster \\(C_k\\).\nEach observation (\\(x_i\\)) is assigned to a given cluster such that the sum of squares (SS) distances of the observation to their assigned cluster \\(\\mu_k\\) is minimum.\nWe define the total within-cluster variation as follow: \\[\n\\text{tot.withinss} = \\sum_{k=1}^K W(C_k) = \\sum_{k=1}^K \\sum_{x_i \\in C_k} (x_i - \\mu_k)^2\n\\]\nThe total within-cluster sum of square measures the compactness (i.e. goodness) of the clustering and we want it to be as small as possible.\n\n\n10.3.1.2 \\(K\\)-means Algorithm\nGiven a dataset \\(\\mathbf{X}= \\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\):\n\nChoose the number of clusters \\(K\\).\n\nInitialize \\(k\\) centroids randomly.\n\nAssign step: Assign each observation \\(\\mathbf{x}_i\\) to the nearest centroid based on the Euclidean distance between the object and the centroid:\n\\[\nc_i = \\arg\\min_k \\|\\mathbf{x}_i - \\mu_k\\|^2\n\\] where \\(\\mu_k\\) is the \\(k\\)st centroid.\nUpdate step: Recalculate each centroid as the mean of points in its cluster: \\[\n\\mu_k = \\frac{1}{n_k} \\sum_{\\mathbf{x}_i \\in C_k} \\mathbf{x}_i\n\\]\nRepeat steps 3–4 until centroids stop changing (convergence) or the maximum number of iterations is reached.\n\nFor better understanding, see this video\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n10.3.1.3 Properties and Limitations of \\(K\\)-Means\nAlthough \\(K\\)-means is one of the most popular clustering algorithms, its effectiveness depends on the structure and scale of the data. Understanding its limitations helps decide when to use it — and when to choose a more robust alternative like \\(K\\)-Medoids.\n\n10.3.1.3.1 Data Type and Cluster Shape\n\\(K\\)-Means works best when:\n\nAll variables are numeric\nand measured on a comparable scale.\nThe clusters are spherical (or roughly circular in 2D).\nEach cluster has similar variance and density.\n\nWhen clusters are elongated, overlapping, or non-spherical, \\(K\\)-Means may assign points incorrectly because it relies on Euclidean distance.\n\n\n10.3.1.3.2 Sensitivity to Scale\nIn clustering, the distance between data points determines how groups are formed.\nIf variables are measured on very different scales (for example, income in euros and age in years), the variable with the largest numerical range will dominate the distance calculation. As a result, the algorithm may ignore other variables, even if they carry meaningful structure. To correct this imbalance, we use scaling transformations, which adjust the magnitude or spread of each variable before computing distances.\n\n\n\n10.3.1.4 Normalization (Min–Max Scaling)\nNormalization rescales all variables to a fixed range, usually between \\(0\\) and \\(1\\): \\[\nx' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}\n\\] This transformation preserves the shape of the distribution but ensures all features contribute equally in magnitude to the distance measure. It is particularly effective when all features have bounded ranges or similar distributions.\n\n\n10.3.1.5 Standardization (Z-score Scaling)\nStandardization rescales variables so they have mean 0 and standard deviation 1: \\[\nx' = \\frac{x - \\bar{x}}{s_x}\n\\] where \\(\\bar{x}\\) is the mean and \\(s_x\\) is the standard deviation.\nThis approach centers the data and adjusts for variance, which helps when features have different dispersions or units of measurement. Standardization is preferred when features are expected to follow roughly Gaussian distributions, or when outliers might distort min–max scaling.\n\n\n\n\n\n\nImportant\n\n\n\nWhy It Matters for \\(k\\)-Means and Similar Algorithms\n\n\\(K\\)-means relies on Euclidean distance, which is sensitive to scale. Without scaling, one feature can dominate, producing distorted clusters.\nAfter scaling or standardization, all variables contribute more fairly, and the clusters tend to reflect combined variation across features.\nScaling can change the cluster boundaries, but it does not change the underlying data relationships — it only ensures that distances are comparable.\n\n\n\nWe will show the effect of normalization and standardization on the data.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, we apply \\(K\\)-means on the raw dataset.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, we normalize the dataset and apply \\(k\\)-means.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIt is the time to standarize the dataset before applyong \\(k\\)-means.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n10.3.1.6 Measures of Cluster Quality\nTo evaluate and compare clustering results, we rely on internal validation measures — those that assess how compact and well-separated the clusters are, using only the data itself.\nThe most common measures include:\n\n10.3.1.6.1 Within-Cluster Sum of Squares (WSS)\nThe Within-Cluster Sum of Squares measures how tightly the data points in a cluster are grouped around their centroid. \\[\n\\text{WSS} = \\sum_{k=1}^{K} \\sum_{\\mathbf{x}_i \\in C_k} \\|\\mathbf{x}_i - \\boldsymbol{\\mu}_k\\|^2\n\\] where \\(C_k\\) is the set of points in cluster \\(k\\), \\(\\boldsymbol{\\mu}_k\\) is the centroid of cluster \\(k\\), and \\(\\|\\cdot\\|^2\\) shows the squared Euclidean distance between a point and its centroid.\nA smaller WSS means more compact clusters.\n\n\n10.3.1.6.2 Between-Cluster Sum of Squares (BSS)\nThe Between-Cluster Sum of Squares measures how far apart the cluster centroids are from the overall mean of the dataset. \\[\n\\text{BSS} = \\sum_{k=1}^{K} n_k \\| \\boldsymbol{\\mu}_k - \\boldsymbol{\\mu} \\|^2\n\\] where \\(n_k\\) is the number of points in cluster \\(k\\), \\(\\boldsymbol{\\mu}_k\\) is the centroid of cluster \\(k\\), and \\(\\boldsymbol{\\mu}\\) is the global mean of all data points.\nA larger BSS means better separation between clusters.\n\n\n10.3.1.6.3 Total Sum of Squares (TSS)\nThe Total Sum of Squares represents the overall variation in the dataset: \\[\n\\text{TSS} = \\sum_{i=1}^{n} \\|\\mathbf{x}_i - \\boldsymbol{\\mu}\\|^2\n\\]\nThis relationship always holds: \\[\n\\text{TSS} = \\text{BSS} + \\text{WSS}\n\\]\nThis decomposition allows us to express clustering performance as a proportion of explained variance: \\[\n\\text{Ratio} = \\frac{\\text{BSS}}{\\text{TSS}}\n\\]\nA higher ratio means that the clusters explain a greater proportion of the data’s variance.\n\n\n10.3.1.6.4 Silhouette Coefficient\nThe Silhouette Coefficient combines cohesion (how close points are within a cluster) and separation (how far points are from other clusters): \\[\ns(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n\\] where \\(a(i)\\) is the average distance from point \\(i\\) to all other points in its cluster, and \\(b(i)\\) is the smallest average distance from point \\(i\\) to points in another cluster.\nValues range from \\(–1\\) to \\(1\\), where values close to \\(1\\) indicate that observations are well-clustered, values around \\(0\\) suggest that points lie near cluster boundaries, and negative values indicate that observations are likely misclassified or assigned to the wrong cluster.\n\n\n\n\n\n\nNote\n\n\n\nSummary\n\n\n\nMeasure\nDescription\nIdeal Value\n\n\n\n\nWSS\nCompactness within clusters\nLow\n\n\nBSS\nSeparation between clusters\nHigh\n\n\nBSS / TSS\nProportion of explained variance\nHigh\n\n\nSilhouette\nCohesion and separation combined\nClose to 1\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nGood clustering minimizes WSS and maximizes BSS, producing compact, well-separated groups.\nThe Elbow Method helps find a balance between simplicity (few clusters) and accuracy (tight, distinct groups).\n\n\nImplementation in R\nThe standard R function for \\(K\\)-means clustering is stats::kmeans() which simplified format is as follow:\n\nkmeans(x, centers, iter.max = 10, nstart = 1)\n\nwhere: - x: numeric matrix, numeric data frame or a numeric vevtor - centers: possible values are the number of clusters (\\(k\\)) or a set of initial (distinct) cluster centers. If a number, a random set of (distinct) rows in \\(x\\) is chosen as the initial centers. - iter.max: the maximum number of iterations allowed. Default value if \\(10\\). - nstart: the number of random starting partitions when centers is a number. Trying nstart &gt; 1 is often recommended.\nTo create a beautiful graph of the clusters generated with the kmeans() function, will use the factoextra package.\nComputing \\(K\\)-means clustering\nAs \\(K\\)-means clustering algorithm starts with \\(K\\) randomly selected centroids, it is always recommended to use the set.seed() function in order to set a seed for R’s random number generator. The aim of using this function is to make reproducible the results.\nThe R code below performs \\(K\\)-means clustering with \\(K = 4\\):\n\nset.seed(123)\nkm.res &lt;- kmeans(df, k = 4, nstart = 25)\n\n\n\n\n\n\n\nTip\n\n\n\nActually, \\(K\\)-means starts by randomly choosing \\(K\\) points as initial centroids. Depending on these starting points, the algorithm can converge to different local minima — meaning the results may vary across runs. So, nstart means number of random initial sets of centroids to try and then, R will run the algorithm that many times and keep the best result (the one with the lowest total within-cluster sum of squares).\nThe default value of nstart in R is one. But, it is strongly recommended to compute \\(K\\)-means clustering with a large value of nstart such as \\(25\\) or \\(50\\), in order to have a more stable result.\n\n\nWith the function print(), the results of kmeans() that for example in this note, saved in km.res\n\nprint(km.res)\n\nThe printed output displys: - the cluster means or centers, that is a matrix which rows are cluster number and columns are variables. - the clustering vector which is a vector of integers (from 1 to \\(K\\)) indicating the cluster to which each point is allocated.\n\n\n\n\n\n\nNote\n\n\n\nIt is possible to compute the mean of each variable in the dataset by clusters using the original dataset.\n\naggregate(df, by = list(cluster=km.res$cluster), mean)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt the user is intersted in adding the clusters to the original data, use the following R code\n\ncbind(df, cluster = km.res$cluster)\n\n\n\nkmeans() function returns a list of components, including: - cluster: a vector of integers (from \\(1\\) to \\(k\\)) indicating the cluster to which each point is allocated. - centers: a matrix of cluster centers (cluster means) - totss: the total sum of squares (TSS). It measures the total variance in the data. - withinss: vector of within-cluster sum of squares, one component per cluster - tot.withinss: total within-cluster sum of squares, i.e., \\(\\text{sum}(withinss)\\) - betweenss: the between-cluster sum of squares, i.e., totss - tot.withinss - size: the number of observations in each cluster\nThese components can be accessed as follow:\n\n# Cluster number for each of the observations\nkm.res$cluster \n\n# Cluster size\nkm.res$size\n\n# Cluster means\nkm.res$centers\n\nVisualizing \\(K\\)-means Clusters\nIt is a good idea to plot the cluster results. These can be used to assess the choice of the number of clusters as well as comparing two different cluster analyses.\nThe idea is to visualize the data in scatter plot with coloring tach data point according to its cluster assignment.\nThe problem is that the data contains more than 2 variables and the question is what variables to choose for the \\(x\\)-\\(y\\) scatter plot. A solution is to reduce the number of dimensions by applying a dimensionality reduction algorithm, such as principal component analysis (PCA).\nIn other words, if we have a multi-dimensional dataset, a solution is to perform PCA and to plot data points according to the first two principal components coordinates.\nThe function factoextra::fviz_cluster() can be used to easily visulize \\(K\\)-means clusters. It takes \\(K\\)-means results and the original data as arguments. In the resulting plot, observations are represented by points, using orincipal components if the number of variables is greater than \\(2\\). It is also possible to draw concentration ellipse around each cluster.\n\nfviz_cluster(km.res, data = df,\npalette = c(\"#2E9FDF\", \"#00AFBB\", \"#E7B800\", \"#FC4E07\"), #Define colors\nellipse.type = \"euclid\", # Concentration ellipse\nstar.plot = TRUE, # Add segments from centroids to items\nrepel = TRUE, # Avoid label overplotting (slow)\nggtheme = theme_minimal()\n)\n\n\n\n\n\n\n\nImportant\n\n\n\n\\(K\\)-means clustering is very simple and fast algorithm. It can efficiently deal with very large data sets.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\\(K\\)-means has some weaknesses, including:\n\nIt assumes prior knowledge of the data and requires the analyst to choose the appropriate number of cluster (\\(k\\)) in advance.\n\nSolution: Compute \\(K\\)-means for a range of \\(K\\) values, for example by varying \\(k\\) between \\(2\\) and \\(10\\). Then choose the best \\(k\\) by comparing the clustering results obtained for different \\(k\\) values.\n\nThe final results obtained is sensitive to the initial random selection of cluster centers. Why is this a problem? Because, for every different run of the algorithm on the same data set, you may choose different set of initial centers. This may lead to different clustering results on different runs of the algorithm.\n\nSolution: Compute \\(K\\)-means algorithm several times with different initial cluster centers. The run with the lowest total within-cluster sum of square is selected as the final clustering solution.\n\nIt is sensitive to outliers.\n\nSolution: To avoid distortions caused by excessive outliers, it is possible to use PAM algorithm, which is less sensitive to outliers.\n\n\n\n\n\n10.3.1.7 Analysis of iris Dataset\nThe classic iris dataset contains \\(150\\) flower observations from three species (setosa, versicolor, virginica). Each observation has four numeric features: Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSuppose that we do not know that there are three groups and the goal is to determine the optimal number of clusters using Elbow method\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe plot represents the variance within the clusters. It decreases as \\(k\\) increases, but it can be seen a bend (or elbow) at \\(k = 3\\), matching the three known species.\nAnother way to visualize the elbow method for choosing the optimal number of clusters is by using the function factoextra::fviz_nbclust(). We can also add a vertical dashed line to indicate the chosen value of \\(k\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow, we apply \\(k\\)-means algorithm with \\(k = 3\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe compute cluster quality measures.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nNote\n\n\n\nWhat are “discriminative features”?\nDiscriminative features are those that best separate groups or clusters in your data. They are the variables that most strongly differentiate one cluster (or class) from another.\nThere are two most famous approach to find discriminative features:\n\nExploratory approach\nStatistical approach\n\nExploratory approach\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn this plot, you will notice that:\n\nSetosa is perfectly separated in the Petal.Length–Petal.Width plane.\nVersicolor and Virginica overlap slightly but still show visible separation.\n\nThat is why we often choose Petal.Length and Petal.Width for visualization — they are the most discriminative pair of features.\nStatistical approach We can quantify which features are most discriminative by computing, for each variable: - The between-group variance (BSS) - The within-group variance (WSS) and comparing their ratio.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe higher this ratio, the better that feature separates the species.\nIn this results, Petal.Width is clearly the most discriminative feature.\nAt first glance, it seems that Sepal.Width (124.96) should be more discriminative than Petal.Length (79.51) because its ratio is higher — but this needs context\nWhat the ratio measures? The ratio BSS/WSS BSS/WSS for each feature is: \\[\n\\text{Ratio} = \\frac{Between-Class Variance}{Within-Class Variance}\n\\] This tells you, for that one variable alone, how distinct the species means are compared to the variability within each species.\nSo, mathematically, if Sepal.Width has a higher ratio, it means that species differ more in Sepal.Width (on average) than they do within themselves. But “discriminative” depends on combined separability\nThe tricky part is that Sepal.Width alone does not visually separate the species well, even if its numeric ratio looks high. Let us check the data visually:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nYou see that: - For Sepal.Width, there is a lot of overlap between Versicolor and Virginica. The means differ, but not enough to separate groups clearly.\n\nFor Petal.Length, Setosa is completely separated, and Versicolor/Virginica overlap less.\n\nSo even though Sepal.Width’s ratio is high, its overlap pattern means it is less useful for distinguishing all three species.\n\n\n\n\n\n\nTip\n\n\n\nThe BSS/WSS ratio is a helpful indicator, but not the whole story. Always complement numerical measures with visual inspection.\n\n\n\n\nNow, based on the most discriminative features, Petal.Length and Petal.Width, we visualize the clusters.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAnother way to visulize \\(K\\)-means clusters is using the function fviz_cluster() that used the PCA (the iris_data has 4 variables) with the following code\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can also evaluat clustering with the Silhouette coefficient\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe average silhouette width (displayed in the summary) gives an overall quality score.\n\nValues &gt; \\(0.5\\) indicate clear, well-separated clusters.\nValues around \\(0.25\\)–\\(0.5\\) indicate overlapping clusters.\nValues &lt; \\(0.25\\) indicate poor structure or misclassification.\n\nIn the iris dataset, the average silhouette is typically around \\(0.55\\)–\\(0.60\\), confirming that \\(k=3\\) is a reasonable choice.\n\nSetosa usually has near-perfect silhouette values, while Versicolor and Virginica show some overlap — as expected.\n\nWe can compute the average silhouette width for different numbers of clusters.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBased on the Silhouette plots, the highest average silhouette width occurs at \\(k = 2\\). This means that, mathematically, dividing the data into two clusters gives the tightest and most well-separated grouping according to the Silhouette metric.\nHowever, the best numerical score does not always mean the most meaningful clustering.\nAt \\(k =2\\), the algorithm merges two of the real Iris species (versicolor and virginica) into a single cluster. The result has high compactness, but low interpretive accuracy.\nAt \\(k = 3\\), the average silhouette width is slightly lower, but the clusters correspond more closely to the true biological species. This result is more meaningful, even if slightly less compact.\nThus, the Silhouette Coefficient and the Elbow Method together suggest that: - \\(k=2\\) yields the most compact clustering, - but \\(k=3\\) represents the optimal trade-off between cluster compactness and real-world interpretability\n\nExercise 1Exercise 2\n\n\nUse the built-in dataset USArrests, which contains statistics on violent crime rates in the 1970s for 50 U.S. states.\n\ndata(\"USArrests\")\n\nStandardize all numeric features before applying \\(k\\)-Means using this R code:\n\nus_data &lt;- scale(USArrests)\n\nElbow Method\n\nCompute the total within-cluster sum of squares (WSS) for \\(k=1\\) to \\(10\\).\nPlot the results.\nIdentify the “elbow” point.\n\nSilhouette Method\n\nCompute the average silhouette width for \\(k=2\\) to \\(10\\).\nPlot the results using the same approach as in the Iris example.\nIdentify the \\(k\\) that maximizes the silhouette width.\n\n\n\nThis exercise demonstrates how \\(k\\)-Means clustering can be used beyond data analysis — for image compression.\nEach color in an image can be treated as a data point in RGB space, and clustering reduces the number of colors while preserving the main structure.\nLoad and visualize an image.\n\nlibrary(jpeg)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n# Read example image from R base (or provide your own file path)\nurl &lt;- \"https://upload.wikimedia.org/wikipedia/commons/3/3f/JPEG_example_flower.jpg\"\ntemp &lt;- tempfile(fileext = \".jpg\")\ndownload.file(url, temp, mode = \"wb\")\nimg &lt;- readJPEG(temp)\n\n\n# Convert to data frame\nimg_df &lt;- data.frame(\nR = as.vector(img[,,1]),\nG = as.vector(img[,,2]),\nB = as.vector(img[,,3])\n)\n\n# Sample only a few thousand pixels to avoid memory explosion\nset.seed(123)\nimg_sample &lt;- img_df %&gt;% sample_n(5000)\n\nDetermine the optimal number of clusters.\nApply \\(k\\)-means clustering to the RGB values of the image (img_sample) for \\(k = 2\\) to \\(10\\), using nstart = 10. Then, use both the Elbow Method and the Silhouette Coefficient to decide which \\(k\\) gives the best balance between cluster compactness and separation. Do both methods suggest the same number of clusters?\nWhen you decide about the optimal \\(k\\), with the following code, you can compare the original and compressed image.\n\nset.seed(123)\n\n\nk &lt;- 'put the optimal value you found'\nkm_colors &lt;- kmeans(img_sample, centers = k, nstart = 10)\n\n\n# Replace each pixel with its cluster centroid color\ncompressed_img &lt;- km_colors$centers[km_colors$cluster, ]\ncompressed_img &lt;- array(compressed_img, dim = dim(img))\n\npar(mfrow = c(1, 2))\nplot(0, type = \"n\", xlim = c(0, 1), ylim = c(0, 1),\nxlab = \"\", ylab = \"\", axes = FALSE, main = \"Original Image\")\nrasterImage(img, 0, 0, 1, 1)\n\n\nplot(0, type = \"n\", xlim = c(0, 1), ylim = c(0, 1),\nxlab = \"\", ylab = \"\", axes = FALSE, main = paste(\"Compressed Image (k =\", k, \")\"))\nrasterImage(compressed_img, 0, 0, 1, 1)\n\n\n\n\n\n\n\n10.3.2 \\(K\\)-Medoids Clustering\nThe \\(k\\)-medoids algorithm is a partitioning clustering method closely related to \\(k\\)-means, but instead of using the mean (centroid) of points to represent a cluster, it uses an actual observation (medoid) — the most centrally located data point within that cluster.\nFormally, a medoid is the point whose average dissimilarity to all other points in the cluster is minimal. i.e, \\[\nm_k = \\arg\\min_{\\mathbf{x}_i \\in C_k} \\sum_{\\mathbf{x}_j \\in C_k} d(\\mathbf{x}_i,\\mathbf{x}_j)\n\\] where \\(C_k\\) is the set of points assigned to cluster \\(k\\), and \\(d(\\mathbf{x}_i,\\mathbf{x}_j)\\) is a chosen distance or dissimilarity measure (e.g., Euclidean, Manhattan, etc.)\n\\(K\\)-medoid is a robust alternative to \\(K\\)-means clustering. This means that, the algorithm of clusters to be generated (like in \\(K\\)-means clustering). A useful approach to determine the optimal number of clusters is the silhouette method.\nThe most common \\(K\\)-medoids clustering methods is the PAM (Partitioning Around Medoids) algorithm (Kaufman and Rousseeuw 1990)\n\n10.3.2.1 PAM Algorithm\nThe PAM algorithm is based on the search for \\(K\\) representative objects or medoids among the observations of the data set. After finding a set of \\(K\\) medoids, clusters are constructed by assigning each observation to the nearest medoid. Next, each selected medoid \\(m\\) and each non-medoid data point are swapped and the objective function is computed. The objective function corresponds to the sum of the dissimilarities of all objects to their nearest medoid. The SWAP step attemps to improve the quality of the clustering by exchanging selected objects (medoids) and non-selected objects. If the objective function can be reduced by interchanging a selected object with an unselected object, then the swap is carried out. This is continued until the objective function can no longer be decreased. The goal is to find \\(k\\) representative objects which minimize the sum of the dissimilarities of the observations to their closest representative object.\nIn summary, PAM algorithm is:\n\nInitialize: Select \\(k\\) representative objects (medoids) randomly from the dataset.\nCalculate the distance (dissimilarity) matrix if it was not provided.\nAssign: Assign each remaining point to the nearest medoid based on the chosen distance metric.\nUpdate: For each cluster, try swapping the medoid with another point from the same cluster and compute the total cost: \\[\n\\text{Cost} = \\sum_{i=1}^n \\min_k d(\\mathbf{x}_i, m_k)\n\\] If a swap reduces the cost, keep it.\nIterate: Repeat assignment and update steps until the medoids no longer change.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe \\(K\\)-medoids algorithm minimizes a sum of dissimilarities, not squared distances. Thus, the cluster representative is always a real observation, which makes the method robust when data contain noise, outliers, or mixed types.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe PAM algorithm works with a matrix of dissimilarity, and to compute this matrix the algorithm can use two metrics: 1. The euclidean distance 2. The Manhattan distance\nIn practice, you should get similar results most of time, using either euclidean or Manhattan distance. If the dataset contains outliers, Manhattan distance should give more robust results, whereas euclidean would be influenced by unusual values.\n\n\n\n\n10.3.2.2 Example\nConsider the following example\n\n\n\n\nX\nY\n\n\n\n\n1\n9\n3\n\n\n2\n8\n4\n\n\n3\n4\n6\n\n\n4\n8\n5\n\n\n5\n2\n5\n\n\n6\n3\n8\n\n\n7\n8\n8\n\n\n8\n4\n4\n\n\n9\n10\n4\n\n\n10\n9\n6\n\n\n\n\nlibrary(tibble)\n\n# Dataset (same as your table)\n\npoints &lt;- tibble::tibble(\nID = 1:10,\nX = c(9, 8, 4, 8, 2, 3, 5, 4, 10, 9),\nY = c(3, 4, 6, 5, 5, 8, 8, 4, 4, 6)\n)\n\npoints\n\n# A tibble: 10 × 3\n      ID     X     Y\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     9     3\n 2     2     8     4\n 3     3     4     6\n 4     4     8     5\n 5     5     2     5\n 6     6     3     8\n 7     7     5     8\n 8     8     4     4\n 9     9    10     4\n10    10     9     6\n\n\nAssume \\(k = 2\\) and use the specified points (\\(m_1 = (4,6)\\)) and \\(m_2 = (9,3)\\)\n\nlibrary(ggplot2)\n\n# Define medoid points\nmedoids &lt;- data.frame(\nX = c(4, 6),\nY = c(9, 3),\nlabel = c(\"Medoid 1\", \"Medoid 2\")\n)\n\n# Scatter plot\n\nggplot(points, aes(x = X, y = Y)) +\ngeom_point(size = 3, color = \"black\") +\ngeom_point(data = medoids, aes(x = X, y = Y, color = label), size = 5) +\ngeom_text(aes(label = ID), vjust = -1, size = 3) +\nscale_color_manual(values = c(\"Medoid 1\" = \"red\", \"Medoid 2\" = \"blue\")) +\nlabs(\ntitle = \"K-Medoids Example Dataset\",\nsubtitle = \"Initial medoids highlighted in red and blue\",\nx = \"X coordinate\",\ny = \"Y coordinate\",\ncolor = \"Medoids\"\n) +\ntheme_minimal() +\ntheme(legend.position = \"bottom\")\n\n\n\n\nScatter plot of the k-medoids example dataset with initial medoids highlighted.\n\n\n\n\nLet consider we use Manhattan distance because it is easy to compute and commonly used with \\(K\\)-medoids.\nStep 1 – Assign each point to the nearest medoid\n\n\n\nID\nX\nY\nd to (4,6)\nd to (9,3)\nAssign\n\n\n\n\n1\n9\n3\n8\n0\nC2\n\n\n2\n8\n4\n6\n2\nC2\n\n\n3\n4\n6\n0\n8\nC1\n\n\n4\n8\n5\n5\n3\nC2\n\n\n5\n2\n5\n3\n9\nC1\n\n\n6\n3\n8\n3\n11\nC1\n\n\n7\n5\n8\n3\n9\nC1\n\n\n8\n4\n4\n2\n6\nC1\n\n\n9\n10\n4\n8\n2\nC2\n\n\n10\n9\n6\n5\n3\nC2\n\n\n\nClusters after assignment is:\n\n\\(C_1(m_1) = \\{3,5,6,7,8\\}\\) (around \\((4,6)\\))\n\\(C_2(m_2) = \\{1,2,4,9,10\\}\\) (around \\((9,3)\\))\n\nThe total cost (sum of within-cluster distances) is \\[\nC_1 + C_2 = 9 + 11 = 20\n\\] where: - \\(C_1 = 0 + 3 + 3 + 3 + 2  = 11\\) - \\(C_2 = 0 + 2 + 2 + 2 + 3 = 9\\)\n** Step2** – Update (try swaps)\nWe evaluate each point in a cluster as a potential medoid, computing the sum of Manhattan distances to all other points in that cluster.\nCluster C1 = {3,5,6,7,8}\n\n\n\nCandidate (ID)\nSum of distances\n\n\n\n\n3 (4,6)\n11 ← best\n\n\n5 (2,5)\n16\n\n\n6 (3,8)\n14\n\n\n7 (5,8)\n16\n\n\n8 (4,4)\n15\n\n\n\nCluster C2 = {1,2,4,9,10}\n\n\n\nCandidate (ID)\nSum of distances\n\n\n\n\n1 (9,3)\n10\n\n\n2 (8,4)\n8 ← best\n\n\n4 (8,5)\n9\n\n\n9 (10,4)\n10\n\n\n10 (9,6)\n11\n\n\n\nUpdate result:\n\nMedoid for \\(C_1\\) stays ID 3 (\\((4,6)\\))\nMedoid for \\(C_2\\) changes to ID 2 (\\((8,4)\\)) which it improves the total cost.\n\nAfter updating the medoids, the next step is to recompute the total cost (sum of distances from each point to its nearest medoid).\n\n\n\nID\nX\nY\nd to (4,6)\nd to (8,4)\nmin d\nAssign\n\n\n\n\n1\n9\n3\n8\n2\n2\nC2\n\n\n2\n8\n4\n6\n0\n0\nC2\n\n\n3\n4\n6\n0\n6\n0\nC1\n\n\n4\n8\n5\n5\n1\n1\nC2\n\n\n5\n2\n5\n3\n7\n3\nC1\n\n\n6\n3\n8\n3\n9\n3\nC1\n\n\n7\n5\n8\n3\n7\n3\nC1\n\n\n8\n4\n4\n2\n4\n2\nC1\n\n\n9\n10\n4\n8\n2\n2\nC2\n\n\n10\n9\n6\n5\n3\n3\nC2\n\n\n\nThe total cost (sum of within-cluster distances) is \\[\nC_1 + C_2 = (0+1+3+3+3+2) + (2+0+1+2+3) = 19\n\\]\nthe following R code helps to do the computations.\n\nlibrary(dplyr)\n\n# Data\npts &lt;- tibble::tibble(\n  ID = 1:10,\n  X  = c(9,8,4,8,2,3,5,4,10,9),\n  Y  = c(3,4,6,5,5,8,8,4, 4,6)\n)\n\nmanhattan &lt;- function(a, b) sum(abs(a - b))\n\n# Helper to compute total cost for given medoids\ntotal_cost &lt;- function(m1, m2) {\n  pts %&gt;%\n    rowwise() %&gt;%\n    mutate(\n      d1 = manhattan(c(X, Y), m1),\n      d2 = manhattan(c(X, Y), m2),\n      mind = min(d1, d2),\n      Assign = if_else(d1 &lt;= d2, \"C1\", \"C2\")\n    ) %&gt;%\n    ungroup()\n}\n\n# Initial medoids: (4,6), (9,3)\ninit_tbl &lt;- total_cost(c(4,6), c(9,3))\nsum_init &lt;- sum(init_tbl$mind)\n\n# Updated medoids: (4,6), (8,4)\nupd_tbl  &lt;- total_cost(c(4,6), c(8,4))\nsum_upd  &lt;- sum(upd_tbl$mind)\n\nlist(\n  initial_total = sum_init,\n  updated_total = sum_upd,\n  initial_assignments = init_tbl %&gt;% select(ID, X, Y, d1, d2, mind, Assign),\n  updated_assignments = upd_tbl %&gt;% select(ID, X, Y, d1, d2, mind, Assign)\n)\n\n$initial_total\n[1] 21\n\n$updated_total\n[1] 19\n\n$initial_assignments\n# A tibble: 10 × 7\n      ID     X     Y    d1    d2  mind Assign\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n 1     1     9     3     8     0     0 C2    \n 2     2     8     4     6     2     2 C2    \n 3     3     4     6     0     8     0 C1    \n 4     4     8     5     5     3     3 C2    \n 5     5     2     5     3     9     3 C1    \n 6     6     3     8     3    11     3 C1    \n 7     7     5     8     3     9     3 C1    \n 8     8     4     4     2     6     2 C1    \n 9     9    10     4     8     2     2 C2    \n10    10     9     6     5     3     3 C2    \n\n$updated_assignments\n# A tibble: 10 × 7\n      ID     X     Y    d1    d2  mind Assign\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; \n 1     1     9     3     8     2     2 C2    \n 2     2     8     4     6     0     0 C2    \n 3     3     4     6     0     6     0 C1    \n 4     4     8     5     5     1     1 C2    \n 5     5     2     5     3     7     3 C1    \n 6     6     3     8     3     9     3 C1    \n 7     7     5     8     3     7     3 C1    \n 8     8     4     4     2     4     2 C1    \n 9     9    10     4     8     2     2 C2    \n10    10     9     6     5     3     3 C2    \n\n\nSince there is an improvement in the total cost, we continue with the new medoids: \\((4,6)\\) and \\((8,4)\\).\nExercise Continue this process and find the best medoids.\n\n\n10.3.2.3 Implementation PAM in R\nThe function cluster::pam() and fpc::pamk() can be used compute PAM.\nThe function pamk() does not require a user to decide the number of clusters \\(K\\).\nThe simplified format of pam() is\n\npam(x, k, metric = 'euclidean')\n\nwhere:\n\nx: possible value includes: numeric data matrix or numeric data frame, or dissimilarity matrix – in this case x is typically the output of daisy() or dist().\nk: the number of clusters\nmetric: the distance matrix to be used. Available options are \"euclidean\" and \"manhattan\".\nstand: logical value; if TRUE, the variables (columns) in x are standardized before calculating the dissilarities. Ignored when x is a dissimilarity matrix.\n\nSimilar to \\(K\\)-means, the following R code computes PAM algorithm\n\npam.res &lt;- pam(df, k = 2)\n\n# print the results \nprint(pam.res)\n\nThe printed output shows:\n\nthe cluster medoids: a matrix, which rows are the medoids and columns are varibles\nthe clustering vector: a vector of integers (from \\(1\\) to \\(k\\)) indicating the cluster to which point is allocated.\n\nThe function pam() returns an object of class pam which components include:\n\nmedoids: Objects that represent clusters\nclustering: a vector containing the cluster number of each object\n\nThese components can be accessed as\n\n# Cluster medoids\npam.res$medoids\n\n# Cluster numbers\npm.res$clustering\n\nTo create a beautiful graph of the clusters generated with the pam() function, similar to \\(K\\)-means, the function factoextra::fviz_cluster() can be used as\n\nfviz_cluster(pam.res,\n  palette = c(\"#00AFBB\", \"#FC4E07\"), # color palette\n  ellipse.type = \"t\", # Concentration ellipse\n  repel = TRUE, # Avoid label overplotting (slow)\n  ggtheme = theme_classic()\n)\n\n\n\n10.3.2.4 Estimating the optimal number of clusters\nTo estimate the optimal number of clusters, we will use the average silhouette method. The idea is to compute PAM algorithm using different values of clusters \\(k\\). Next, the average clusters silhouette is drawn according to the number of clusters. The average silhouette measures the quality of a clustering. A high average silhouette width indicates a good clustering. The optimal number of clusters \\(k\\) is the one that maximize the average silhouette over a range of possible values for \\(k\\) (Kaufman and Rousseeuw 1990).\nThe R function factoextra::fviz_nbclust() provides a convenient solution to estimate the optimal number of clusters.\n\nlibrary(cluster)\nlibrary(factoextra)\nfviz_nbclust(df, pam, method = \"silhouette\")+\ntheme_classic()\n\n\n\n10.3.2.5 Example of PAM in R\nHere, we apply PAM algoritm to iris dataset and we expect to have similar results to \\(K\\)-means since there is no outlier in this dataset.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSimilar to \\(K\\)-means, we can use the funtion fviz_cluster() with the following code:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAs you see, the black stars represent the medoids — actual data points from the dataset.\nUnlike \\(k\\)-means, the medoid is a real observation, not an artificial centroid.\nVisual Comparison: k-Means vs k-Medoids\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n10.3.2.6 Exercise\nThe dataset has a mix of numerical, ordinal, and categorical features: Age, Sex, ChestPain, Thal, RestBP, Chol, MaxHR, AHD (disease outcome), etc.\nDo the PAM algorithm for this dataset\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Clustering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/clustring.html#cluster-validation",
    "href": "chapters/clustring.html#cluster-validation",
    "title": "10  Clustering",
    "section": "10.4 Cluster Validation",
    "text": "10.4 Cluster Validation\nThe cluster validation consists of measuring the goodness of clustering results.\nBefore applying any clustering algorithm to a dataset, the first thing to do is to assess the clustering tendency. That is, whether applying clustering is suitable for the data. If yes, then how many clusters are there. Next, you can perform clustering methods. Finally, you can use a number of measures to evaluate the goodness of the clustering results.\n\n10.4.1 Assessing Clustering Tendency\nBefore applying any clustering method on your data, it is important to evaluate whether the datasets contains meaningful (i.e. non-random structures) or not. If yes, then how many clusters are there. This process is defined as the assessing of clustering tendency or the feasibility of the clustering analysis.\n\n\n\n\n\n\nImportant\n\n\n\nA big issue, in cluster analysis, is that clustering methods will return clusters even if the data does not contain any clusters. In other words, if you blindly apply a clustering method on a data set, it will divide the data into clusters because that is what it supposed to do.\n\n\n\n10.4.1.1 Methods for assessing clustering tendency\nThere are two methods for evaluating the clustering tendency:\n\na statistical method like Hopkins statistic\na visual method like Visual Assessment of Cluster Tendency (VAT) algorithm\n\n\n10.4.1.1.1 Statistical methods\nThe Hopkins statistic is used to assess the clustering tendency of a dataset by measuring the probability that a given dataset is generated by a uniform data distribution. In other words, it tests the spatial randomness of the data.\nLet \\(D\\) be a real dataset. The Hopkins statistic can be calculated as follows:\n\nSample uniformly \\(n\\) points (\\(p_1\\), \\(\\ldots\\), \\(p_n\\)) from \\(D\\).\nFor each point \\(p_i \\in D\\), find its nearest neighbor \\(p_j\\); then compute the distance between \\(p_i\\) and \\(p_j\\) and denote it as \\(x_i = \\text{dist}(p_i, p_j)\\)\nGenerate a simulated dataset (\\(\\text{random}_{D}\\)) drawn from a random uniform distribution with \\(n\\) points (\\(q_1\\), \\(\\ldots\\), \\(q_n\\)) and the same variation as the original real dataset \\(D\\).\nFor each point \\(q_i \\in \\text{random}_D\\), find its nearest neighbour \\(q_j\\) in \\(D\\); then compute the distance between \\(q_i\\) and \\(q_j\\) and denote it as \\(y_i = \\text{dist}(q_i, q_j)\\)\nCalculate the Hopkins statistic (denoted by \\(H\\)) as the mean nearst neighbour distance in the random dataset divided by the sum of the mean nearst neighbour disrances in the real and across the simulated dataset.\n\nThe formula is defind as follow: \\[\nH = \\frac{\\sum_{i=1}^n y_i}{\\sum_{i=1}^n x_i + \\sum_{i=1}^n y_i}\n\\]\nA value of \\(H\\) about \\(0.5\\) means that \\(\\sum_{i=1}^n y_i\\) and \\(\\sum_{i=1}^n x_i\\) are close to each other, and thus the data \\(D\\) is uniformaly distributed.\nThe null and the alternative hypotheses are defined as follow:\n\nnull hypothesis: the dataset \\(D\\) is uniformly distributed (i.e. no meaninful clusters)\nalternative hypotheis: the dataset \\(D\\) is not uniformly distributed (i.e. contains meaningful clusters)\n\n\nIf the value of Hopkins statistic is close to zero, then we can reject the null hypothesis and conclude that the dataset \\(D\\) is significantlt a clusterable data.\n\n\n\n\n\n\n\nThe R function hopkins::hopkins() can be used to statistically evaluate clustering tendency. The simplified format is\n\nhopkins(data, m)\n\nwhere:\n\ndata: a dataframe of matrix\nm: the number of points to be selected from the data\n\n\n\n\nExample:\nConsider a generated dataset to compare with the iris dataset.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIt can be seen that the iris dataset is highly clusterable (the \\(H\\) value = \\(0.18\\) which is far below the threshold \\(0.5\\)). However, the random dataset is not clusterable (\\(H = 0.5\\)).\n\n\n10.4.1.1.2 Visual methods\nThe algorithm of the visual assessment of cluster tendency (VAT) approach (Bezdek and Hathaway 2002) is as follow:\n\nCompute the dissimilarity (DM) matrix between the observations in the dataset using the Euclidean distance meature\nReorder the DM so that similar objects are close to one another. This process create an ordered dissimilarity (ODM)\nThe ODM is displayed as an ordered dissimilarity image (ODI), which is the visual output of VAT.\n\n\n\n\n\n\n\nFor the visual assessment of clustering tendency, first, the dissimilarity matrix between observations should be computed using the function dist(). Next, the function factoextra::fviz_dist() is used to display the dissimilarity matrix.\n\n\n\nExample: Following the example provided in the statistical methods, we use those datasets, here.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn these plots, red color shows high similarity (i.e., low dissimilarity) and blue colors means low similarity.\nTHe colour level is proportional to the value of the dissilarity between observations: pure red if \\(\\text{dist}(x_i, x_j) = 0\\) and pure blue if \\(\\text{dist}(x_i,x_j) = 1\\). Observations belonging to the same cluster are displayed in consecutive order.\nThe dissimilarity matrix image confirms that there is a cluster structure in the iris dataset but not un the random one.\nThe VAT algorithm detects the clustering tendency in a visual form by computing the number of square shaped dark blocks along the diagonal in a VAT image.\n\n\n\n\n10.4.2 Determing the optimal number of clusters\nDeterming the optimal number of clusters in a dataset is a fundamental issue in partitioning clustering, such as \\(k\\)-means clustering, which requires the user to specify the number of clusters \\(k\\) to be generated.\nUnfortunately, there is no definitive answer to this question. The optimal number of clusters is somehow subjective and depends on the method used for measuring similarities and the parameters used for partitioning.\nThe methods include direct methods and statistical testing methods:\n\nDirect methods: consists of optimizing a criterion, such as the within cluster sums of squares or the average silhouette. The corresponding methods are named elbow and silhouette methods, respectively.\nStatistical testing methods: consists of computing evidence against null hypothesis. An example is gap statistic\n\nIn addition to elbow, silhouette, and gap statistic methods, there are more than \\(30\\) other indices and methods that have been published for identifying the optimal number of clusters. There are some R codes for computing all these \\(30\\) indices in order to decide the best number of clusters using the ``majority rule”.\n\n10.4.2.1 Elbow method\nRecall that, the basic idea behind partitioning methods, such as \\(K\\)-means clustering, is to define clusters such that the total intra-cluster variation [or total within-cluster sum of square (WSS)] is minimized. The total WSS measures the compactness of the clustering and we want it to be as small as possible.\nThe Elbow method looks at the total WSS as a function of the number of clusters: One should choose a number of clusters so that adding another cluster does not improve much better the total WSS.\nThe optimal number of clusters can be defined as follow:\n\nCompute clustering algorithm (e.g. \\(K\\)-means clustering) for different values of \\(k\\). For example, by varying \\(k\\) from \\(1\\) to \\(10\\) clusters.\nFor each \\(k\\), calculate the total within-cluster sum of square (WSS).\nPlot the curve of WSS according to the number of clusters \\(k\\).\nThe location of a bend (knee) in the plot is generally considered as an indicator of the appropraite number of clusters.\n\n\n\n\n\n\n\nTip\n\n\n\nNote that the elbow method is sometimes ambiguous. An alternative is the average silhoutte method (Kaufman and Rousseeuw 1990) which can be also used with any clustering approach.\n\n\n\n\n10.4.2.2 Average silhouette method\nThe average silhoutte approach measures the quality of a clustering. That is, it determines how well each observation lies within its cluster. A high average silhouette width indicates a good clustering.\nAverage silhouette methods computes the average silhouette of observations for different values of \\(k\\). The optimal number of clusters \\(k\\) is one that maximize the average silhouette over a range of possible values for \\(k\\) (Kaufman and Rousseeuw 1990).\nThe average silhouette method is similar to the elbow method and can be computed as follow:\n\nCompute clustering algorithm (e.g. \\(k\\)-means clustering) for different values of \\(k\\). For instance, by varying from \\(1\\) to \\(10\\) clusters.\nFor each \\(k\\), calculate the average silhouette of observations (avg.sil)\nPlot the curve of avg.sil according to the number of clusters \\(k\\).\nThe location of the maximum is considered as the appropraite number of clusters.\n\n\n\n10.4.2.3 Gap statistic method\nThe gap statistic (Tibshirani, Walther, and Hastie 2001) can be applied to any clustering method.\nThe gap statistic compares the total within intra-cluster variation for different values of \\(k\\) with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nThe algorithm works as follows:\n\nCluster the observed data, varying the number of clusters (\\(k\\)) from \\(1\\) to \\(k_{\\max}\\), and compute the corresponding total within intra-cluster variation \\(W_k\\).\nGenerate \\(B\\) reference datasets with a random uniform distribution. Cluster each of these reference dataset with varying number of clusters \\(k = 1, \\ldots, k_{\\max}\\), and compute the corresponding total within intra-cluster variation \\(W_{kb}\\).\nCompute the estimated gap statistic as the deviation of the observed \\(W_k\\) value from its expected value \\(W_{kb}\\) under the null hypothesis: \\[\n\\text{Gap}(k) = \\frac{1}{B} \\sum_{b=1}^{B} \\log(W_{kb}^*) - \\log(W_k)\n\\] Compute also the standard deviation of the statistics.\nChoose the number of clusters as the smallest value of \\(k\\) such that the gap statistic is within one standard deviation of the gap at \\(k+1\\): \\(\\text{Gap}(k) \\ge \\text{Gap}(k+1) - s_{k+1}\\).\n\n\n\n\n\n\n\nNote\n\n\n\nNote that using \\(B = 500\\) gives quite precise results so that the gap plot is basically unchanged after an another run.\n\n\n\n\n10.4.2.4 Computing the number of clusters using R\nThere are two functions for determing the optimal number of clusters:\n\nfactoextra::fviz_nbclust() function: It can be used to compute the three differet methods for any partitioning clustering methods.\nNbClust::NbClust() function (Charrad et al. 2014): It provides \\(30\\) indices for determining the relevant number of clusters and proposes to users the best clustering sheme from the different results obtained by varying all combinations of number of clusters, distance measures, and clustering methods. It can simultanously computes all the indices and determine the number of clusters in a single function call.\n\nThe simplified format is as:\n\nfvix_nbclust(x, FUNcluster, method = c('silhouette', 'wss', 'gap_stat'))\n\nwhere:\n\nx: numeric matrix or dataframe\nFUNcluster: a partitioning function. Allowed values include kmeans, pam, and others.\nmethod: the method to be used for determing the optimal number of clusters.\n\nThe R code below determine the optimal number of clusters for \\(k\\)-means clustering:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe results show that:\n\nElbow method suggested \\(4\\) clusters\nSilhouette method suggested \\(2\\) clusters\nGap statistic method suggested \\(2\\) clusters\n\nAccording to these observatios, it is possible to define \\(k = 2\\) as the optimal number of clusters in the iris dataset although, we know that the dataset has three clusters but based on the plots of the data, choosing \\(k=2\\) makes sense.\n\n\n\n\n\n\nNote\n\n\n\nThe disadvantage of elbow and average silhouette methods is that, they measure a global clustering characteristic only. A more sophisticated method is to use the gap statistic which provides a statistical procedure to formalize the elbow/silhouette heuristic in order to estimate the optimal number of clusters.\n\n\nThe simplified format of the function NBClust() is\n\nNbClust(data = NULL, diss = NULL, distance = 'euclidean', min.nc = 2, max.nc = 15, method = NULL)\n\nwhere: - data: matrix - diss: dissimilarity matrix to be used. By default, diss=NULL, but if it is replaced by a dissimilarity matrix, distance should be NULL - distance: the distance measure to be used to compute the dissimilarity matrix. Possible values include \"euclidean\", \"manhattan\" or NULL. - min.nc and max.nc: minimal and maximal number of clusters, respectively. - method: The cluster analysis method to be used including \"ward.D\", \"ward.D2\", \"single\", \"complete\", \"average\", \"kmeans\", and more.\nTo compute NbClust() for \\(k\\)-means, use method = \"kmeans\".\nTh R code below computes NbClust() for \\(k\\)-means:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBased on this output, the best number of clusters is \\(2\\).\n\n\n\n10.4.3 Cluster Validation Statistics\nThe term cluster validation is used to design the procedure of evaluating the goodness of clustering algorithm results. This is important to avoid finding patterns in a random data, as well as, in the situation where you want to compare two clustering algorithms.\nBased on Theodoridis and Koutroumbas (2008), Brock et al. (2008), Charrad et al. (2014); clustering validation statistics can be categorized into \\(3\\) classes:\n\nInternal cluster validation. which uses the internal information of the clustering process to evaluate the goodness of a clustering structure without reference to external information. It can be also used for estimating the number of clusters and the appropriate clustering algorithm without any external data.\nExternal cluster validation, which consists of comparing the results of a clutser analsis to an externally known result, such as externally provided class labels. It measures the extent to which cluster labels match externally supplied class labels. Since we know the “true” cluster number in advance, this approach is mainly used for selecting the right clustering algorithm for a specific data set.\nRelative cluster validation, which evaluates the clustering structure by varying different parameter values for the same algorithm (e.g.,: varying the number of clusters \\(k\\)). It is generally used for determining the optimal number of clusters.\n\n\n10.4.3.1 Internal measures for cluster validation\nRecall that the goal of partitioning algoritms is to split the dataset into clusters of observations, such that:\n\nthe observations in the same cluster are similar as much as possible\nthe observation in the different clusters are highly distict\n\nThus, the average distance within cluster should be as samll as possible, and the average distance between clusters should be as large as possible.\nInternal validation measures reflect often the compactness, the connectedness and the separation of the cluster partitions.\n\nCompactness or cluster cohesion: Measures how close are the objects within the same cluster. A lower within-cluster variation is an indicator of a good compactness (i.e., a good clustering). The different indices for evaluating the compactness of clusters are base on distance measures such as the cluster-wise within average/median distances between observations.\nSeparation: Measures how well-separated a cluster is from other clusters. The indices used as separation measures include:\n\n\ndistances between cluster centers\nthe pairwise minimum distances between objects in different clusters\n\n\nConnectivity: corresponds to what extent items are placed in the same cluster as their nearest neighbors in the data space. The connectivity has a value between \\(0\\) and \\(\\infty\\) and should be minimized.\n\nGenerally most of the indices used for internal clustering validation combine compactness and speration measures as follow: \\[\n\\text{Index} = \\frac{\\alpha \\times \\text{Separation}}{\\beta \\times \\text{Compactness}}\n\\] where \\(\\alpha\\) and \\(\\beta\\) are weights.\nThere are two commonly used indices for assessing the goodness of clustering:\n\nthe *silhouette width**\nthe Dunn index\n\nThese internal measure can be used also to determine the optimal number of clusters in the data.\n\n10.4.3.1.1 Silhouette coefficient\nThe silhouette analysis measures how well an observation is clustered and it estimates the average distance between clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters.\nFor each observation \\(\\mathbf{x}_i\\), the silhouette width \\(s_i\\) is calculated as follows:\n\nFor each observation \\(\\mathbf{x}_i\\), calculate the average dissimilarity \\(a_i\\) between \\(\\mathbf{x}_i\\) and all other points of the cluster to which \\(\\mathbf{x}_i\\) belongs.\nFor all other clusters \\(C\\), to which \\(\\mathbf{x}_i\\) does not belong, calculate the average dissimilarity \\(d(\\mathbf{x}_i,C)\\) of \\(\\mathbf{x}_i\\) to all observations of \\(C\\). The smallest of these \\(d(\\mathbf{x}_i,C)\\) is defined as \\(b_i = \\min_C d(\\mathbf{x}_i,C)\\) The value of \\(b_i\\) can be seen as the dissimilarity between \\(\\mathbf{x}_i\\) and its “neighbor” cluster, i.e., the nearest one to which it does not belong.\nFinally the silhouette width of the observation \\(\\mathbf{x}_i\\) is defined by the formula: \\[\nS_i = \\frac{b_i - a_i}{\\max(a_i, b_i)}\n\\]\n\nSilhouette width can be interpreted as follow:\n\nObservations with a large \\(S_i\\) (almost \\(1\\)) are very well clustered.\nA small \\(S_i\\) (around \\(0\\)) means that the observation lies between two clusters.\nObservations with a negative \\(S_i\\) are probably placed in the wrong cluster.\n\n\n\n10.4.3.1.2 Dunn Index\nThe Dunn index is another internal clustering validation measure which can be computed as follow:\n\nFor each cluster, compute the distance between each of the observations in the cluster and the observations in the other clusters\nUse the minimum of this pairwise distance as the inter-cluster separation (min.separation)\nFor each cluster, compute the distance between the observations in the same cluster.\nUse the maximal intra-cluster distance (i.e. maximum diameter) as the intra-cluster compactness\nCalculate the Dunn index (\\(D\\)) as follow: \\[\nD = \\frac{min.separation}{max.diameter}\n\\]\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the dataset contains compact and well-separated clusters, the diameter of the clusters is expected to be small and the distance between the clusters is expected to be large. Thus, Dunn index should be maximized.\n\n\n\n\n\n10.4.3.2 External measures for clustering validation\nThe aim is to compare the udentified clusters (e.g. by \\(k\\)-means, PAM, etc.) to an external reference.\nIt is possible to quantify the agreement between partitioning clusters and external reference using either the corrected Rand index and Meila’s variation index VI, which are implemented in R function fpc::cluster.stats().\nThe corrected Rand index varies from \\(-1\\) (no agreement) to \\(1\\) (perfect agreement).\n\n\n\n\n\n\nNote\n\n\n\nExternal clustering validation can be used to select suitable clustering algorithm for a given dataset.\n\n\n\n\n10.4.3.3 Implementation in R\nThe main package for computing clustering validation statistics is fpc.\n\n\n\n\n\n\nTip\n\n\n\nThere is another function that does enhanced clustering is factoextra::eclust() which provides several advantages:\n\nIt simplifies the workflow of clustering analysis.\nIt can be used to cumpute different methods of clustering in a single line function call\nCompared to the standard partitioning functions (\\(k\\)-means, PAM, …) which requires the user to specify the optimal number of clusters, the function eclust() computes automatically the gap statistic for estimating the right number of clusters.\nIt provides silhouette information for all partitioning methods and hierarchical clustering\nIt draws beautiful graphs using ggplot2 package\n\n\n\nThe simplified format of the eclust() function is:\n\neclust(x, FUNcluster = 'kmeans', hc_metric = 'euclidean', ...)\n\nwhere:\n\nx: numeric vector, data matrix, or data frame\nFUNcluster: a clustering function including \"kmeans\", \"pam\", \"clara\", \"fanny\", \"hclust\", \"agnes\", and \"diana\"\nhc_metric: character string specifying the metric to be used for calculating dissimilarities between observations. Allowed values are those accepted by function dist(), including \"euclidean\", \"manhattan\", \"maximum\", \"canberra\", \"binary\", \"minkowski\" and correlation-based distance measures including \"perason\", \"spearman\", or \"kendall\". This parameter is used only when FUNcluster is a hierarchical clustering function such as one of hclust, agnes, or diana.\n...: other arguments to be passed to FUNcluster.\n\nThe function eclust() returns an object of class eclust containing the result of the standard function used. It includes:\n\ncluster: the cluster assignment of observations after cutting the tree (in hirarical clustering)\nnbclust: the number of clusters\nsilinfo: the silhouette information of observations\nsize: the size of clusters\ndata: a matrix containing the original or the standardized data (if stand = TRUE)\ngap_stat: containing gap statistic\n\nWe countine our example with iris dataset as\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n10.4.3.4 Cluster validation\n\n10.4.3.4.1 Silhouette plot\nRecall that the silhouette coefficient (\\(S_i\\)) measures how similar an observation \\(\\mathbf{x}_i\\) is to the other observations in its own cluster versus those in the neighbor cluster. \\(S_i\\) values range from \\(1\\) to \\(-1\\):\n\nA value of \\(S_i\\) close to \\(1\\) indicates that the observation is well clustered. In the other words, the observation \\(\\mathbf{x}_i\\) is similar to the other observations in its group.\nA value of \\(S_i\\) close to \\(-1\\) indicates that the observation is poorly clustered, and that assignment to some other cluster would probably improve the overall results. It is possible to draw silhouette coefficients of observations using the function factoextra::fviz_silhouette(), which will also print a summary of the silhouette analysis output. To avoid this, you can use the option print.summary = FALSE.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSilhouette information can be extracted as follow:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIt can be seen that several samples, in cluster 2, have a negative silhouette coefficient. This means that they are not in the right cluster. We can find the name of these samples and determine the clusters they are closer (neighbor cluster), as follow:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n10.4.3.4.2 Computing Dunn index and other cluster validation statistics\nThe function fpc::cluster.stats() and the function NbClust::NbClust() can be used to compute Dunn index and many other indices.\nThe simplified format is\n\ncluster.stats(d = NULL, clustering, al.clustering = NULL)\n\nwhere: - d: a distance object between cases as generated by the dist() function - clustering: vector containing the cluster number of each observation - alt.clustering: vector such as for clustering, indicating an alternative clustering\nThe function cluster.stats() returns a list containing many components useful for analyzing the intrinsic characteristics of a clustering:\n\ncluster.number: number of clusters\ncluster.size: vector containing the number of points in each cluster\naverage.distance, median.distance: vector containing the cluster-wise within average/median distances\naverage.between: average distance between clusters. We want it to be as large as possible\naverage.within: average distance within clusters. We want it to be as small as possible\nclus.avg.silwidths: vector of cluster average silhouette widths. Recall that, the silhouette width is also an estimate of the average distance between clusters. Its value is comprised between \\(1\\) and \\(-1\\) with a value of \\(1\\) indicating a very good cluster\nwithin.cluster.ss: a generalization of the within clusters sum of squares (\\(k\\)-means objective function), which is obtained if \\(d\\) is a Euclidean distance matrix\ndunn, dunn2: Dunn index\ncorrected.rand, vi: Two indexes to assess the similarity of two clustering: the corrected Rand index and Meila’s VI\n\nThe clustering quality statistic for \\(k\\)-means can be computed. Look at the within.cluster.ss (within clusters sum of squares), the average.within (average distance within clusters) and clus.avg.silwidths (vector of cluster average silhouette widths).\n\nlibrary(fpc)\n\ndf_iris &lt;- iris[, 1:4]\n# standardize the dataset\ndf_iris_scaled &lt;- scale(df_iris)\n\nkm.res &lt;- eclust(df_iris_scaled, FUNclust = 'kmeans', k = 3, nstart = 25, graph = FALSE)\n\n# Statistics for k-means clustering\nkm_stats &lt;- cluster.stats(dist(df_iris_scaled), km.res$cluster)\n# Dun index\nkm_stats$dunn\n\n[1] 0.02649665\n\n\n\n\n\n10.4.3.5 External clustering validation\nAmong the values returned by the function cluster.stats(), there are two indexes to assess the similarity of two clustering, namely the corrected Rand index and Meila’s VI.\nWe know that the iris data contains exactly \\(3\\) groups of species. Does the \\(k\\)-means clustering matches with the true structure of the data? We can use the function cluster.stats() to answer to this question.\nLet start by computing a cross-tabulation between \\(k\\)-means clusters and the reference Species labels:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIt can be seen that: - All setosa species (\\(n = 50\\)) has been classified in cluster 1. - A large number of versicolor species (\\(n = 39\\)) has been classified in cluster 3. Some of them (\\(n = 11\\)) have been classified in cluster 2. - A large number of virginica species (\\(n = 36\\)) has been classified in cluster 2. Some of them (\\(n = 14\\)) have been classified in cluster 3. It is possible to quantify the agreement between Species and \\(k\\)-means clusters\nIt is possible to quantify the agreement between Species and \\(k\\)-means clusters using either the corrected Rand index and Meila’s VI provided as follow:\n\nlibrary(\"fpc\")\n\ndf_iris &lt;- iris[, 1:4]\n# standardize the dataset\ndf_iris_scaled &lt;- scale(df_iris)\n\nkm.res &lt;- eclust(df_iris_scaled, FUNclust = 'kmeans', k = 3, nstart = 25, graph = FALSE)\n\n# Compute cluster stats\nspecies &lt;- as.numeric(iris$Species)\nclust_stats &lt;- cluster.stats(d = dist(df_iris_scaled),\nspecies, km.res$cluster)\n# Corrected Rand index\nclust_stats$corrected.rand\n\n[1] 0.6201352\n\n# VI\nclust_stats$vi\n\n[1] 0.7477749\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe corrected Rand index provides a measure for assessing the similarity between two partitions, adjusted for chance. Its range is \\(-1\\) (no agreement) to \\(1\\) (perfect agreement).\n\n\nThe same analysis can be computed for PAM clustering\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n10.4.4 Choosing the Best Clustering Algorithms\nChoosing the best clustering method for a given data can be a hard task for the analyst. This article describes the R package clValid (Brock et al. 2008), which can be used to compare simultaneously multiple clustering algorithms in a single function call for identifying the best clustering approach and the optimal number of clusters.\n\n10.4.4.1 Measures for comparing clustering algorithms\nThe clValid package compares clustering algorithms using two cluster validation measures:\n\nInternal measures, which uses intrinsic information in the data to assess the quality of the clustering. Internal measures include the connectivity, the silhouette coefficient and the Dunn index.\nStability measures, a special version of internal measures, which evaluates the consistency of a clustering result by comparing it with the clusters obtained after each column is removed, one at a time.\n\nCluster stability measures include:\n\nThe average proportion of non-overlap (APN)\nThe average distance (AD)\nThe average distance between means (ADM)\nThe figure of merit (FOM)\n\nThe APN, AD, and ADM are all based on the cross-classification table of the original clustering on the full data with the clustering based on the removal of one column.\n\nThe APN measures the average proportion of observations not placed in the same cluster by clustering based on the full data and clustering based on the data with a single column removed.\nThe AD measures the average distance between observations placed in the same cluster under both cases (full data set and removal of one column).\nThe ADM measures the average distance between cluster centers for observations placed in the same cluster under both cases.\nThe FOM measures the average intra-cluster variance of the deleted column, where the clustering is based on the remaining (undeleted) columns.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe values of APN, ADM and FOM ranges from \\(0\\) to \\(1\\), with smaller value corresponding with highly consistent clustering results. AD has a value between \\(0\\) and \\(\\infty\\), and smaller values are also preferred.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that, the clValid package provides also biological validation measures, which evaluates the ability of a clustering algorithm to produce biologically meaningful clusters. An application is microarray or RNAseq data where observations corresponds to genes.\n\n\n\n\n10.4.4.2 Compare clustering algorithms in R\nThe simplified format of function cvlValid::clVaild() is:\n\nclVaild(obj, nClust, clMethods = 'kmeans', validation = 'stability', maxitems = 600, metric = \"euclidean\", method = \"average\")\n\nwhere:\n\nobj: A numeric matrix or data frame. Rows are the items to be clustered and columns are samples.\nnClust: A numeric vector specifying the numbers of clusters to be evaluated. e.g., 2:10\nclMethods: The clustering method to be used. Available options are \"hierarchical\", \"kmeans\", \"diana\", \"fanny\", \"som\", \"model\", \"sota\", \"pam\", \"clara\", and \"agnes\", with multiple choices allowed.\nvalidation: The type of validation measures to be used. Allowed values are \"internal\", \"stability\", and \"biological\", with multiple choices allowed.\nmaxitems: The maximum number of items (rows in matrix) which can be clustered.\nmetric: The metric used to determine the distance matrix. Possible choices are \"euclidean\", \"correlation\", and \"manhattan\".\nmethod: For hierarchical clustering (hclust and agnes), the agglomeration method to be used. Available choices are \"ward\", \"single\", \"complete\" and \"average\".\n\nFor example, consider the iris dataset. We start by cluster internal measures, which include the connectivity, silhouette width and Dunn index. It is possible to compute simultaneously these internal measures for multiple clustering algorithms in combination with a range of cluster numbers.\nThe clValid() function can be used for the iris as follows:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAll the measures indicate \\(k = 2\\) performs best for \\(k\\)-means becasue it hase lowest connectivity, highest Dunn, and highest silhouette. The PAM algorithm shows \\(k = 2\\) as optimal under all three metrics. Overall, the \\(k\\)-means is slightly better than PAM in connectivity and silhouette.\nThe iris dataset, when analyzed with these internal metric, exhibits two conpact and well-separated groups rather than three (the known biological species). This is common because: - Two of the species (versicolor and virginica) have overlapping features, so algorithms often merge them. - setosa forms one very distinct cluster.\nThe stability measures can be computed as follow:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe outputs shows that - APN = \\(0.0128\\) (\\(k\\)-means, k = 2) is extremely small which it means clusters are very stable under resampling. - ADM = \\(0.055\\) (\\(k\\)-means, k = 2) confirms that average distances between means hardly change. - AD = \\(1.00\\) (PAM, \\(k = 6\\)) and FOM = \\(0.45\\) (PAM, \\(k = 6\\)) indicate PAM is slightly more stable when \\(6\\) clusters are allowed, though at the cost of over-fragmenting the data.\nThe stability measures (APN, AD, ADM, FOM) confirm that the k-means method with two clusters is the most robust to resampling. Although PAM shows slightly lower instability at k = 6, this likely represents over-partitioning rather than a meaningful improvement in model stability.",
    "crumbs": [
      "Clustering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/clustring.html#references",
    "href": "chapters/clustring.html#references",
    "title": "10  Clustering",
    "section": "10.5 References",
    "text": "10.5 References\n\n\n\n\nBezdek, J. C., and R. J. Hathaway. 2002. “VAT: A Tool for Visual Assessment of (Cluster) Tendency.” In Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN’02 (Cat. No.02CH37290), 3:2225–30.\n\n\nBrock, Guy, Vasyl Pihur, Susmita Datta, and Somnath Datta. 2008. “clValid: An r Package for Cluster Validation.” Journal of Statistical Software 25 (4): 1–22. https://doi.org/10.18637/jss.v025.i04.\n\n\nCharrad, Malika, Nadia Ghazzali, Véronique Boiteau, and Azam Niknafs. 2014. “NbClust: An r Package for Determining the Relevant Number of Clusters in a Data Set.” Journal of Statistical Software 61 (6): 1–36. https://doi.org/10.18637/jss.v061.i06.\n\n\nGower, J. C. 1971. “A General Coefficient of Similarity and Some of Its Properties.” Biometrics 27 (4): 857–71.\n\n\nHartigan, John A., and Manchek A. Wong. 1979. “A K-Means Clustering Algorithm.” Applied Statistics 28: 100–108.\n\n\nKaufman, Leonard, and Peter J. Rousseeuw. 1990. Finding Groups in Data: An Introduction to Cluster Analysis. New York: Wiley.\n\n\nMacQueen, J. 1967. “Some Methods for Classification and Analysis of Multivariate Observations.” In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, edited by L. M. Le Cam and J. Neyman, 1:281–97. Berkeley, CA: University of California Press.\n\n\nTheodoridis, Sergios, and Konstantinos Koutroumbas. 2008. Pattern Recognition. 4th ed. Amsterdam, Netherlands: Academic Press.\n\n\nTibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the Number of Data Clusters via the Gap Statistic.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 63 (2): 411–23.",
    "crumbs": [
      "Clustering",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5_logistic_regression.html",
    "href": "chapters/chapter5_logistic_regression.html",
    "title": "11  Logistic Regression",
    "section": "",
    "text": "11.1 Modelling Probabilities\nA binary classification problem is one where the outcome variable can take on only two possible values. For example:\nFor this kind of problem, the dataset used to fit the model is composed of observations \\((\\mathbf{x}_1, y_1)\\), \\((\\mathbf{x}_2, y_2)\\), …, \\((\\mathbf{x}_n, y_n)\\) where each \\(y_i\\) is usually labelled 0 (failure, no event, …) or 1 (success, event, …) and \\(\\mathbf{x}_i\\) is a vector of input variables.\nThe task at hand is to build a model that, given an arbitrary \\(\\mathbf{x}\\), will predict the probability that \\(y = 1\\).\nWhy not use a linear regression model to predict the probability that \\(y = 1\\)? After all, linear regression is a well-known and widely used method.\nThere are some problems with this approach:",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5_logistic_regression.html#modelling-probabilities",
    "href": "chapters/chapter5_logistic_regression.html#modelling-probabilities",
    "title": "11  Logistic Regression",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\nthe output values are never (with probability 1) exactly 0 or 1, the values that are taken by \\(y\\);\neven considering a threshold (e.g. 0.5) to classify the predicted values into 0 or 1, this would be mostly an arbitrary chosen value.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5_logistic_regression.html#sigmoid-functions",
    "href": "chapters/chapter5_logistic_regression.html#sigmoid-functions",
    "title": "11  Logistic Regression",
    "section": "11.2 Sigmoid Functions",
    "text": "11.2 Sigmoid Functions\nInstead of a linear function, we can opt to model the probability of success of the event described by \\(y\\), i.e \\(\\operatorname{P}(Y=1)\\). This is a more sound and natural way to model the prediction of a random binary event. In order to do this, we can use sigmoid functions. A sigmoid function \\(f(x)\\) is a function having an “S” shaped curve (sigmoid curve) with the following properties:\n\n\\(0&lt;f(x)&lt;1\\);\n\\(\\displaystyle \\lim_{x \\to -\\infty} f(x) = 0\\), \\(\\displaystyle \\lim_{x \\to +\\infty} f(x) = 1\\);\n\\(f'(x)&gt;0\\).\n\nA common sigmoid function is the logistic function, defined as:\n\\[\nf(x) = \\frac{e^x}{1 + e^x} = \\frac{1}{1 + e^{-x}}.\n\\]\nA logistic function can be used to model the probability that \\(y = 1\\) given \\(\\mathbf{x}\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis function will play a central row in the logistic regression model, as we will see briefly.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5_logistic_regression.html#logistic-model",
    "href": "chapters/chapter5_logistic_regression.html#logistic-model",
    "title": "11  Logistic Regression",
    "section": "11.3 Logistic Model",
    "text": "11.3 Logistic Model\nThe first random variable that comes to mind in order to model a random binary event is the Bernoulli random variable, which is a discrete random variable that takes value 1 with probability \\(p\\) and value 0 with probability \\(1-p\\). The parameter \\(p\\) is the probability of success of the event described by the random variable, and it’s probability function is\n\\[\np(y) = p^y (1-p)^{1-y}, \\quad y \\in \\{0, 1\\}.\n\\] The expected value of a Bernoulli random variable is \\(\\operatorname{E}[Y] = p\\) and it’s variance is \\(\\operatorname{V}(Y) = p(1-p)\\).\n\n\n\n\n\n\nNote\n\n\n\nA natural extension is the Binomial random variable, which is the sum of \\(n\\) independent Bernoulli random variables with the same parameter \\(p\\). The parameter \\(p\\) is again the probability of success of the event described by the random variable, and it’s probability function is\n\\[\np(y) = \\binom{n}{y} p^y (1-p)^{n-y}, \\quad y \\in \\{0, 1, ..., n\\}.\n\\]\n\n\nAnother way of expressing the probability of an event is through the odds of the event, defined as the ratio between the probability of the event and the probability of it’s complementary event:\n\\[\n\\operatorname{odds} = \\frac{p}{1-p} = \\frac{1}{1-p^{-1}}, \\quad p \\in ]0, 1[.\n\\]\nThe odds are a number between 0 and \\(+\\infty\\). If the odds are equal to 1, then the event and it’s complementary are equally likely. If the odds are greater than 1, then the event is more likely than it’s complementary event, and vice versa. It’s easy to see that odds are monotonically increasing with respect to \\(p\\):\n\\[\n{(\\operatorname{odds})}' = \\frac{1}{(1-p)^2} &gt; 0.\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOdds are an alternative way of expressing probabilities, still being relatively intuitive concerning interpretation. Nevertheless, they are still confined to the interval \\(]0, +\\infty[\\), because\n\\[\n\\lim_{p \\to 0^+} \\frac{1}{1-p^{-1}} = 0, \\quad \\lim_{p \\to 1^-} \\frac{1}{1-p^{-1}} = +\\infty.\n\\]\nIf we want to have a represantation of probabilities that spans the whole real line, we can use the log-odds, also called logit function, defined as\n\\[\n\\operatorname{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right), \\quad p \\in ]0, 1[.\n\\]\nThis creates a one-to-one mapping between the interval \\(]0, 1[\\) and the whole real line \\(\\mathbb{R}\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe logit function is also monotonically increasing with respect to \\(p\\):\n\\[\n{\\big(\\operatorname{logit}(p)\\big)}' = \\frac{1}{p(1-p)} &gt; 0.\n\\]\nThis representation of probabilities is well suited to be used in a regression model, because it spans the whole real line. In fact, in a logistic regression model, we assume that the log-odds of the probability that \\(y = 1\\) is a linear combination of the input variables: \\[\n\\operatorname{logit}\\big(\\operatorname{P}(Y=1|\\mathbf{x})\\big) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p = \\mathbf{x}^T \\boldsymbol{\\beta} = \\eta (\\mathbf{x}),\n\\] where \\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, ..., \\beta_p)^T\\) is the vector of parameters to be estimated.\nNote that the logit function is the inverse of the logistic function! Hence, we can rewrite \\(\\operatorname{P}(Y=1|\\mathbf{X}=\\mathbf{x})\\) as\n\\[\n\\operatorname{P}(Y=1|\\mathbf{x}) = \\frac{e^{\\eta(\\mathbf{x})}}{1 + e^{\\eta(\\mathbf{x})}} = \\frac{1}{1 + e^{-\\eta(\\mathbf{x})}}.\n\\tag{11.1}\\]\n\n\n\n\n\n\nWarning\n\n\n\nEquation 11.1 describes a model for the expected value of the random variable \\(Y\\) given \\(\\mathbf{x}\\), that follows a Bernoulli distribution with parameter \\(p = \\operatorname{P}(Y=1|\\mathbf{x})\\). It does not describe a model for \\(Y\\) itself, which is a discrete random variable taking values 0 or 1.",
    "crumbs": [
      "Classification",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1_data_ethics.html",
    "href": "chapters/chapter1_data_ethics.html",
    "title": "12  Data Ethics",
    "section": "",
    "text": "12.1 What is the Ethics of Data Science?\nThe ethics of data science are very important, and all data analyst, data scientists, and everyone who works with data should know the principles of ethics.\nAll individuals handling data are obligated to report any occurrences such as data theft, improper storage, unethical data collection, or data misuse.\nFor example, a business might track and store information about a customer’s path, starting from the moment they enter their email address on the website until they purchase products. In such cases, it is essential that the individual’s report remains confidential and is safeguarded it from unauthorized access.\nThe study and evaluation of ethical issues related to data have led to the emergence of a new domain in ethics, known as “data science ethics.” Data can be collected, recorded, generated, processed, shared, and used. This field also covers various data and technologies, including programming, professional code, and algorithms.\nIn the past, ethical principles in computer and information science primarily focused on the content and information present within computer systems. However, with the advancement of technology and the increasing volume of data, the scope of data science ethics has expanded to include concepts and principles related to data collection, use, processing, and sharing.\nWhen companies collect data from individuals, ethical issues related to privacy, personal information, and data misuse arise. However, when companies begin to use these data for purposes not initially specified, even more ethical concerns emerge. In other words, companies try to monetize the collected data, using it in ways that were not previously disclosed. This practice further challenges privacy, trust, and ethics in data collection and use.\nAlthough “ethics” seems complicated, it is really about understanding what is right or wrong. There are different ways people think about ethics, especially when it comes to data world.",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Ethics</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1_data_ethics.html#what-is-the-ethics-of-data-science",
    "href": "chapters/chapter1_data_ethics.html#what-is-the-ethics-of-data-science",
    "title": "12  Data Ethics",
    "section": "",
    "text": "Important\n\n\n\nData Ethics covers the moral and ethical obligations related to the collection, sharing, and use of data, focusing on ensuring its fair and beneficial applications. It is mainly focused on any negative impacts that data might have on individuals, groups or wider society.\n\n\n\n12.1.1 Principles of Data Ethics for Business Professionals\nIn an era where data is considered the new oil, understanding the ethical issues related to its collection, analysis, and use is of most important. Below, we will introduce the five key ethical principles that every data professional should be aware of.\n\nOwnership: The first principle of data ethics is that every individual has the right to ownership over their personal information. Just as it is considered theft to take an object without its owner’s permission, collecting someone’s personal information without their consent is both illegal and unethical. Some common methods for obtaining consent include signed written agreements, digital privacy policies that require users to agree to a company’s terms and conditions, and pop-up windows with check-boxes that allow websites to track users’ online behavior with cookies. *Never assume customers agree to their information being collected; always seek their permission to avoid ethical and legal problems.\nTransparency: In addition to ownership, individuals whose data you collect have the right to know how you plan to gather, store, and use their personal information. When collecting data, prioritize transparency and provide them with all necessary information.\nFor example, imagine your company decides to implement an algorithm to personalize the website experience based on user shopping habits and behavior. You should develop a policy explaining that cookies will track user behavior, the collected data will be stored in a secure database, and an algorithm will be trained to personalize the website experience.\nUsers have the right to access this information so they can decide whether to accept or reject your website’s cookies. Hiding or lying about your company’s methods and goals is deceptive, illegal, and unfair to the individuals whose data you handle.\nPrivacy: One of the ethical responsibilities associated with handling data is preserving the privacy of individuals whose data is involved. Even if a customer has given your company permission to collect, store, and analyze their Personally Identifiable Information (PII), it does not mean they want this information to be publicly accessible.\nPII is any data that relates to an individual’s identity. This typically includes items such as full name, address, phone number, date of birth, credit card number, bank account number, social security number, passport number, and other information that can be used to identify a person.\nTo protect individuals’ privacy, ensure you store data in a secure database to prevent it from falling into the wrong hands. Data security methods that help maintain privacy include file encryption (transforming information into a readable format only with an encryption key) and dual-authentication (password protection with two-factor authentication).\nOne way to prevent errors is to de-identify the dataset. When all PII is removed, only anonymous data remains, and the dataset can no longer be traced back to individuals. This allows analysis to identify relationships between variables of interest without linking data points to personal identities.\nIntention : Before you even begin collecting data, it is crucial to consider why you need this data and what purpose you aim to achieve with it. If your intention is to engage in unethical activities, such as harassing others or improperly exploiting individuals’ vulnerabilities, then collecting their data is not ethical. Ethical data collection requires a legitimate purpose and operating in accordance with regulations and ethical principles.\nEven when your intention is good - for instance, collecting data to understand health experience in order to create an app that addresses urgent needs – you still need to evaluate your purpose for collecting every piece of data.\nOutcomes : Even if your intention and purpose in collecting and analyzing data are good, the outcome of this process might unintentionally cause harm to individuals or groups. This type of harm is known as disparate impact and is considered invalid under civil rights laws, meaning it is illegal.\nUnfortunately, you cannot definitively predict the exact impact of data analysis until it is complete. However, by considering this question and its importance before conducting the analysis, you can identify any potential disparate impacts that might occur. By being aware if the possibility of disparate impact, you can then take the necessary steps to prevent and mitigate it.",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Ethics</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1_data_ethics.html#why-is-ethics-important-in-the-age-of-data-abundance",
    "href": "chapters/chapter1_data_ethics.html#why-is-ethics-important-in-the-age-of-data-abundance",
    "title": "12  Data Ethics",
    "section": "12.2 Why is Ethics Important in the Age of Data Abundance?",
    "text": "12.2 Why is Ethics Important in the Age of Data Abundance?\nUsing information ethically within decision-making has always been important. However, two factors have made data ethics business-critical:\n\nData Volumes : There has been an explosion in the amount of data available to organizations, both collected themselves, ans sourced from third-parties. It is not always clear where this information has come from, particularly in the case of personal information, and what permissions have been provided for its reuse.\nArtificial Intelligence : Organizations are increasing using machine learning and artificial intelligence algorithms to make sense of data and take automated decisions based on data analysis without involving human oversights. This can lead to issues around fairness and discrimination, even if these are unintended consequences of how data is used.\n\nThe importance of ethics in data science stems from the necessity of having a clear set of rules and guidelines that determine what businesses can and cannot do with the personal information they collect from their customers. This is crucial for safeguarding customer privacy and rights, and appropriate laws and standards must be established to protect personal information.\nAll experts agree that certain fundamental principles must be implemented, even if this field still contains gray areas and nothing is simple or uniform. These are just a few important topics and strategic ideas that have currently garnered the most attention. However, there is still much ground to cover in data ethics, and further progress and development are needed in this area.\n\n\n\n\n\n\nNote Exercise 1\n\n\n\nIdentify the data ethics principle related to each question:\n\nIs it clear what data is being used for, how and where it is being stored, and is this information freely available to all?\nAre you collecting only data that is necessary and relevant to your clearly defined goals?\nHave you anticipated any potential harmful, negative, or biased impacts that could result from your use of the data?\nIs any personal or identifiable data being securely stored and anonymized to prevent unauthorized access?\nHave you obtained informed consent from individuals for how their data will be used in your project?\n\n\n\n\n\n\n\nTip Answers\n\n\n\n\n\n\nTransparency\nIntention\nOutcomes\nPrivacy\nOwnership\n\n\n\n\n\n\n\n\n\n\n\n\nNote Exercise 2\n\n\n\nIdentify the data ethics principle related to each question:\n\nA company collects location data from users’ mobile phones but does not disclose that this information will be shared with advertisers.\nAn analyst stores survey responses on an unsecured shared drive, and the file includes names and phone numbers of participants.\nA data science team begins using a health dataset for a study unrelated to the one participants originally agreed to take part in.\nBefore collecting any data, a team carefully evaluates whether the data they plan to gather is essential to their project goals and excludes anything not directly needed.\nBefore launching a new app feature based on user behavior data, a team conducts a risk assessment to explore how it might negatively affect vulnerable users.\n\n\n\n\n\n\n\nTip Answers\n\n\n\n\n\n\nTransparency\nPrivacy\nOwnership\nIntention\nOutcomes",
    "crumbs": [
      "Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data Ethics</span>"
    ]
  }
]